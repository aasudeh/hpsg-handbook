\documentclass[output=paper]{langsci/langscibook} 
\author{Andy Lücking\affiliation{Universit\'{e} de Paris, Goethe-Universität Frankfurt}}
\title{Gesture}

% \chapterDOI{} %will be filled in at production

\epigram{% 
\begin{tikzpicture}[scale=1.1]  
  % head:      
  \fill[color=black,rounded corners=1pt] (0.05,0.02) .. controls (0.15,0.25) .. (0.13,0.35) .. controls (0.03,0.5) .. (0.03,0.7) .. controls (0.08,0.86) .. (0.2,0.95) .. controls (0.35,0.99) .. (0.55,0.95) .. controls (0.65,0.85) .. (0.7,0.7) -- (0.7,0.65) -- (0.69,0.6) -- (0.77,0.47) -- (0.76,0.44) -- (0.72,0.43) -- (0.73,0.36) -- (0.71,0.35) -- (0.72,0.33) -- (0.7,0.3) -- (0.71,0.25) .. controls (0.68,0.21) .. (0.65,0.2) -- (0.47,0.18) -- (0.44,0) -- cycle;
  % Schallwellen:
  \draw[very thick,color=gray,decorate,decoration={expanding waves,angle=20}] (0.8,0.3) -- (2.1,0.3); 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  % right interlocutor:
  \begin{scope}[xshift=3.5cm]
    \pgftransformxscale{-1}
    % head  
    \fill[color=black,rounded corners=1pt] (0.05,0.02) .. controls (0.15,0.25) .. (0.13,0.35) .. controls (0.03,0.5) .. (0.03,0.7) .. controls (0.08,0.86) .. (0.2,0.95) .. controls (0.35,0.99) .. (0.55,0.95) .. controls (0.65,0.85) .. (0.7,0.7) -- (0.7,0.65) -- (0.69,0.6) -- (0.77,0.47) -- (0.76,0.44) -- (0.72,0.43) -- (0.73,0.36) -- (0.71,0.35) -- (0.72,0.33) -- (0.7,0.3) -- (0.71,0.25) .. controls (0.68,0.21) .. (0.65,0.2) -- (0.47,0.18) -- (0.44,0) -- cycle;
    % Schallwellen:
    % \draw[very thick,color=gray,decorate,decoration={expanding waves,angle=25}] (0.8,0.3) -- (1.3,0.3); 
  \end{scope}
  %% Hand links
  \begin{scope}[scale=0.55,xshift=1.6cm,yshift=-0.8cm,rotate=320]
    \fill[color=black,rounded corners=1pt] (0,0) -- (0.1,0.22) .. controls (0.08,0.35) .. (0.05,0.46) .. controls (0.03,0.56) .. (0.05,0.63) .. controls (0.06,0.65) .. (0.06,0.75) .. controls (0.07,0.78) and (0.14,0.78) .. (0.15,0.65) .. controls (0.18,0.54) .. (0.3,0.75) .. controls (0.35,0.87) .. (0.4,0.97) .. controls (0.43,1) and (0.47,0.98) .. (0.47,0.93) -- (0.4,0.68) .. controls (0.54,0.95) .. (0.6,0.97) .. controls (0.61,0.96) and (0.64,0.96) .. (0.62,0.9) -- (0.49,0.63) .. controls (0.58,0.75) .. (0.65,0.84) .. controls (0.72,0.87) and (0.72,0.81) .. (0.7,0.77) .. controls (0.66,0.7) .. (0.56,0.57) .. controls (0.65,0.63) .. (0.7,0.67) .. controls (0.74,0.7) and (0.78,0.63) .. (0.58,0.47) .. controls (0.47,0.26) .. (0.35,0.17) .. controls (0.33,0.05) .. (0.32,0) -- cycle;
  \end{scope}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  % Hand rechts
  \begin{scope}[scale=0.55,xshift=3.3cm, yshift=-1.75cm]
    \fill[color=black,rounded corners=1pt] (1,0.43) .. controls (0.92,0.38) .. (0.84,0.35) .. controls (0.7,0.15) and (0.5,0.05) .. (0.4,0.05) .. controls (0.25,0) .. (0.2,0.02) -- (0.12,0.01) .. controls (0.06,0.02) and (0,0.04) .. (0,0.07) .. controls (0.02,0.09) .. (0.05,0.09) .. controls (0.06,0.16) and (0.1,0.17) .. (0.15,0.18) .. controls (0.17,0.22) and (0.22,0.23) .. (0.25,0.24) .. controls (0.28,0.3) and (0.32,0.36) .. (0.37,0.42) .. controls (0.39,0.43) and (0.41,0.46) ..(0.43,0.46) -- (0.44,0.5) .. controls (0.35,0.46) .. (0.32,0.46) .. controls (0.27,0.43) and (0.18,0.44) .. (0.17,0.47) .. controls (0.2,0.5) .. (0.25,0.53) .. controls (0.45,0.64) .. (0.65,0.65) .. controls (0.75,0.61) .. (0.85,0.65) -- cycle;
  \end{scope}
\end{tikzpicture}
} 
% end epigram


\abstract{
  The received view in (psycho-)linguistics, dialogue theory and gesture studies is that co-verbal gestures, i.e. hand and arm movement, are part of the utterance and contribute to meaning \citep{Kendon:1980,McNeill:1992}. The relationships between gesture and speech obey regularities that need to be defined in terms of not just the relative timing of gesture to speech, but also in terms of the linguistic form of that speech: for instance, prosody and syntactic constituency and headedness \citep{Loehr:2007,Ebert:Evert:Wilmes:2011,Alahverdzhieva:Lascarides:Flickinger:2017}. Consequently, speech-gesture integration is captured in grammar by means of a gesture-grammar interface. This chapter provides basic snapshots from gesture research, reviews constraints on speech-gesture integration and summarises their implementations into HPSG frameworks. Pointers to future developments conclude the exposition. Since there are already a couple of overviews on gesture such as \citet{Ozyurek:2012,Wagner:Malisz:Kopp:2014,Abner:Cooperrider:Goldin-Meadow:2015}, this chapter aims at distinguishing itself in that it provides a guided tour of research that focuses on using (mostly) standard methods for semantic composition in constraint-based grammars like HPSG to model gesture meanings.
}

\maketitle

\begin{document}
\label{chap-gesture}

\avmoptions{}

\section{Why gestures?} 
\label{sec:why-gestures}

\is{non-verbal behaviour|(}
People talk with their whole body. 
%
A verbal utterance is couched into an intonation pattern that, \textit{via} prosody, articulation speed or stress, function as \emph{paralinguistic} \is{paralinguistics} signals \citep[e.g.][]{Birdwhistell:1970}. 
%
The temporal dimension of paralinguistics gives rise to \emph{chronemic} codes \is{chronemics} \citep{Poyatos:1975,Bruneau:1980}. \emph{Facial expressions} \is{facial expressions} are commonly used to signal emotional states \is{emotions} \citep{Ekman:Friesen:1978}, even without speech \citep{Argyle:1975}, and are correlated to different illocutions of the speech acts performed by a speaker \citep{Domaneschi:Passarelli:Chiorri:2017}.
%
Interlocutors use \emph{gaze} \is{gaze} as a mechanism to achieve joint attention \citep{Argyle:Cook:1976} or provide social signals \citep{Kendon:1967}. 
%
Distance and relative direction of speakers and addressees are organised according to culture-specific radii into social spaces \is{proxemics} \citep[\emph{proxemics},][]{Hall:1968}. 
%
Within the inner radius of private space, tactile codes of \emph{tacesics} \is{tacesics} \citep{Kauffman:1971} are at work. 
%
Since the verbal and nonverbal communication means of face to face interaction \is{face to face interaction} may occur simultaneously, \emph{synchrony} \is{synchrony} (i.e., the relative timing of verbal vs. non-verbal actions) is a feature of the form of the multimodal utterance that contributes to identifying the word(s) affiliated to the gesture \citep{Wiltshire:2007}. 
%
A special chronemic case is signalling at the right moment -- or, for that matter, to the contrary: missing the right moment (an aspect of communication dubbed \emph{kairemics} \is{kairemics} by \citet[\page 600]{Luecking:Pfeiffer:2012}).
%
Besides the manifold areas of language use, the conventionalised, symbolic nature of language secures language's primacy in communication, however \citep{de:Ruiter:2004}.
%
For thorough introductions into \isi{semiotics} and \isi{multimodal communication} see \citet{Noeth:1990}, \citet{Posner:Robering:Sebeok:1997:2004} or \citet{Mueller:Cienki:Fricke:Ladewig:McNeill:Tessendorf:2013:2014}.
\is{non-verbal behaviour|)}


\is{co-verbal gesture|(}
The most conspicuous non-verbal communication means of everyday interaction are hand and arm movements, known as \emph{gestures}.
%
In seminal works, \citet{McNeill:1985,McNeill:1992} and \citet{Kendon:1980,Kendon:2004} argue that co-verbal gestures\is{co-verbal gesture}, i.e. hand and arm movements, can be likened to words in the sense that they are part of a speaker's utterance and contribute to discourse.
%
Accordingly, integrated \isi{speech-gesture production models} have been devised \citep{Kita:Ozyurek:2003,de:Ruiter:2000,Krauss:Chen:Gottesmann:2000} that treat utterance production as a multimodal process.
%
Given gestures' imagistic \is{imagistic gesture} and often spontaneous character, it is appealing to think of them as \enquote{postcards from the mind} \citep[p.~21]{de:Ruiter:2007:a}.
%
Clearly, given this entrenchment in speaking, the fact that one can communicate meaning with non-verbal signals has repercussions to areas hitherto taken to be purely linguistic (in the sense of being related to the verbal domain).
%
This section highlights some phenomena particularly important for grammar, including, for instance,  \emph{mixed syntax} \is{mixed syntax} \citep{Slama-Cazacu:1976}, or \emph{pro-speech gesture}: \is{pro-speech gesture}
%
\ea \label{ex:mixed-syntax}
He is a bit [\textit{circular movement of index finger in front of temple}].
\z

In (\ref{ex:mixed-syntax}), a gesture replaces a position that is usually filled by a syntactic constituent.
%
The gesture is emblematically related to the property of \textit{being mad} so that the mixed utterance from (\ref{ex:mixed-syntax}) is equivalent to the proposition that the referent of \textit{he} is a bit mad.

\begin{figure}
  \centering
  \includegraphics[width=5cm]{figures/Betonsockel-1}
  \caption{\enquote{die Skulptur die hat 'n [BETONsockel]} ($\approx$ \textit{the sculpture has a [CONCRETE base]}) [V5, 0:39].}
  \label{fig:betonsockel}
\end{figure}

The gesture shown in Figure~\ref{fig:betonsockel} depicts the shape of a concrete base, which the speaker introduces into discourse as an attribute of a sculpture:\footnote{The examples in Figures~\ref{fig:betonsockel}, \ref{fig:so} and \ref{fig:staircases} are drawn from the (German) \emph{Speech and Gesture Alignment Corpus} \citep[\isi{SaGA},][]{Luecking:Bergmann:Hahn:Kopp:Rieser:2010} and are quoted according to the number of the dialogue they appear in and their starting time in the respective file (e.g. \enquote{V9, 5:16} means that the datum can be found in the video file of dialogue V9 at minute 5:16). The examples have also been used in \citet{Luecking:2016}.}
%
\ea \label{ex:betonsockel}
\gll die Skulptur die hat 'n [BETONsockel] \\
     the sculpture it has a {[CONCRETE base]} \\
\glt \enquote*{The scultpure has a concrete base.}
\z
%
Square brackets roughly indicate the portion of speech which is temporally overlapped by the gesture (to be more precise: by the gesture stroke, see Figure~\ref{fig:gesture-phases} below), upper case is used to mark main stress.
%
From the gesture, but not from speech, we get that the concrete base of the sculpture has the shape of a flat cylinder -- thus, the gesture acts as a nominal modifier.
%
However, the gesture is incomplete with regard to its interpretation -- it just depicts about half of cylinder. 
%
Thus, gesture interpretation may involve extensions known from gestalt theory \citetext{see \citealp{Luecking:2016} on a \emph{good continuation} \is{good continuation} constraint}.


\begin{figure}
  \centering
  \includegraphics[width=5cm]{figures/mmsubcat2}
  \caption{\enquote{dann ist das Haus halt so []} ($\approx$ then is the house just this []) [V11, 2:32].}
  \label{fig:so}
\end{figure}

The speaker of the datum in Figure~\ref{fig:so} uses just a demonstrative adverb in order to describe the shape of a building he is talking about.
%
The demonstrative shifts the addressee's attention to the gesture, which accomplishes the full shape description, namely a cornered U-shape.
%
In contrast to the exmaple in Figure~\ref{fig:betonsockel}, the utterance associated with Figure~\ref{fig:so} is not even interpretable without the gesture.


% Probably the most conspicuous gestures accompanying speech are pointing gestures (or deictic gestures).
%
% Given that the pronoun in (\ref{ex:mixed-syntax}) is used exophorically (that is, referring to someone from the physical utterance situation instead to a discourse referent from the preceding discourse), a pointing signal is required indicating where to find the referent. 
%
A lack of interpretability is shared by exophorically used demonstratives, which are \emph{incomplete} \is{incomplete demonstratives} without a demonstration act like a pointing gesture \citep[\page 490]{Kaplan:1989:a}.
%
For instance, Claudius would experience difficulties to understand how serious Polonius is about his (Polonius) conjecture about the reason of Hamlet's madness, if Polonius had not produced pointing gestures (Shakespeare, \is{Shakespeare} \textit{Hamlet, Prince of Denmark} Act II, Scene 2; the third occurrence of \textit{this} is anaphoric and refers back to Polonius' conjecture):
%
\ea \label{ex:this}
\textsc{polonius} (points to his head and shoulders): 
Take this from this if this be otherwise.
\z

With regard to deictic gestures \citet[Sec.~5.4]{Fricke:2012} argues that deictic words \is{deictic word}  within noun phrases -- her prime example is \ili{German} \textit{so} (\textit{like this}) --, provide a \emph{structural}, that is, \emph{language-systematic} integration point between the vocal plane of conventionalized words and the non-vocal plane of body movement. \is{speech-gesture integration}
%
Therefore, on this conception, not only utterance production but \emph{grammar} is inherently multimodal. \is{multimodal grammar}


In order for Claudius to interpret Pololinus' multimodal utterance in (\ref{ex:this}) properly, he has to associate correctly the two pointing gestures with the first two occurrences of \textit{this} \citep[cf.][]{Kupffer:2014}. 
%
\is{affiliation|(}
Polonius facilitates such an interpretation by means of a temporal coupling of pointing gestures and their associated demonstratives -- a relationship that is called \emph{affiliation}.
%
The role of synchrony in multimodal utterances is illustrated by the following example, (\ref{ex:staircases}) and Figure~\ref{fig:staircases}, (taken from \citet[\page 189]{Luecking:2013:a}):
%
\ea \label{ex:staircases}
\gll Ich g[laube das sollen TREP]pen sein.\\
     I think those should STAIRCAses be. \\
\glt \enquote*{I think those should be staircases.}
\z

\begin{figure}[tb]
  \includegraphics[width=5cm]{figures/treppen-2}
  \caption{\enquote{Ich g[laube das sollen TREP]pen sein} (\textit{I think those should be staircases}) [V10, 3:19].}
  \label{fig:staircases}
\end{figure}

The first syllable of the \ili{German} noun \textit{Treppen} (\textit{staircases}) carries main stress, indicated by capitalization. 
%
The square brackets indicate the temporal overlap between speech and gesture stroke, which is shown in Figure~\ref{fig:staircases}.
%
The gesture attributes to the noun it attaches to: from the multimodal utterance the observer retrieves the information that the speaker talks about spiral staircases. 
%
This interpretation assumes that the common noun is the affiliate of the gesture.
%
Obviously, mere temporal synchrony is too weak to be an indicator of affiliation.
%
In fact, there are speech-gesture affiliations without temporal overlap between gesture and verbal affiliate at all \citep[e.g.][]{Luecking:Rieser:Stegmann:2004}.
%
Therefore, temporal overlap or vicinity is just one indicator of affiliation, a second one is intonation: a gesture is usually related to a stressed element in speech \citep{McClave:1994,Nobe:2000,Loehr:2004,Loehr:2007}. \is{phonetic speech-gesture constraint}
%
As a result, multimodal communication gives rise to complex \enquote{peak pattern} \citep{Tuite:1993,Loehr:2004,Jannedy:Mendoza-Denton:2005}.


The interpretation of a gesture changes with different affiliations. \is{gesture affiliation}
%
Suppose the gesture from Figure~\ref{fig:staircases} is produced in company to stressed \textit{glaube} (\textit{think}) instead of \textit{staircases}: 
%
\ea \label{ex:think}
\gll Ich G[LAUbe das sollen Trep]pen sein.\\
     I THINK those should staircases be \\
\glt \enquote*{I think those should be stairscases.}
\z
%
Now the spiral movement is interpreted as a metaphorical depiction of a psychological process.
%
Thus, the interpretation of a gesture depends on the integration point (affiliation), \is{gesture affiliation} which in turn is marked by \isi{prosody}.
\is{affiliation|)}


The crucial observations in any case are that gesture affiliation is sensitive to temporal relations \citetext{supplemented by prosodic and also semantic information, as argued by \citealp{Alahverdzhieva:Lascarides:Flickinger:2017}, amongst others} and contributes to propositional content.
%
Interestingly, gestures share the latter aspect with laughter, which also has propositional content \citep{Ginzburg:Breitholz:Cooper:Hough:Tian:2015}, for instance, referring to real world events.
%
Thus, a \is{multimodal utterance} may express a richer content than speech alone, as in (\ref{ex:staircases}), or a content equivalent to speech content, as in (\ref{ex:think}); it can even express less than speech or contradict speech:\footnote{In case of contradiction, that, speech-gesture mismatch, the resulting multimodal utterance is perceived as ill-formed and induces N400 effects \citep{Wu:Coulson:2005,Kelly:Kravitz:Hopkins:2004}.} \is{gesture-speech relations}
%
\begin{quote}
The nonverbal act can repeat, augment, illustrate, accent, or contradict the words; it can anticipate, coincide with, substitute for or follow the verbal behavior; and it can be unrelated to the verbal behavior.\hfill 
\citep[53]{Ekman:Friesen:1969}
\end{quote}


Moving from sentence to dialogue, \emph{interactive gestures} \is{interactive gesture} are bound up with \isi{turn management}, among others \citep{Bavelas:Chovil:Lawrie:Wade:1992,Bavelas:Chovil:Coates:Roe:1995}.
%
For instance, pointing gestures can be used to indicate the next speaker \citep{Rieser:Poesio:2009}. 
%
Thus, inasmuch as grammar is multimodal, also dialogue theory has to deal with specific non-verbal interaction means (see also \crossrefchaptert{pragmatics}).


It should be noted, however, that not all gestures are as clearly a part of a sentence or an utterances as those in the examples given so far.
%
A prominent counter-example in particular for \isi{interactive gesture} views is gesturing on the telephone (see \citet{Bavelas:Gerwing:Sutton:Prevost:2008} for an overview of a number of respective studies).
%
Since such gestures are not observable for the addressee, they cannot reasonably be taken to be a constituent of the intended content. 
%
Rather, \enquote{telephone gestures} seem to be purely speaker-oriented, presumably facilitating \isi{word retrieval}.
%
Furthermore, the lion's share of everyday gestures seem to consist of rather sloppy movements that do not contribute to the content of the utterance -- they are \emph{contingent} \is{contingent gesture} as opposed to be an obligatory \is{obligatory gesture} semantic component \citep{Luecking:2013:a}.
%
Gestures (or some other demonstration) can be obligatory when they occur as an element of a demonstrative expression.
%
Within two-dimensional semantics this distinction is rendered in such a way that a contingent gesture contributes to non-at-issue content, whereas an obligatory gesture contributes to at-issue content \citet{Ebert:2014:a,Schlenker:2018}.
%
Based on such dividing evidence, gestures have broadly been distinguished into \emph{foreground} \is{foreground gesture} vs. \emph{background} gestures \is{background gesture} \citep{Cooperrider:2017}. 
%
This distinction aims to cover a basic difference in gestures and in the way a gesture is integrated into an utterance. 
%
Roughly, foreground gesture, but not background gestures, are semantically interpretable components of an utterance.
%
This chapter focuses on foreground gestures.
\is{co-verbal gesture|)}



\section{Kinds of gestures}
\label{sec:kinds-gestures}

Pointing at an object seems to be a different kind of gesture than mimicking drinking by moving a bent hand (virutally holding something) towards the mouth while slightly roating the back of hand upwards.
%
And both seem to be different from actions like scratching.
% 
On such grounds, gestures are usually assigned to \isi{gesture classes}.
%
On a basic level gestures that have an intrinsic relation to speech and what is communicated are distinguished from movements unrelated to the production or content of a speech act. 
%
The latter have been called \emph{Adaptors} \is{adaptor gesture} \citep{Ekman:Friesen:1969} and are not dealt with further here (thus, adaptors include scratching, foot-shaking, pen-fiddling; but see \citet{Zywiczynski:Wacewicz:Orzechowski:2017} for evidence that adaptors may be associated with turn transition points in dialogue).
%
The former have been called \emph{regulators} \is{regulator gesture} and \emph{illustrators} \is{illustrator gesture} \citep{Ekman:Friesen:1969} and cover a variety of gesture classes.
%
These gesture classes are characterized by the function performed by a gesture and the meaning relation the gesture bears to its content (if any).
%
A classic taxonomy consists of the following classes \citep{McNeill:1992}:
%
\begin{itemize}
\item iconic (or representational) \is{iconic gesture} \is{representational gesture} gestures. Spontaneous hand and arm movements that are commonly said to be based on some kind of \isi{resemblance} \is{similarity} relation.\footnote{But see footnote \ref{fn:resemblance} in Section~\ref{sec:iconic-gestures} for pointers to critical discussions of resemblance as a sign-bearing relation.} Iconic gestures employ a mode of representation such as \textit{drawing}, \textit{modeling}, \textit{shaping} or \textit{placing} \citep{Streeck:2008,Mueller:1998}.
\item \isi{deictic gestures} (\isi{pointing}). Typically hand and arm movements that perform a \isi{demonstration act}. 
%
In which way pointing is standardly accomplished is subject to culture-specific conventions, however \citep{Wilkins:2003}. 
%
In principle, any extended body part, artifact or locomotor momentum will serve the demonstrative purpose. 
%
Accordingly, there are deictic systems that involve \isi{lip-pointing} \citep{Enfield:2001} or nose-pointing \citep{Cooperrider:Nunez:2012}. 
%
Furthermore, under certain circumstances, pointing with the eyes (\isi{gaze-pointing}) is also possible \citep{Hadjikhani:Hoge:Snyder:de:Gelder:2008}. 
%
Note further that the various deictic means can be interrelated. For instance, manual pointing can be differentiated by cues of head and gaze \citep{Butterworth:Itakura:2000}.
%
Furthermore, pointing with the hand can be accomplished by various hand shapes: \citet{Kendon:Versante:2003} distinguish \emph{index finger pointing}, \is{index finger pointing} (with a \emph{palm down} \is{index palm down} and a \emph{palm vertical} \is{index palm vertical} variant) \emph{thumb pointing}, \is{thumb pointing} and \emph{open hand pointing} \is{open hand pointing} (again with various palm orientations).
%
\citet[\page 109]{Kendon:Versante:2003} claim that \enquote{the form of pointing adopted provides information about how the speaker wishes the object being indicated to be regarded}.
%
For instance, pointing with the thumb is usually used when the precise location of the intended referent is not important \citep[\page 121--125]{Kendon:Versante:2003}, while the typical use of index finger palm down pointing is to single out an object \citep[\page 115]{Kendon:Versante:2003}. 
%
Open hand pointing has a built-in metonymic function since the object pointed at is introduced as an example for a issues related to the current disourse topic (the \emph{question under discussion} \is{question under discussion} \citep{Ginzburg:2012}).
%
For instance, with open hand palm vertical one can indicate the type of the object pointed at \citep[\page 126]{Kendon:Versante:2003}.
% \item interactive (dialogue-oriented)
%%%
\item beats \is{beat gesture} (rhythmic gestures, \isi{baton}). Hand and arm movements that are coupled to the intonational or rhythmic contour of the accompanying speech. Beats lack representational content but are usually used to an emphasising effect. \enquote{The typical beat is a simple flick of the hand or fingers up and down, or back and forth} \citep[\page 15]{McNeill:1992}. Hence, a beat is a gestural means to accomplish what is usually expressed by vocal stress in speech. 
%%%
\item emblem \is{emblem} \is{emblematic gesture} (lexicalized gestures). In contrast to the other classes, emblems are special in that they follow a fully conventionalized form-meaning relation. A common example in Western countries is the \isi{thumbs-up gesture}, signalling \enquote{approval or encouragement} (Merriam Webster online dictionary\footnote{\url{https://www.merriam-webster.com/dictionary/thumbs-up}, accessed \printdate{2018-08-20}. The fact that emblems can be lexicalized in dictionaries emphasizes their special, conventional status among gestures.}).
\end{itemize}


Reconsidering gestures that have been classified as beats, amongst others, \citet{Bavelas:Chovil:Lawrie:Wade:1992} observed that many of these stroke movements accomplish functions beyond rhythmic structuring or emphasis.
%
Rather, they appear to contribute to dialogue management and have been called \emph{interactive gestures}.
%
Therefore, one can add these gestures to the taxonomy:

\begin{itemize}
\item interactive gestures. \is{interactive gestures} Hand and arm movements that accomplish the function \enquote{of helping the inlocutors coordinate their dialogue} \citep[\page 394]{Bavelas:Chovil:Coates:Roe:1995}. Interactive gestures include pointing gestures that serve turn allocation (\enquote{go ahead, its your turn}) and gestures that are bound up with speaker attitudes or the relationship between speaker and addressee. Examples can be found in open palm/palm upwards gestures used to indicate the information status of a proposition (\enquote{as you know}) or the mimicking of quotation marks in order to signal a report of direct speech.
\end{itemize}


The gesture classes should not be considered as mutually exclusive categories, but rather as dimensions according to which gestures can be defined, allowing for multi-dimensional cross-classifications \citep{McNeill:2005,Gerwing:Bavelas:2004}.
%
For instance, it is possible to superimpose pointing gestures with iconic traits.
%
This has been found in the study on pointing gestures described in \citet{Kranstedt:Luecking:Pfeiffer:Rieser:Wachsmuth:2006:b}, where two participants a time were involved in an idenfication game: one participant pointed at one of several parts of a toy airplane scattered over a table, the other participant had to identify the pointed object.
%
When pointing at a disk (a wheel of the toy airplane), some participants used index palm down pointing, but additionally turned around their index finger in a circle -- that is, the pointing gesture not only locates the disk (deictic dimension) but also depicted its shape (iconic dimension).\footnote{Since the study was only concerned with purely deictic gesture, such mixed gesture have been ignored in analyses, however.}
%
See \citet{Ozyurek:2012} for an overview of various \isi{gesture classification} schemes.



In addition to classifying gestures according to the above-given functional groups, a further distinction is usually made with regard to the ontological place of their referent: representational and deictic gestures can relate to concrete or to abstract objects or scenes. 
%
For instance, an iconic drawing gesture can metaphorically display the notion \enquote{genre} via a \isi{conduit metaphor} \citep[\page 14]{McNeill:1992}:
%
\ea \label{ex:conduit-gesture}
It [was a Sylves]ter and Tweety cartoon \\
\textit{both hands rise up with open palm handshape, palms facing}; brackets indicate segments concurrent with the gesture stroke (see Figure~\ref{fig:gesture-phases}). 
\z 
%
The gesture in (\ref{ex:conduit-gesture}) virtually hold an object, thus depicting the abstract concept of the genre of being a Sylvester and Tweety cartoon as a bounded container.
%
Likewise, pointing into a location within gesture space as a proxy for a real-world place is a point in case.
%
An example is provided in Figure~\ref{fig:drivetowards} below.
%
Accordingly, gestures can be cross-classified into concrete and \emph{abstract} or \emph{metaphorical} \is{metaphorical gesture} \is{abstract gesture} ones (see the volume of \citet{Cienki:Mueller:2008} on gesture and metaphor).


On the most basic, kinematic level the movement of a prototypical gesture follows an \enquote{anatomic triple}: gestures have to be partitioned into at least a preparation, a stroke, and a retraction \is{preparation phase} \is{gesture stroke} \is{stroke phase} \is{retraction phase} phase \citep{Kendon:1972}.
%
The gesture phases are shown in the diagram in Figure~\ref{fig:gesture-phases}.
%
The stroke is the movement part that carries the gesture's meaning. 
%
Preparation and retraction phase bring hand and arms into respectively out of the stroke. 
%
Unless stated otherwise, when talking about gestures what follows, the stroke phase, which is the \enquote{gesture proper} or the \enquote{semantic interpretable} phase, is referred to.

\begin{figure}[tb]
  \centering
  \begin{tikzpicture}[
    gp/.style={rectangle, align=center, text depth=0.25ex, text height=1.5ex, font=\sffamily, inner sep=5pt}
    ]
    \matrix[ampersand replacement=\&, inner sep=0.3cm] (gesture phases) {
      \node [gp, fill=GoldenGate2] (prep) {preparation}; \& 
      \node [gp, fill=GoldenGate5, text width=3cm, text=white] (stroke) {stroke}; \&
      \node [gp, fill=GoldenGate7] (hold) {post-stroke hold}; \&
      \node [gp, fill=GoldenGate4] (retr) {retraction}; \\
    };
    \draw [->, thick] (gesture phases.south west) -- (gesture phases.south east) node [midway, below, font=\sffamily\footnotesize] {timeline};
    \foreach \p in {0,0.1,0.2,...,0.9} {
      \path (gesture phases.south west) -- (gesture phases.south east) node [coordinate, pos=\p] (a) {};
      \draw [thin] (a) -- + (90:0.2cm);
    }
  \end{tikzpicture}  
  \caption{Gesture phases}
  \label{fig:gesture-phases}
\end{figure}


In any case, the spontaneous, usually co-verbal hand and arm movement considered in this chapter are different from the signed signs of sign languages \is{sign language} (see \crossrefchaptert{sign-lg}) and pantomime \is{pantomime} (not spontaneous and co-verbal).\footnote{In languages like German the difference between free gesticulation and sign language signs is also reflected terminologically: the former are called \textit{Gesten}, the latter \textit{Gebärden}.}




\section{Gestures in HPSG}
\label{sec:gestures-hpsg}

Integrating a gesture's contribution to speech was initiated in computer science \citep{Bolt:1980}.
%
Coincidentially, these early works used typed feature structure description akin to HPSG grammars.
%
Though linguistically limited, the crucial invention has been a \emph{multimodal chart parser}, \is{multimodal chart parser} that is, an extension of chart parsing that allows to process input on two modalities (namely speech and gesture).
%
Such approaches are reviewed in Section~\ref{sec:precursors}.
%
Afterwards, a more elaborate gesture representation format is introduced that allows to encode the observable form side of a gesture in terms of kinematically derived attribute-value structures (Section~\ref{sec:repr-gest-with}).
%
Following the basic semiotic distinction between deictic (or indicating or pointing) gestures and iconic (or representational or imagistic) gestures, the analysis of both classes of gestures is exemplified in Sections~\ref{sec:pointing-gestures} and \ref{sec:iconic-gestures}, respectively.
%
To begin with, however, some basic phenomena that should be covered by a multimodal grammar are briefly summarized in Section~\ref{sec:empir-desid-gramm}.



\subsection{Basic empirical phenomena of grammatical gesture integration}
\label{sec:empir-desid-gramm}

\is{speech-gesture integration|(}
With regard to grammar-gesture integration, three main phenomena have to be dealt with:
%
\begin{itemize}
\item What is the meaning of a gesture? How to assign semantic representations or truth conditions to hand and arm movements? \is{gesture meaning}
\item What is the affiliate of a gesture, that is, its verbal attachment site? \is{gesture affiliation} \is{affiliate}
\item What is the result of multimodal integration, that is, the outcome of composing verbal and non-verbal meanings? \is{multimodal integration}
\end{itemize}

Given the linguistic significance of gestures as sketched in the preceding sections, grammar-oriented accounts on speech-gesture integration have recently been developed that try to deal with (at least one of) the three basic phenomena, though with different settings of priorities, including
%
\citet{Alahverdzhieva:2013}, % diss 
%
\citet{Alahverdzhieva:Lascarides:2010},
%
\cite{Ebert:2014:a},
%
\citet{Giorgolo:2010},
%
\citet{Giorgolo:Asudeh:2011},
%
\citet{Luecking:2013:a,Luecking:2016},
%
\citet{Rieser:2008,Rieser:2011,Rieser:2015},
%
\citet{Rieser:Poesio:2009},
%
\citet{Schlenker:2018}.
\is{speech-gesture integration|)}


The HPSG-related approaches are briefly reviewed below.
%
For grammars for sign languages see \crossrefchaptert{sign-lg}. 
%
There also belong investigations into iconic features of \ili{American Sign Language} \citep{Schlenker:Lamberton:Santoro:2013,Schlenker:2014}.



\subsection{Precursors} 
\label{sec:precursors}

Using typed feature structure descriptions in order to represent the form and meaning of gestures goes back to computer science approaches to human-computer interaction (HCI). % \citep[cf.][]{Luecking:Pfeiffer:2012}.
%
The \textit{QuickSet} system \is{QuickSet system} \citep{Cohen:et:al:1997} allows users to operate on a map and move objects or create barb wires (the project was funded by a grant from the US army) by giving verbal commands and manually indicating coordinates.
%
The system processes voice and pen (gesture) input by assigning signals from both media representations in the form of attribute-value matrices (AVMs) \citep{Johnston:1998,Johnston:et:al:1997}.
%
For instance, \textit{QuickSet} will move a vehicle to a certain location on the map when asked to \emph{Move this}[\Pointing] \emph{motorbike to here}[\Pointing], where \enquote*{\Pointing} represents an occurrence of touch gesture (i.e., pen input). 


\is{multimodal chart parser|(}
Since a linguistic unification-based grammar rests on a conventional, \enquote{unimodal} parser, \citet{Johnston:1998} and \citet{Johnston:et:al:1997} developed a \emph{multimodal chart parser}, which is still a topic of computational linguistics \citep{Alahverdzhieva:Flickinger:Lascarides:2012} (see also \crossrefchaptert{cl}).
%
A multimodal chart parser consists of two or more layers and allows for layer-crossing charts.
%
The multimodal NP \emph{this}[\Pointing] \emph{motorbike}, for instance, is processed in terms of a multimodal chart parser covering of a speech (s) and a gesture (g) layer:
%
\ea \label{ex:multimodal-chart-parser}
\begin{tikzpicture}[
  baseline, 
  node distance=3cm, 
  shorten >=1pt,
  >=latex,
  State/.style={circle, fill=black, minimum size=6pt, inner sep=0pt}
  ]
  \node (speech) {s:};
  \node [State] (q_0) [right=of speech, label=below:0] {};
  \node [State] (q_1) [right=of q_0, label=below:1]    {};
  \node [State] (q_2) [right=of q_1, label=below:2]    {};
  \path [->] (q_0) edge [bend left]  node [above] {\textsc{det}}  (q_1)
                   edge              node [below] {\textit{this}} (q_1)
                   edge [loop above, min distance=1cm] node {\textsc{np}$\rightarrow$\textsc{.det n}} (q_0)
             (q_1) edge [bend left]  node [above] {\textsc{n}}    (q_2)
                   edge              node [below] {\textit{motorbike}} (q_2);
  \begin{scope}[yshift={-1.5cm}]
    \node (gesture) {g:};
    \node [State] (q_3) [right=5cm of gesture, label=below:3] {};
    \node [State] (q_4) [right=of q_3, label=below:4]     {};
    \path [->] (q_3) edge             node [above] {\Pointing}          (q_4)
                     edge [bend right] node [below] {\textit{pointing}} (q_4);
  \end{scope}
\end{tikzpicture}
\z

A multimodal chart or \emph{multichart} \is{multichart} is defined in terms of sets of identifiers from both layers.
%
Possible multicharts from (\ref{ex:multimodal-chart-parser}) include the following ones:
%
\ea
multichart 1: \{[s,0,1], [g,3,4]\} \\
multichart 2: \{[s,1,2], [g,3,4]\} \\
\ldots
\z
\is{multimodal chart parser|)}

% The meaning (\textit{content}) of a gesture of category (\textit{cat}) \textit{spatial\_gesture}  like \enquote*{\Pointing} is represented as a \textit{latitude-longitude} coordinate pair \citep{Johnston:1998}:
% %
% \ea
% \begin{avm}
% \[cat: & \textit{spatial\_gesture} \\
%   content: & \[fsType: & \textit{point} \\
%                coord: & \textit{latlong}$(x,y)$\]
% \]
% \end{avm}
% \z


The basic rule for integrating spatial gestures with speech commands is the \emph{basic integration scheme} \citep{Johnston:1998,Johnston:et:al:1997}, reproduced in (\ref{ex:basic-integration-scheme}): \is{basic integration scheme} \is{multimodal integration scheme}
%
\ea \label{ex:basic-integration-scheme}
\begin{avm}
  \[lhs : & \[cat : & \textit{command} \\
 	modality : & \@2 \\
  	content : & \@1 \\
    time : & \@3\] \\
    rhs : & \[dtr1 : & \[cat : & \textit{located\_command} \\
	modality : & \@6 \\
    content : & \@1\textup{[}location \@5\textup{]} \\
    time : & \@7\] \\
    dtr2 : & \[cat : & \textit{spatial\_gesture} \\
    content : & \@5 \\
    modality :& \@9 \\
    time :& \@{10}\]\] \\
    constraints : & \{overlap(\@7,\@{10}) $\lor$ follow(\@7,\@{10},\texttt{4}s) \\
    total-time(\@7,\@{10},\@3) \\ 
    assign-modality(\@6,\@9,\@2)\}\]
\end{avm}
\z

The AVM in (\ref{ex:basic-integration-scheme}) implements a mother-daughter structure along the lines of a context free grammar rule, where a left-hand side (\textsc{lhs}) expands to a right-hand side (\textsc{rhs}).
%
The right-hand side consists of two constituents (daughters \textsc{dtr1} and \textsc{dtr2}), a verbal expression (\textit{located\_command}) \istype{located\_command} and a gesture.
%
The semantic integration between both modalities is achieved in terms of \isi{structure sharing}, see tag \avmbox{5}: 
%
the spatial gesture provides the location coordinate for the verbal command. 

The \isi{bimodal integration} is constrained by a set of restrictions, mainly regulating the \isi{temporal relationship} between speech and gesture (see tags \avmbox{7} and \avmbox{10} in the \textsc{constraints} set): 
%
the gesture may overlap with its affiliated \is{affilate} word in time, or follow it in at most four seconds.


An integration scheme akin to that displayed in (\ref{ex:basic-integration-scheme}) also underlies current grammar-oriented approaches to deictic and iconic gestures (see Sections~\ref{sec:pointing-gestures} and \ref{sec:iconic-gestures} below).



\subsection{Representing gestures with AVMs}
\label{sec:repr-gest-with}

\is{gesture representation|(}
Representing the formal features of gestures in terms of attribute-value matrices has been initiated in robotics \citep{Kopp:Tepper:Cassell:2004}. 
%
A representation format that captures the \enquote{phonological}, physical-kinematic properties of a gesture is designed according to the moveable junctions of arms and hands.
%
For instance, the representation of the gesture in Figure~\ref{fig:staircases} according to the format used in \citet{Luecking:Bergmann:Hahn:Kopp:Rieser:2010} is given in (\ref{ex:gesture-representation}):

\ea \label{ex:gesture-representation}
\begin{avm}
  \[\asort{right hand}
    handshape &\[shape & G \\ 
      path & 0 \\
      dir & 0 \] \\
    palm & \[orient & PAB>PAB/PUP>PAB\\
      path & 0 \\
      dir & 0 \] \\
    boh & \[orient & BUP>BTB/BUP>BUP \\
      path & arc>arc>arc \\
      dir & MR>MF>ML\] \\
    wrist & \[position & P-R\\
      path & line \\
      dir & MU \\
      dist & D-EK\\
      extent & small\]  \\
    sync & \[config & BHA \\
      rel.mov & LHH \]
  \]
\end{avm}
\z

The formal description of a gestural movement is given in terms of the \isi{handshape}, \isfeat{handshape} the orientations of the palm \isfeat{palm} and the back of the hand (\textsc{boh}), \isfeat{boh} the \isi{movement trajectory} (if any) of the wrist \isfeat{wrist} and the relation between both hands (synchronicity, \textsc{sync}).\isfeat{sync} 
%
The \isi{handshape} is drawn from the \isi{fingerspelling alphabet} of \ili{American Sign Language}, as illustrated in Figure~\ref{fig:asl}.
%
The orientations \isfeat{orient} of palm and back of hand are specified with reference to the speaker's body (e.g., \textit{PAB} encodes \enquote{palm away from body} and \textit{BUP} encodes \enquote{back of hand upwards}). 
%
Movement features are specified with respect to the wrist: the starting position \isfeat{position} is given and the performed trajectory is encoded in terms of the described path \isfeat{path} and the direction \isfeat{dir} and extent \isfeat{extent} of the movement.
%
Position and extent are given with reference to the \emph{gesture space}, that is the structured area within the speaker's immediate reach \citep[\page 86--89]{McNeill:1992} -- see the left-hand side of Figure~\ref{fig:gesture-space}. \is{gesture space}
%
Originally, McNeill considered the gesture space as \enquote{a shallow disk in front of the speaker, the bottom half flattened when the speaker is seated} \citep[\page 86]{McNeill:1992}. 
%
However, acknowledging also the distance of the hand with respect to the speaker's body (feature \textsc{dist}) turns the shallow disk into a volume, giving rise to the three-dimensional model displayed in the right-hand side of Figure~\ref{fig:gesture-space}.
%
The gesture space regions known as \emph{center-center}, \emph{center} and \emph{periphery}, possible modified by location modifiers (\emph{upper right}, \emph{right}, \emph{lower right}, \emph{upper left}, \emph{left}, \emph{lower left}), are now modeled as nested cuboids. 
%
Thus, gesture space is structured according to all three body axes: the sagittal, the longitudinal, and the transversal axis.
%
Annotations straightfowardly transfer to the three-dimensional gesture space model.
%
Such a three-dimensional gesture space model is assumed throughout this chapter.
%
Rectangular and roundish movement are distinguished in terms of \textit{line} \istype{line} or \textit{arc} \istype{arc} values of feature \textsc{path}.\isfeat{path}
%
An example is given in Figure~\ref{fig:move-direction}.
%
For a documentation of gesture representation along this line see \citet{FIGURE:annotation}.
%
For related formats see \citet{Martell:2002} and \citet{Gibbon:et:al:2003}.
%
For a different approach see \citet{Lausberg:Sloetjes:2009}.


\begin{figure}[tb]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/Asl-alphabet-gallaudet.png}
  \caption[ASL]{American Sign Language fingerspelling alphabet (image released to Public Domain by user Ds13 in the English Wikipedia at \printdate{2004-12-18}, \url{https://commons.wikimedia.org/wiki/File:Asl_alphabet_gallaudet.png}).}
  \label{fig:asl}
\end{figure}

\begin{figure}[tb]
  \centering
  % \includegraphics[width=0.5\linewidth]{figures/gesture-space}
  \begin{tikzpicture}
  \tdplotsetmaincoords{80}{110}
  \begin{scope}[tdplot_main_coords]
      % manikin:
    \begin{scope}[xshift=1.35cm, yshift=-0.15cm,scale=5]
      \fill[GoldenGate1] (0,0.05,0) -- (0,0.125,0) -- (0,0.21,0.4) -- (0,0.28,0)
      -- (0,0.37,0) -- (0,0.28,0.57) -- (0,0.29,0.66) -- (0,0.37,0.42) -- (0,0.42,0.43) [rounded corners=2pt] -- (0,0.32,0.8) -- (0,0.2,0.83) -- (0,0.07,0.8) [sharp corners] -- (0,0,0.41) -- (0,0.05,0.41) -- (0,0.12,0.66) -- (0,0.13,0.57) -- cycle; 
      \fill[GoldenGate1] (0,0.2,0.92) ellipse (0.06cm and 0.08cm);
    \end{scope}
    
    %% \GGS{<x-start>}{<y-start>}{<z-top>}{<z-bottom>}{<Farbe>}{<x-width>}{<y-depth>}{<opacity>}
    \GGS{1}{3}{3.75}{3}{GoldenGate4}{1}{1}{0.5}
    \GGS{1}{2.5}{4}{2.75}{GoldenGate3}{2}{1.5}{0.4}
    \GGS{1}{2}{4.5}{2.75}{GoldenGate2}{3}{3}{0.3}
  \end{scope}


  \begin{scope}[xshift=-5.5cm, yshift=-0.3cm, scale=5]
    \fill[GoldenGate1] (0.05,0) -- (0.125,0) -- (0.21,0.4) -- (0.28,0)
    -- (0.37,0) -- (0.28,0.57) -- (0.29,0.66) -- (0.37,0.42) -- (0.42,0.43) [rounded corners=2pt] -- (0.32,0.8) -- (0.2,0.83) -- (0.07,0.8) [sharp corners] -- (0,0.41) -- (0.05,0.41) -- (0.12,0.66) -- (0.13,0.57) -- cycle; 
    \fill[GoldenGate1] (0.2,0.92) ellipse (0.06cm and 0.08cm);

    \node [rectangle, inner sep=0pt, text width=1cm, minimum height=0.8cm, fill=GoldenGate6, align=center] (cc) at (0.2,0.65) {};
    \node [font=\footnotesize\sffamily, align=center] at (cc) {center-\\center};

    \node [rectangle, text width=1.85cm, minimum height=1.5cm, draw=GoldenGate5, thick] (cent) at (0.2,0.65) {};
    \node [font=\footnotesize\sffamily, anchor=north, text=GoldenGate2] at (cent.north) {center};

    \node [rectangle, text width=2.8cm, minimum height=2.5cm, draw=GoldenGate3, thick] (peri) at (0.2,0.65) {};
    \node [font=\footnotesize\sffamily, anchor=north west, xshift=-2pt] at (peri.north west) {periphery};

    \matrix [
    matrix of nodes,
    nodes={font=\footnotesize\sffamily\itshape},
    right=of cc,
    row sep=0.7cm
    ] (left) {
      upper left\vphantom{g} \\
      left\vphantom{g} \\
      lower left\vphantom{g} \\
    };

    \matrix [
    matrix of nodes,
    nodes={font=\footnotesize\sffamily\itshape},
    left=of cc,
    row sep=0.7cm
    ] (right) {
      upper right \\
      right \\
      lower right \\
    };
  \end{scope}
\end{tikzpicture}
  \caption[Gesture Space]{Gesture Space (left hand side is simplified from \citet[\page 89]{McNeill:1992}.) Although originally conceived as a structured \enquote{shallow disk} \citet[\page 86]{McNeill:1992}, adding distance information gives rise to a three-dimensional gesture space model as illustrated on the right-hand side and assumed in the main text.}
  \label{fig:gesture-space}
\end{figure}


\begin{figure}[tb]
  \centering
  \begin{tikzpicture}[baseline, move/.style={font=\footnotesize, midway, text=black}]
    \draw [thick, ->] (0,-1) -- node [move, left] {MF} (0,0) -- node [move, above] {MR} node [move, below] {line} (2,0) -- node [move, right] {MB} (2,-1);
    \begin{scope}[xshift=4.25cm]
      \draw [thin, gray] (0,-1) -- node [move, left] {MF} (0,0) -- node [move, above] {MR} node [move, below] {arc} (2,0) -- node [move, right] {MB} (2,-1);
      \draw [thick, ->] (0,-1) arc[start angle=180, end angle=0, radius=1];
    \end{scope}
  \end{tikzpicture}
  \caption{The same sequence of direction labels can give rise to an open rectangle or a semicircle, depending on the type of concatenation \citep[\page 385]{Luecking:2016}.}
  \label{fig:move-direction}
\end{figure}


% The \enquote{phonological} representation of gestures is slightly refined in \citet[\page 385--386]{Luecking:2016}, where gestures are analysed in terms of a formal event framework \is{gesture event} (\cite{Fernando:2011}, \citet[Sec.~2.7]{Cooper:Ginzburg:2015}) in order to account for \isi{Gestalt properties} of gesture movements (see also the example discussed by \citet{Rieser:2008}).
\is{gesture representation|)}




\subsection{Pointing Gestures}
\label{sec:pointing-gestures}

\is{pointing gesture|(} \is{deictic gesture|(}
Pointing gestures are \emph{the} referring device:
%
they probably pave a way to reference in both an evolutionary and language acquisition perspective. \citep{Bruner:1998,Masataka:2003,Matthews:Behne:Lieven:Tomasello:2012};
%
they are predominant inhabitants of the \enquote{deictic level} of language, interleaving the symbolic (and the iconic) ones (\citet{Levinson:2008}, see also \citet{Buehler:1934:ORIG});
%
they underlie reference in \textit{Naming Games} in computer simulation approaches \citep{Steels:1995} (for a semantic assessment of naming and categorisation games see \citet{Luecking:Mehler:2012}). \is{naming game} \is{categorisation game}
%
The referential import of pointing gesture has been studied experimentally to some detail \citep{Bangerter:Oppenheimer:2006,Kranstedt:Luecking:Pfeiffer:Rieser:Wachsmuth:2006:a,Kranstedt:Luecking:Pfeiffer:Rieser:Wachsmuth:2006:b,van:der:Sluis:Krahmer:2007}. 
%
As a result, it turns out that pointings do not rest on a direct \enquote{laser}- or \enquote{beam} mechanism \citep{McGinn:1981}.
%
Rather, they serve a (more or less rough) locating function \citep{Clark:1996} that can be modelled in terms of a \emph{pointing cone} \citep{Luecking:Pfeiffer:Rieser:2015}. \is{pointing cone}
%
These works provide an answer to the first basic question (cf. Section~\ref{sec:empir-desid-gramm}): pointing gestures have a \enquote{spatial meaning} which focuses or highlights a region in relation to the direction of the pointing device.
%
Such a spatial semantic model has been introduced in \citet{Rieser:2004} under the name of \emph{region pointing}, where the gesture adds a locational constraint to the restrictor of a noun phrase.
%
In a related way, two different functions of a pointing gesture have been distinguished by \citet{Kuehnlein:Nimke:Stegmann:2002}, namely singling out an object (\ref{ex:object}) or making an object salient (\ref{ex:restrictor}).
%


\ea \label{ex:pointing-functions}
\ea \label{ex:object}
$\lambda F \lambda x (x = c \wedge F(x))$
\z
\ea \label{ex:restrictor}
$\lambda F \lambda x (\textit{salient}(x) \wedge F(x))$
\z 
\z

The approach is expressed in a lambda calculus and couched in a HPSG framework.
%
The derivation of the instruction \textit{Take the red bolt} plus pointing gesture is exemplified in (\ref{ex:take}).
%
A pointing gestures is represented by means of \enquote{$\searrow$} and takes a syntactic position within the linearised inputs according to the start of the stroke phase. 
%
For instance, the pointing gesture in (\ref{ex:serial}) occurred after \textit{the} has been articulated but before \textit{red} was finished.
%
The derivation of the multimodal N$'$-constituent is shown in (\ref{ex:take-tree}).

\ea \label{ex:take}
\ea \label{ex:serial} 
Take [the $\searrow$[N$'$ [N$'$ red bolt]]]
\z
\ea \label{ex:take-tree}
\begin{forest}
baseline
[\begin{avm}
    \[syn & \[\asort{gram-cat} 
              head & \@1 \\
              spr & \q< \@2 \@> \\
              comps & \avmel \] \\
     sem & $\lambda x (\textit{salient}(x) \wedge \textit{red}(x) \wedge \text{bolt}(x))$
    \]
   \end{avm}
  [\begin{avm}
    \[syn & \[\asort{gram-cat} 
              head & \[\asort{deictic}
                        mod & \q< \@3 \q> \\
                        func & restrictor \] \\
              spr & \avmel \\
              comps & \avmel \] \\
     sem & $\lambda F \lambda x (\textit{salient}(x) \wedge F(x))$
    \]
   \end{avm}
  [$\searrow$]]
  [\begin{avm}
    \@{3}\[syn & \[\asort{gram-cat} 
              head & \@1 \\
              spr & \q< \@2 \q> \\
              comps & \avmel \] \\
     sem & $\lambda y (\textit{red}(y) \wedge \text{bolt}(y))$
    \]
   \end{avm}
  [red bolt, roof]]
]
\end{forest}
\z
\z


The spatial model is also adopted in \citet{Lascarides:Stone:2009:a}, where the region denoted by a pointing is represented by a vector $\vec{p}$.
%
This region is an argument to function $\nu$, however, which maps the projected cone region to $\nu(\vec{p})$, the space-time talked about, which may be different from the gesture space (many more puzzles of local deixis are collected by \citet{Klein:1978} and \citet{Fricke:2007:a}).


Let us illustrate some aspects of pointing gesture integration by means of a real world example (taken from dialogue V5 of the SaGA corpus \citep{Luecking:Bergmann:Hahn:Kopp:Rieser:2010}).
%
The speaker in (\ref{ex:drivetowards}) and Figure~\ref{fig:drivetowards} places a pond with his left hand into gesture space.
%
He then points at his left hand/the pond (on \emph{dual points} see \citet{Goodwin:2003}, on \emph{deferred reference} see \citet{Quine:1950,Nunberg:1993}), depicting \textit{driving towards} or \textit{heading straight}.
%
The movement aspect is subsequently enacted by moving the right hand/driver towards the left hand/pond.
%
\ea \label{ex:drivetowards}
\glt wenn du dort eingefahren bist, fährst Du geradeaus auf einen Teich zu .. 'n Teich .. und .. [\textit{gesture from Figure~\ref{fig:drivetowards} occurs here}] 
\glt \textit{when you drive in there , you're heading straight for a pond .. a pond .. and .. } [\textit{gesture from Figure~\ref{fig:drivetowards} occurs here}] 
\z 

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/draufzu}
  \caption{Driving towards/heading straight}
  \label{fig:drivetowards}
\end{figure}

This example highlights two crucial phenomena: firstly, the region pointed at is not the region referred to. 
%
The indicated location in gesture space has to be mapped onto a spatial area of the described situation.
%
Such mapping are accounted for by $\nu(\vec{p})$.
%
Secondly, the gesture is produced \emph{after} speech. 
%
This means that in order to identify the gesture's verbal attachment site something beyond mere temporal synchrony (the second basic question, see Section~\ref{sec:empir-desid-gramm}) has to be exploited.
%
To this end, phonological information on top  of the basic integration scheme in (\ref{ex:basic-integration-scheme}) are used \citep{Alahverdzhieva:Lascarides:2011,Luecking:2013:a}.
%
Within a temporal window (basic integration scheme), a pointing gesture is affiliated to a stressed expression. \is{phonetic speech-gesture constraint}
%
This relaxation captures also post-speech gestures \is{post-speech gesture} as in example (\ref{ex:drivetowards})/Figure~\ref{fig:drivetowards}.
%
Within HPSG, such constraints can be formulated within an interface to metrical trees from the phonological model of \citet{Klein:2000} or phonetic information packing from \citet{Engdahl:Vallduvi:1996} -- see also \crossrefchaptert{phonology}. 
%
In order to deal with gestures that are affiliated with expressions that are larger than single words, \citet{Alahverdzhieva:Lascarides:2011} also develop a phrase or sentence level integration scheme, where the stressed element has to be a semantic head (in the study of \citet{Mehler:Luecking:2012:d} 18.8\% of the gestures had a phrasal affiliate).
%
On this account, semantic integration of gesture location and verbal meaning (the third basic question from Section~\ref{sec:empir-desid-gramm}) is brought about using underspecification mechanism of \emph{Minimal Recursion Semantics} \is{Minimal Recursion Semantics} (MRS) \citep{Copestake:Flickinger:Pollard:Sag:2005}, more precisely, \emph{Robust Minimal Recursion Semantics} \is{Robust Minimal Recursion Semantics} (RMRS), a refinement of MRS where basically scope as well as arity of elementary expressions is underspecified \citep{Copestake:2007}.


\textcolor{red}{example here}
The basic integration scheme is the \emph{Situated Prosodic Word Constraint} \is{Situated Prosodic Word Constraint} \citep[\page 445]{Alahverdzhieva:Lascarides:Flickinger:2017}:
%
\ea
\begin{avm}
\[\asort{word} 
overlap & \<@7, @8\> \\
time & \textup{@7 $\cup$ @8} \\
phon & \textup{@3} \\
synsem & \[cat & \textup{@5} \\ 
           cont & \[rels & \avmbox{C\textsubscript{rel}} $\oplus$ \avmbox{S\textsubscript{rel}} $\oplus$ \avmbox{G\textsubscript{rel}} \\
                    hcons & \avmbox{S\textsubscript{hc}} $\oplus$ \avmbox{G\textsubscript{hc}}\] 
         \] \\
s-dtr & \[\asort{word}
          time & \textup{@7} \\
          phon & \textup{@3}nuclear\_or\_pre-nuclear \\
          synsem & \[cat & \textup{@5} \\
                     cont & \[rels & \avmbox{S\textsubscript{rel}} \\
                              hcons & \avmbox{S\textsubscript{hc}}hcons\]
                   \]
        \] \\
g-dtr & \[\asort{depicting\_or\_deictic}
          time & \textup{@8} \\
          synsem & \[cat & \[g-feature & value \\ \ldots\] \\
                     cont & \[rels & \avmbox{G\textsubscript{rel}} \\
                              hcons & \avmbox{G\textsubscript{hc}}\]
                   \]
        \] \\
c-cont.rels & \avmbox{C\textsubscript{rel}}
\]
\end{avm}
\z 

Pointing gestures that occur in company to a demonstrative expression in speech have received special attention, since they manifest the transition point between the symbolic realm of language and the deictic realm of perception \citep{Levinson:2008,Fricke:2012}.
%
Building on this phenomenon, \citet{Ebert:2014:a} argue within in a dynamic semantic framework that co-demonstrative gestures \is{co-demonstrative gesture} contribute to at-issue content while other gestures % (called \enquote{contingent} in \citet{Luecking:2013:a} and \enquote{background} in \citet{Cooperrider:2017}) 
contribute to non-at-issue content.
%
Within a presuppositional framework, this picture has been refined due to gestures that are entailed in local contexts in terms of \enquote{cosuppositions} \citet{Schlenker:2018}.
%
A dialogue-oriented focus is taken in \citet{Luecking:2018:a}: here pointing gesture play a role in formulating processing instructions that guide the addressee in understanding demonstrative noun phrases (see also \crossrefchaptert{pragmatics}).

% Semantic aspects of iconic features of pointing as discourse referent indication in American Sign Language have been investigated and formally modelled by \citet{Schlenker:Lamberton:Santoro:2013,Schlenker:2014}.
\is{pointing gesture|)} \is{deictic gesture|)}



\subsection{Iconic Gestures}
\label{sec:iconic-gestures}

\is{iconic gesture|(} \is{representational gesture|(}
There is nearly no semantic work on how and which meanings can be assigned to iconic gestures (first basic question from Section~\ref{sec:empir-desid-gramm}).
%
Semantic modelling usually focuses on the interplay of (presupposed) gesture content with speech content, that is, with the third of the basic questions from Section~\ref{sec:empir-desid-gramm}.
%
\citet[\page 296]{Schlenker:2018} is explicit in this respect: \enquote{It should be emphasized that we will not seek to explain how a gesture [\ldots] comes to have the content that it does, but just ask how this content interacts with the logical structure of a sentence.}\footnote{The omission indicated by \enquote{[\ldots] just contains a reference to an example from Schlenker's paper.}}
%
Two exceptions, however, can be found in the approaches of \citet{Rieser:2010} and \citet{Luecking:2013:a,Luecking:2016}.
%
\citet{Rieser:2010} tries to extract a \enquote{depiction typology} out of a speech-and-gesture corpus where formal gesture features are correlated with topological clusters consisting of geometrical constructs. \is{gesture typology}
%
These geometrical objects are used in order to provide a possibly underspecified semantic representation for iconic gestures which is integrated into word meaning \citep{Hahn:Rieser:2010,Rieser:2011}.
%
\citet{Luecking:2013:a,Luecking:2016} draws on Goodman's notion of \emph{exemplification} \citep{Goodman:1976}, that is, iconic gestures are connected to semantic predicates in terms of a reversed denotation relation: the meaning of an iconic gesture is given in terms of the set of predicates which have the gesture event within their denotation. \is{exemplification}
%
In a further step, common perceptual features are extracted and represented as part of a lexical extension of lexemes. 
%
This conception is motivated by psychophysic theories of the perception of biological events \citep{Johansson:1973}, draws on philosophical \isi{similarity} conception beyond isomorphic mappings \citep{Peacocke:1987},\footnote{\label{fn:resemblance}That mere resemblance, usually associated with iconic signs, is too empty a notion to provide the basis for a signifying relation has been emphasised on various occasions \citep{Burks:1949,Bierman:1962,Eco:1976,Goodman:1976,Sonesson:1998}.} and, using a somewhat related approach at least, has been proven to work in robotics \citep{Sowa:2006:a}.
%
These perceptual features serve as the integration locus for iconic gestures, using standard unification techniques. 

\textcolor{red}{example here}

With regard to conventionalized sign language, iconic aspects have also been captured in terms of presuppositions that map, for instance, height specifications of signed loci to social properties and relations like power (that is, a higher locus indicates more power) \citep{Schlenker:Lamberton:Santoro:2013}.


The richer formal, functional and representational features of iconic gestures as compared to deictic gestures (cf. Section~\ref{sec:pointing-gestures}) is accounted for in \citet{Alahverdzhieva:Lascarides:2010} by assigning a formal predicate to each \enquote{phonological} feature of a gesture representation (cf. Section~\ref{sec:repr-gest-with}). 
%
These formal gesture predicates are highly underspecified, using \textit{Robust Minimal Recursion Semantics} (RMRS) \citep{Copestake:2007}.
%
That is, they can be assigned various (so far postulated) predicates (which are assumed to be constrained by iconicity \citep{Alahverdzhieva:Lascarides:2010}) with differing arity in the gesture resolution process.
%
Similar to pointing gesture integration, phonetic constraints on word and sentence level apply and semantic integration is achieved using underspecification techniques of \textit{Minimal Recursion Semantics} (MRS) \citep{Copestake:Flickinger:Pollard:Sag:2005}.
%
The received integrated representation are then resolved using the SDRT tools of \citet{Lascarides:Stone:2009:a}.

\textcolor{red}{example here}
Gestures are interpreted according to a type hierachy rooted by gesture form features.
%
For example, a circular movement of a hand is represented (i.e., annotated) in terms of the \textsc{hand-movement} attribute with value \enquote{circular}, as shown in (\ref{ex:hand-cicular}).
%
The attribute-value pair is mapped onto an RMRS predication \enquote{$\textit{hand\_movement\_circular}(i)$}.
%
outside of grammar \citep[\page 443]{Alahverdzhieva:Lascarides:Flickinger:2017}
%
\ea
\ea \label{ex:hand-cicular}
\begin{avm}
\[hand-movement & circular\]
\end{avm}
\z
\ea \label{ex:interpret-circular}
\begin{forest}
baseline
[{$\textit{hand\_movement\_circular}(i)$}
  [{$\textit{substance}(x') \wedge \textit{rotate}(e', x')$}]
  [{$ rotate(e', j', x')$}]
]
\end{forest}
\z
\z

Iconic gestures in particular are involved in a short-term dynamic phenomenon:
%
on repeated co-occurrence, iconic gestures and affiliated speech can fuse into a \emph{multimodal ensemble} \citep{Kendon:2004,Luecking:Mehler:Menke:2008,Mehler:Luecking:2012:d}. \is{multimodal ensemble} \is{speech-gesture ensemble}
%
The characteristic feature of such an ensemble is that their gestural part, their verbal part, or even both parts can be simplified without changing the meaning of the ensemble.
%
Ensembles, thus, are the result of a process \isi{sign formation} as studied, for instance, in \isi{experimental semiotics} \citet{Galantucci:Garrod:2011}.
%
Such \isi{grammaticalisation process} eventually might lead to conventional signs.
%
However, most conventional, emblematic everyday gesture seem to be the result of circumventing a \isi{taboo}: something you should not name is gesticulated \citep{Posner:2002}. \is{taboo gesture}
\is{iconic gesture|)} \is{representational gesture|)}



\section{Gesture and \ldots}
\label{sec:gesture-and}

Besides being of a genuine linguistic, theoretical interest, gesture studies are a common topic in various areas of investigation, some of which are briefly pointed at under the respective subsequent headings. 


\subsection{\ldots\ tools, annotation, corpora}

Since gesture are signs in the visual modality, they have to be videotaped.
%
Gesture annotation is carried out on the recorded video films.
%
The main tools that allow for video annotation are, in alphabetical order, \isi{Anvil} (\url{https://www.anvil-software.org/}, \citealp{Kipp:2014}), \isi{ELAN} (\url{https://tla.mpi.nl/tools/tla-tools/elan/}, Max Planck Institute for Psycholinguistics, The Language Archive, Nijmegen, The Netherlands, \citealp{Sloetjes:Wittenburg:2008}), and \isi{EXMARaLDA} (\url{https://exmaralda.org/}, \citealp{Schmidt:2012}).

Annotation should follow an annotation standard, which is specified in an annotation scheme.
%
Various annotation schemes for gestures and speech-gesture integration have been proposed, partly differing in annotation foci, including the following ones:
%
Annotation schemes that focus on form description and gestures classification in terms of a taxonomy like that introduced in Section~\ref{sec:kinds-gestures} havebeen developed by R. Breckenridge Church, published in the appendix of \citet{McNeill:1992}, CoGEST \citep{Gibbon:et:al:2003}, FORM \citep{Martell:Osborn:Friedman:Howard:2002}, and the SaGA annotation \citep{Luecking:Bergmann:Hahn:Kopp:Rieser:2013}.
%
The form of gestures and their timing with speech is the object of the coding scheme of \citet{Kipp:Neff:Albrecht:2007}.
%
An interaction-oriented scheme has been proposed by \citet{Allwood:et:al:2007}, which is formulated on the level of turns and dialogue management.
%
A detailed annotation scheme for the form and function of gestures has been formulated in terms of \enquote{annotation decision trees} within the NEUROGES system \citep{Lausberg:Sloetjes:2009}.


Annotated videos of real life interactions give rise to multimodal corpora. 
%
Among those that include data on gestures are the following ones.

The multimodal SmartKom corpus \citep{Schiel:Steininger:Tuerk:2003}, which grew out of the SmartKom project \citep{Wahlster:2006}, comprises recording sessions of various Wizard-of-Oz experiments (that is, human-computer interaction where the human participant is made believe that the system she or he interacts with is autonomous while in fact it is (at least partly) operated by another human).
%
Recordings are extended basically by a transliteration and labeling of natural speech, labeling of gestures, annotation of user states (first release). T
%
he first public release, SKP 1.0, contains 90 recording sessions of 45 users. 
%
The multimodal SmartKom corpus as well as further SmartKom resources are hosted at the \textit{Bavarian Archive for Speech Signals} (\url{https://www.bas.uni-muenchen.de/Bas/}).


The AMI Meeting Corpus \citep{Carletta:et:al:2006} consists of 100 hours of meeting recordings.
%
The meetings were recorded in English but include mostly non-native speakers. 
%
The AMI Meeting Corpus provides orthographic transcriptions, but also has a couple of further annotations, including dialogue acts, named entities, head gesture, hand gesture, gaze direction, movement and emotional states.


The SaGA (\enquote{Speech and Gesture Alignment}) corpus  consists of 24 route direction dialogues obtained after a bus ride through a virtual town \citep{Luecking:Bergmann:Hahn:Kopp:Rieser:2010}. 
%
Audio and video data from the direction giver were recorded. 
%
The SaGA corpus consists of 280 minutes video material containing \numprint{4961} iconic/deictic gestures, approximately \numprint{1000} discourse gestures and \numprint{39435} words \citep{Luecking:Bergmann:Hahn:Kopp:Rieser:2013}.
%
Gesture annotation has been carried out in great detail, following a kinematic, form-based approach (cf. the above-given remark on annotation schemes).
%
Part of the SaGA corpus is available from the \textit{Bavarian Archive for Speech Signals} (\url{https://www.bas.uni-muenchen.de/Bas}).


The DUEL (\enquote{Disfluency, exclamations and laughter in dialogue}) corpus \citep{Hough:Tian:de:Ruiter:Betz:Kousidis:Schlangen:Ginzburg:2016} comprises 24 hours of natural, face-to-face dialogue in German, French and Mandarin Chinese.
%
It includes audio, video and body tracking data and is transcribed and annotated for disfluency, laughter and exclamations.


The FIGURE (derived from \enquote{Frankfurt Image GestURE}) corpus \citep{Luecking:Mehler:Walther:Mauri:Kurfuerst:2016} is built on recordings of 50 participants spontaneously producing gestures in response to five or six terms from a total of 27 stimulus terms, which have been compiled mainly from image schemata \citep{Lakoff87a-u}.
%
The gestures has been kinematically annotated by means of a variant of the SaGA annotation scheme.
%
The FIGURE annotation is available from the Text Technology Lab Frankfurt (\url{https://www.texttechnologylab.org/applications/corpora}).



\subsection{\ldots\ virtual agents}

generation of multimodal behaviour
\citep{Cassell:Stone:Yan:2000}

markup language for representing multimodal utterances and timing for generation MURML \citep{Kranstedt:Kopp:Wachsmuth:2002:b}

EMMA: Extensible MultiModal Annotation \citep{Johnston:2009}



\subsection{\ldots\ learning}

Following a \enquote{gesture as a window to the mind} view, gestures must be a prime object of educational theory and practice, and they are indeed, as demonstrated by research of \citet{Cook:Goldin-Meadow:2006} and colleagues.
%
Effectiveness of gestures has been studied in math lessons \citep{Goldin-Meadow:Nusbaum:Kelly:Wagner:2001}, in the acquisition of counting competence \citep{Alibali:DiRusso:1999}, and in bilingual education \citep{Breckinridge-Church:Ayman-Nolley:Mahootian:2004}, amongst others.
%
The rather unanimous result is that gestures can indeed reflect concepts and provides insights into cognitive processes involved in learning.
%
Therefore, they can be used as a teaching device as well as an indicator of learning progress and understaning. 


\subsection{\ldots\ aphasia}

Current models of utterance production are speech-gesture production models, assuming a (more or less) integrated generation of multimodal utterances.
%
Based on such models, one expects an effect of gesture performance when speech production is impaired, as is the case with aphasic speakers. 
%
Aphasia \is{aphasia} is an acquired speech disorder, which can be caused by a stroke, ischaemia, haemorrhage, craniocerebral trauma, and further brain damaging diseases.

\is{speech-gesture production models|(}
With the exception of the growth point theory, speech and gesture production models are based on Levelt's \citeyear{Levelt:1989} \ia{Levelt} model.

The \emph{Growth Point model} \citep{McNeill:Duncan:2000} assumes that the \enquote{seed} of an utterance is an inherently multimodal idea unit that comprises imagistic as well as symbolic proto-representations which unfold into gesture and speech respectively in the process of articulation \citetext{see also \citealt{Roepke:2011} on growth point's involvedness in contexts and frames}.

The \emph{Sketch model} \is{sketch model} \citep{de:Ruiter:2000} reflects explicitely different kinds of gestures (see Section~\ref{sec:kinds-gestures}). 
%
Its name is due to the sketch component, an abstract spatio-temporal representation alongside Levelt's preverbal message. 
%
Independently from each other, the sketch is send to a gesture planner, while the preverbal message is processed by the formulator.

According to the \emph{Lexical Access model} \is{Lexical Access model} of \citet{Krauss:Chen:Gottesmann:2000}, iconic gestures are related to words and are used in order to facilitate speaker-internal word retrieval rather than communicating pictorial information.

The \emph{Interface model} \is{Interface model} \citep{Kita:Ozyurek:2003} speech generating and the gesture generating processes negotiate with each other and therefore can influence each other during the production phase.
\is{speech-gesture production models|)}

The different speech-gesture production models make slighty different predictions for speakers suffering from aphasia and can be evaluated accordingly \citep{deRuiter:deBeer:2013}.
%
Indeed, observing the gesture behaviour of aphasic speakers is one aspect of gesture and aphasia \citep{Jakob:Bartmann:Goldenberg:Ziegler:Hogrefe:2011,Kong:Law:Chak:2017,Sekine:Rose:2013}.
%
Other aspects include the use of gesture in speech therapy. 
%
Close to the Lexical Access model, gestures have been used in order to facilitate word retrieval in what can be called \emph{multimodal therapy} \is{multimodal therapy} \citet{Rose:2006}.
%
Following a different strategy, gestures are also be used in order to enhance the communicative range of patients: they learn to employ gestures instead of words in order to communicate at least some of their needs and thoughts more fluently in \citep{Cubelli:Trentini:Montagna:1991,Caute:et:al:2013}.

However, just counting on gestures in therapy does not automatically lead to success \citep{Auer:Bauer:2011}. 
%
The kind and severity of aphasia, the individual traits of the aphasic speaker, and the kinds of gestures impaired or at disposal, amongst other factors, seem to a constitute a complex network for which currently no generally applicable clinical pathway can be given.




\section{Outlook}
\label{sec:outlook}

What are (still) challenging issues with respect to grammar-gesture integration, in particular from a semantic point of view? Candidates include:

\begin{itemize}
\item gestalt phenomena: the trajectories described by a gesture are often incomplete and have to be completed by drawing on gestalt principles or everyday knowledge \citep{Luecking:2016};
\item negligible features: not all formal features of a gesture are meaning"=carrying features in the context of utterance. For instance, in a dynamic gesture the handshape often (though not always) does not provide any semantic information. How to distinguish between significant and negligible gesture features?
\item \enquote{semantic endurance}: due to holds, gestural meanings can keep fixed for some time is keeps available for semantic attachment. This may call for a more sophisticated algebraic treatment of speech-gesture integration than offered by typed feature structures \citep{Rieser:2015}.
\end{itemize}

Finally, the empirical domain of \enquote{gesture} has to extended to other non-verbal signals, in particular propositional ones such as laughter \citep{Ginzburg:Breitholz:Cooper:Hough:Tian:2015}, facial expressions or gaze (see Section~\ref{sec:why-gestures} for a brief list of non-verbal signals), in isolation as well as in mutual combination.
%
Thus, there is still some way to go in order to achieve a fuller understanding of natural language interaction and thereby natural languages.


 
% \section*{Abbreviations}
% \section*{Acknowledgements}

% I want to thank Markus Steinbach and Jonathan Ginzburg for comments on an outline of this chapter. 
% Alex Lascarides
% %
% I also want to thank Stefan Müller for his comments and hints.


\avmoptions{active}

{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}
\end{document}
