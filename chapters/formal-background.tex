\documentclass[output=paper
                ,modfonts
                ,nonflat
	        ,collection
	        ,collectionchapter
	        ,collectiontoclongg
 	        ,biblatex
                ,babelshorthands
                ,newtxmath
                ,draftmode
                ,colorlinks, citecolor=brown
]{./langsci/langscibook}

\IfFileExists{../localcommands.tex}{%hack to check whether this is being compiled as part of a collection or standalone
  \input{../localpackages}
  \input{../localcommands}
  \input{../localhyphenation}
  \bibliography{../Bibliographies/stmue,
                ../localbibliography,
../Bibliographies/formal-background,
../Bibliographies/understudied-languages,
../Bibliographies/phonology,
../Bibliographies/case,
../Bibliographies/evolution,
../Bibliographies/agreement,
../Bibliographies/lexicon,
../Bibliographies/np,
../Bibliographies/negation,
../Bibliographies/argst,
../Bibliographies/binding,
../Bibliographies/complex-predicates,
../Bibliographies/coordination,
../Bibliographies/relative-clauses,
../Bibliographies/udc,
../Bibliographies/processing,
../Bibliographies/cl,
../Bibliographies/dg,
../Bibliographies/islands,
../Bibliographies/diachronic,
../Bibliographies/gesture,
../Bibliographies/semantics,
../Bibliographies/pragmatics,
../Bibliographies/information-structure,
../Bibliographies/idioms,
../Bibliographies/cg,
../Bibliographies/udc}

  \togglepaper[3]
}{}

\author{%
 Frank Richter\affiliation{Goethe UniversitÃ¤t Frankfurt}%
}
\title{Formal Background}  

% \chapterDOI{} %will be filled in at production
% \epigram{}
\abstract{
  This chapter provides a very condensed introduction to a formalism for
  \cite{PollardSag1994} and explains its fundamental concepts. It pays
  special attention to the model-theoretic meaning of HPSG grammars. In
  addition it points out some links to other, related formalisms, such as
  feature logics of partial information, and to related
  terminology in the context of grammar implementation platforms.
}


\begin{document}
\maketitle
\label{chap-formal-background}


%--------------------------------------------------------------
\section{Introduction} 

The two HPSG books by Pollard and Sag
\citep{PollardSag1987,PollardSag1994} do not present grammar formalisms
with the intention to provide precise definitions.
Instead they refer to various inspirations in the
logics of typed feature structures or in predicate logic, informally
characterize the intended formalisms, and explain them as they are
used in concrete grammars of English. \citet{PollardSag1994} further clarifies
their intentions in an appendix which lists most (but not all) of the components
of their grammar of English explicitly, and summarizes most of their core
assumptions. With this strategy, both books leave room for
interpretation.

There are a number of challenges with reviewing the formal background
of HPSG. Some of them have to do with the long or
complicated publication history of relevant articles and books, some
with the effects of grammar implementation platforms, which have
their own underlying formalisms and influence the way in which
linguists think and talk about their grammars with their own terminology
and notational conventions. % book chapter? 
Salient examples include
notations for phrase structure rules, the treatment of lexical
representations or the lexicon, mechanisms for lexical rules, and
notations for default values, among many other devices.  Many of these
notations are well-known in the HPSG community. Even if they do not
strictly belong to a formal syntax, there is usually a way to
interpret their convenient notations in formalisms intended for, or at least
compatible with, HPSG. However,
their re-interpretation can deviate to a larger or lesser degree from
what their users have in mind when they write grammars. These
differences may be sometimes subtle and sometimes significant, but
they entail that the meaning of the notations when adapted to a
formalism is not what their users might have deduced from the behavior
of a given implementation platform for parsing or generation.
Similarly, terminology that belongs to the computational environment
of implementations is often transferred to grammar theory, and
again, a re-interpretation in terms of the underlying
formalism can sometimes be trivial and sometimes nearly impossible,
and different available choices of re-interpretations might
have significantly different effects.

It is not only the flexibility of the informal AVM notation and its
convenient notational enhancements (for lexical rules, decorated sort
hierarchies, phrase structure trees, etc.) but also early changes in
underlying foundational assumptions and terminology that one needs to
be aware of when reviewing HPSG's formal background. When first
presented in a book in 1987, HPSG was conceived of as a
\emph{unification-based} grammar theory, a name, the authors
explain, which ``arises from the algebra that governs partial
information structures'' \citep[7]{PollardSag1987}. This algebra was
populated by partial feature structures with unification as a
fundamental algebraic operation. In the framework envisioned seven
years later in \citet{PollardSag1994},
that algebra did not exist anymore, feature structures were no longer
partial but total objects in models of a logical theory, and
unification was no longer defined in the new setting (as the relevant
algebra was gone). However, most of the notation and considerable
portions of the terminology of 1987 remain with us to this day, such as the
\emph{types} of feature structures (for the \emph{sorts} of 1994,
when the term \emph{type} was used for a different concept, to be
discussed below), the pieces of information (for 1987-style feature
structures) or even the word \emph{unification}, which took on a
casual life of its own without the underlying original algebra in
which it had been defined. Occasionally these words still have a
concrete technical interpretation in the language of grammar
implementation environments or in their run-time system, which may
reinforce their use in the community despite their lack of meaning in
the standard formalism of HPSG. Implementation platforms may also add
their own convenient technical and notational devices, implicitly
inviting linguists to import them into their theoretical grammar
writing.

%Another aspect that is frequently overlooked or ignored concerns
%feature structures themselves: In the earlier version of HPSG they
%were so fundamental to the framework and their properties were

%\#\# further explanation, including references to the literature where
%the classical HPSG formalism was discussed and what these papers focused
%on  --> if at all, this will be in Sections \ref{sec-alt-gr-meaning} and
%\ref{sec-alt-formalisms}

This handbook article cannot disentangle the history and relationships
between the various formalisms leading to an explication of the
1994 version of HPSG, nor of those that existed and still exist in
parallel. It sets out to clarify the terminology and
structure of a formalism for \cite{PollardSag1994} and
presents a canonical formalism of the final version of
HPSG in \cite{PollardSag1994}. It will only occasionally point out some of
the differences to its 1987 precursor where
the older terminology is still present in current HPSG papers and
may be confusing to an audience unaware of the different
usages of terms which mean different things in different
contexts.

%I will clarify parallel terminological traditions and

The main sources of the present summary are the construction of a model
theory for HPSG by \citet{King99a-u} and \citet{Pollard99a}, and their
synoptic reconstruction on the basis of  a comprehensive logical language
for HPSG, \emph{Relational Speciate Re-entrant Language} (RSRL) by
\citet{Richter2004a-u}, including the critique and extensions sketched
in \citet{Richter2007a}.


\section{Signatures and descriptions}

As logical theories of entities in a domain of objects, HPSG grammars
consist of two main components. First, a logical signature, which
provides the symbols for describing the domain of interest, in this
case a natural language. And second, an exact delineation of all and
only the legitimate entities in the denotation of the grammar, written
as a collection of statements about their configuration. These statements are
descriptions within a logical language and are composed from logical
constants, variables, quantifiers, brackets and the symbols provided
by the signature. They are variously known to linguists as principles
of grammar, constraints, or rules. In the following, I will use the
term \emph{principles} to designate these statements.
As we will see, they must be of a certain form in order to do the job
they are meant to perform, and linguists often use abbreviatory
conventions for conceptually distinguished groups of principles, such
as grammar rules, lexical entries, or lexical rules. From a logical
perspective,
then, a grammar is a pair consisting of a signature and a collection
of principles.  The appendix of \citet{PollardSag1994} provides an
early example in HPSG of this conception.
%\#\# say more?\#\#; July 2019: no, later

Signatures in HPSG go beyond supplying non-logical symbols for
descriptions, they impose additional restrictions on the
organization of the non-logical symbols. These restrictions ultimately have an effect on
how the domain of described objects is structured. Let us first
investigate the two most prominent sets of non-logical symbols, sorts
and attributes. The set of \emph{sort} symbols is arranged in a
\emph{sort hierarchy}, and that sort hierarchy is in turn connected to
the set of \emph{attribute} symbols (also known as
\emph{features}). The sort hierarchy is a partial order, and\marginpar{def.\ partial order?}
attributes are declared \emph{appropriate to} sorts
in the sort hierarchy. This appropriateness declaration must not be
entirely random: if an attribute is declared appropriate to some sort,
it must also be declared appropriate to all its subsorts. This requirement
is known as \emph{feature inheritance}.  Moreover, for each\marginpar{pictures?,
incl. to vs for (JP)}
sort $\sigma$ and attribute $\phi$ such that $\phi$ is appropriate to
$\sigma$, some other sort $\sigma'$ is \emph{appropriate for} $\phi$
at $\sigma$. In other words, a certain attribute value ($\sigma'$) is declared
appropriate for $\phi$ at $\sigma$. These attribute values must not be
completely random either: For any subsort of $\sigma$, the value of an
appropriate feature $\phi$ of $\sigma$ is of course also appropriate
to that subsort (by feature inheritance), but in addition, the value of $\phi$ at that subsort
must be at least as specific as it is at $\sigma$. This means the value is either
$\sigma'$ or a subsort thereof. It may not be less specific, or, to put
it differently, it may not be a
supersort of $\sigma'$.

Some sorts in the sort hierarchy enjoy a special status by being
\emph{maximally specific}. They are called \emph{species}. Species
are sorts without subsorts.
Sorts that are maximally specific and lack any appropriate attribute receive
a special name and are called \emph{atomic} sorts or simply \emph{atoms}.

In addition to sorts and attributes, a signature provides relation
symbols.  Well known examples are a ternary \texttt{append} relation
and a binary \texttt{member} relation, but grammars may also require
relations such as (often ternary) \texttt{shuffle} and binary
\texttt{o-command}. Each relation symbol comes with
a positive natural number for the number of arguments, its \emph{arity}.


Putting all of this together, we obtain a definition of signatures:

\begin{mydef}\label{def-signature}
  $\Sigma$ is a \emph{signature} iff\\
  $\Sigma$ is a septuple $\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,\\
  $\left<S,\sqsubseteq\right>$ is a partial order,\\
  $S_{max} = \left\{\sigma\in S\ |\ \mbox{for each}\ \sigma' \in S, \mbox{if}\ \sigma'\sqsubseteq\sigma \mbox{then}\ \sigma=\sigma'\right\}$,\\
  $A$ is a set,\\
  $F$ is a partial function from $S\times A$ to $S$,\\
  for each $\sigma_1\in S$, for each $\sigma_2\in S$, for each $\phi\in A$,\\
  \hspace*{.5cm} if $F(\sigma_1,\phi)$ is defined and $\sigma_2\sqsubseteq\sigma_1$\\
  \hspace*{.5cm} then $F(\sigma_2,\phi)$ is defined and
             $F(\sigma_2,\phi)\sqsubseteq F(\sigma_1,\phi)$,\\
  $R$ is a finite set, and\\
  $Ar$ is a total function from $R$ to the positive integers.
\end{mydef}

The partial order $\left<S,\sqsubseteq\right>$ is the sort hierarchy,
and the set of sorts $S$, just like the set of attributes $A$, can in principle
be infinite. In actual grammars it is finite, and in HPSG grammars it is
also assumed that $S$ contains a top element, which is a sort that subsumes
all other sorts in the sort hierarchy. $S_{max}$ is the set of maximally
specific sorts, which will play a prominent role in the semantics of
descriptions. $F$ is a function for fixing the appropriateness conditions
on attributes and attribute values, and the conditions on that function
reflect HPSG's restrictions on feature declarations. $F$ is called the
\emph{(feature) appropriateness function}. The last two lines of
the definition provide the set of relation symbols with their arity.
We assume that relations are at least unary.

Relations in HPSG often express relationships between
lists (\texttt{append}, \texttt{shuffle}) or sets (\texttt{union},
\texttt{intersection}). In order to give grammarians the flexibility
that some of the applications of these relations in the literature
starting with \citet{PollardSag1994} require, RSRL adds
dedicated sorts and attributes with a fixed interpretation to every signature.
They can be thought of as a more flexible treatment of lists along their
regular explicit encoding in HPSG (usually with attributes \textsc{first} and
\textsc{rest}, and sorts \textit{list}, \textit{elist}, and
\textit{nelist}, but of course the exact naming does not matter).\marginpar{add picture (Stefan)}
Informally, the extra symbols act very much like sorts and attributes for
lists. In order to integrate the reserved new sort symbols with any
signature a linguist might specify, a distinguished sort \textit{metatop} serves as unique
top element of the extended signature. The extension is defined for
any signature by adding reserved \emph{pseudo-sorts} and \emph{pseudo-attributes}
and structuring the expanded sort hierarchy in the desired way:

\begin{mydef}\label{def-sig-chains}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,\\
$\Aug{S}= S \cup \left\{\mathit{chain, echain, nechain, metatop}\right\}$,\\
$\Aug\sqsubseteq=\ \sqsubseteq\cup\left\{\left<\mathit{echain},\mathit{chain}\right>,\left<\mathit{nechain},\mathit{chain}\right>\right\}\cup\left\{\left<\sigma,\sigma\right>|\sigma\in\Aug{S}\backslash S\right\}\cup\left\{\left<\sigma,\mathit{metatop}\right>|\sigma\in\Aug{S}\right\}$,\\
$\Aug{S}_{max}=S_{max}\cup\left\{\mathit{echain},\mathit{nechain}\right\}$, and\\
$\Aug{A}=A\cup\left\{\dagger,\triangleright\right\}$.
\end{mydef}

%For simplicity, the reserved additional symbols in $\Aug{S}$ and
%$\Aug{A}$ will occasionally be referred to as \emph{pseudo-sorts} and
%\emph{pseudo-attributes}, respectively.\marginpar{delete?}
Apart from the non-logical constants from (expanded) signatures and
some logical symbols, we also need a countably infinite set of
variables, which will be symbolized by $V$.


For expository reasons the syntax of descriptions, to be introduced
next, does not employ attribute-value matrices (AVMs), the common
lingua franca of con\-straint-based grammar formalisms. The reasons are
twofold: Most importantly, although AVMs provide an extremely readable
and flexible notation, they are quite cumbersome to define as a
rigorous logical language which meets all the expressive needs of HPSG. Some
of this awkwardness in explicit definitions derives from the very
flexibility and redundancy in notation that makes AVMs perfect for
everyday linguistic practice. Second, the original syntax of (R)SRL
is, by contrast, extremely easy to define, and, as long as it is not used
for descriptions as complex as they occur in real grammars, its expressions are
still transparent for everyone who is familiar with AVMs. Readers who
want to explore how our description syntax relates to a formal syntax
of AVMs are referred to \cite{Richter2004a-u} for details and a
correspondence proof.

The definition of the syntax of descriptions proceeds in two steps,
quite similar to first-order predicate logic. We will first introduce
terms and then build formulae and descriptions from terms. Terms are essentially
what is known to linguists as \emph{paths}, sequences of attributes:

\begin{mydef}
  For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
  $T^{\Sigma}$ is the smallest set such that\\
  $\its\in T^{\Sigma}$,\\
  for each $v\in V$, $v\in T^{\Sigma}$,\\
  for each $\phi \in \Aug{A}$ and each $\tau\in T^{\Sigma}$, $\tau\phi\in T^{\Sigma}$.
\end{mydef}

Simply put, sequences of attributes (including the two pseudo-attributes
$\dagger$ and $\triangleright$)
starting either with the colon or a single variable are $\Sigma$ terms.
Equipped with terms, we can immediately proceed to formulae, the penultimate
step on the way to descriptions.
%Descriptions will be defined as formulae in which no occurrence of a
%variable is free.
There are three kinds of simple formulae: Formulae that assign a sort
to the value of a path, formulae which state that two paths have the
same value (\emph{structure sharing}, in linguistic terminology), and
relational formulae. Complex formulae can be built from these by existential
and universal quantification, negation, and the classical binary logical
connectives.

\begin{mydef}
  For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
  $D^{\Sigma}$ is the smallest set such that\\
  for each $\sigma\in\Aug{S}$, for each $\tau\in T^{\Sigma}$,
  $\tau\sim\sigma\in D^{\Sigma}$,\\
  for each $\tau_1, \tau_2\in T^{\Sigma}$, $\tau_1 \approx \tau_2 \in D^{\Sigma}$,\\
  for each $\rho\in R$, for each $x_1, \ldots, x_{Ar(\rho)}\in V$,
  $\rho(x_1,\ldots,x_{Ar(\rho)})\in D^{\Sigma}$,\\
  for each $x\in V$, for each $\delta\in D^{\Sigma}$,
  $\exists x\delta\in D^{\Sigma}$, \hfill (analogous for $\forall$)\\
%  for each $x\in V$, for each $\delta\in D^{\Sigma}$,
%  $\forall x\delta\in D^{\Sigma}$,\\
  for each $\delta\in D^{\Sigma}$, $\neg\delta\in D^{\Sigma}$,\\
  for each $\delta_1,\delta_2\in D^{\Sigma}$, and
  $\left(\delta_1\land\delta_2\right) \in D^{\Sigma}$.
  \hfill (analogous for $\lor,\rightarrow,\leftrightarrow$)
\end{mydef}

Finally, $FV$ is a function that determines for every $\Sigma$ term and
$\Sigma$ formula the set of variables that occur free in them.

\begin{mydef}
  For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,\\
  $FV(\its)=\left\{\right\}$,\\
  for each $x\in V$, $FV(x)=\left\{x\right\}$,\\
  for each $\tau\in T^{\Sigma}$, for each $\phi\in\Aug{A}, FV(\tau\phi)=FV(\tau)$,\\
  for each $\tau\in T^{\Sigma}$, for each $\sigma\in\Aug{S}, FV(\tau\sim\sigma)=FV(\tau)$,\\
  for each $\tau_1, \tau_2\in T^{\Sigma}$, $FV(\tau_1\approx\tau_2)=FV(\tau_1)\cup FV(\tau_2)$,\\
  for each $\rho\in R$, for each $x_1,\ldots, x_{Ar(\rho)}\in V$,
  $FV(\rho(x_1,\ldots, x_{Ar(\rho)}))=\left\{x_1,\ldots, x_{Ar(\rho)}\right\}$,\\
  for each $\delta\in D^{\Sigma}$, for each $x\in V$,
  $FV(\exists x\delta)=FV(\delta)\backslash\left\{x \right\}$,\hfill(analogous for $\forall$),\\
  for each $\delta\in D^{\Sigma}$, $FV(\neg\delta)=FV(\delta)$,\\
  for each $\delta_1,\delta_2\in D^{\Sigma}$,
  $FV((\delta_1\land\delta_2))=FV(\delta_1)\cup FV(\delta_2)$.
  \hfill (analogous for $\lor,\rightarrow,\leftrightarrow$)
\end{mydef}

Informally, an occurrence of a variable is free in a $\Sigma$ term or a
$\Sigma$ formula if it is not bound by a quantifier. We single out
$\Sigma$ formulae without free occurrences of variables as a kind of formula
of special interest and reserve the term $\Sigma$ \emph{description} for them:

\begin{mydef}
  For each signature $\Sigma$,
  $D_0^{\Sigma}=\left\{\delta\in D^{\Sigma} | FV(\delta)=\left\{\right\}\right\}$.
\end{mydef}

$D_0^{\Sigma}$ is the set of $\Sigma$ descriptions.
When a signature is fixed by the context, or when the exact signature is
irrelevant in the discussion, we can simply speak of \emph{descriptions}
instead of $\Sigma$ descriptions. Descriptions are the syntactic units
that linguists use in grammar writing. Grammars, as we will see in
Section~\ref{sec-grammar-meaning}, are written by declaring a signature
and stating a set of descriptions. But before we can investigate grammars and
what they mean, we
have to explain the meaning of signatures and of descriptions.


  
\section{Meaning of signatures and descriptions}

Descriptions of RSRL are interpreted similar to expressions of
classical logics such as first order logic, except that they are not
evaluated as true or false in a given structure; instead, they denote
collections of structures.

Defining the meaning of descriptions begins with delineating the
structures which interpret signatures. In particular, species and
attributes must receive a meaning, which should be tied to the
HPSG-specific intentions behind sort hierarchies and feature
declarations; and so must relation symbols, whose interpretation
should heed their arity. Due to some extra restrictions which will
ultimately be put on the interpretation of relation symbols (to meet
intuitions of grammarians) and whose
formulation requires a notion of term interpretation, we start with
\emph{initial interpretations}. They will be refined in a second
step to full interpretations (Definition~\ref{def-full-interpretation}).

Some additional notation is needed in the next definition. If $S$ is a
set, $S^{*}$ is the set of all finite sequences (or $n$-tuples) of elements
of $S$. $S^{+}$ is the same set without the empty sequence. $\overline{S}$
is short for the set $S\cup S^{*}$.

\begin{mydef}\label{def-initial-int}
  For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
  $\Inte$ is an \emph{initial $\Sigma$ interpretation} iff\\
  $\Inte = \Interpretation$,\\
  $\Unive$ is a set,\\
  $\Speci$ is a total function from $\Unive$ to $S_{max}$,\\
  $\Atti$ is a total function from $A$ to the set of partial functions from
  $\Unive$ to $\Unive$,\\
  for each $\phi\in A$ and each $u\in\Unive$\\
  \hspace*{.5cm} if $\Atti(\phi)(u)$ is defined\\
  \hspace*{.5cm} then $F(\Speci(u),\phi)$ is defined, and
  $\Speci(\Atti(\phi)(u))\sqsubseteq F(\Speci(u),\phi)$, and\\
  for each $\phi\in A$ and each $u\in\Unive$,\\
  \hspace*{.5cm} if $F(\Speci(u),\phi)$ is defined
  then $\Atti(\phi)(u)$ is defined,\\
  $\Reli$ is a total function from $R$ to the power set of
  $\bigcup\limits_{n\in\NatNum}{\overline{\Unive}}^n$, and\\
  \hspace*{.5cm} for each $\rho\in R$,
  $\Reli(\rho)\subseteq\overline{\Unive}^{Ar(\rho)}$.
\end{mydef}

Initial $\Sigma$ interpretations are quadruples consisting of four
components. The first three of them will remain unchanged in full
$\Sigma$ interpretations (Definition~\ref{def-full-interpretation}). The elements of $\Unive$
are entities which populate the universe of structures. Their ontological
status has been debated fiercely in HPSG, and will be discussed in
Sections~\ref{sec-grammar-meaning}--\ref{sec-alt-gr-meaning}. For the
moment, assume that they are either linguistic objects or appropriate
abstractions thereof. $\Speci$ assigns each object in the universe
a species, which is another way of saying that each object is of exactly
one maximally specific sort. This is what is known as the property of being
\emph{sort-resolved}. The attribute interpretation function $\Atti$
interprets each attribute symbol as a (partial) function that assigns an
object of the universe to an object of the universe, and as such it
obeys the restrictions of the feature declarations of the signature,
embodied in the function $F$: Attributes are defined on all and only
those objects $u_1$ which have a species to which the attributes are appropriate
according to $F$; and the object which $u_1$ is mapped to by the attribute
must in turn be of a species which is appropriate for the attribute
(at the species of $u_1$). This is what is known as the property of
interpreting structures of being \emph{totally well-typed}. Originally both
of these properties of interpreting structures were formulated with
respect to so-called \emph{feature structures}, but, as we will see below,
this conception of interpreting structures for grammars
was soon given up for philosophical reasons.\footnote{Of course, the
  informal term \emph{feature structure} is still alive among
  linguists, and in a technical sense they are essential constructs
  for implementation platforms.} The relation interpretation function
$\Reli$ finally interprets $n$-ary
relation symbols as sets of $n$-tuples of objects. However, there is
an additional option, which makes the definition look more complex: An
object in an $n$-tuple may in fact not be an atomic object, it can
alternatively be an $n$-tuple of objects itself. These $n$-tuples in
argument positions of relations will be described as \emph{chains}
with the extra symbols, pseudo-sort and pseudo-attributes, which were
added to signatures in
Definition~\ref{def-sig-chains} above.  As pointed out there, chains
are additional constructs which give grammarians the flexibility to
use (finite) lists in all the ways in which they are put in relations
in actual HPSG grammars. %\footnote{The advantage is that objects can be treated as being ordered on a chain without actually occurring on a list. See \cite{Richter2004-u} for discussion.}

Since chains are provided by an extension of the set of sort symbols
and attributes (Definition~\ref{def-sig-chains}), the interpretation of the additional symbols must be
defined separately. This is very simple, since these
symbols behave essentially analogous to the conventional sort and attribute
symbols of HPSG's list encoding.

\begin{mydef}
  For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
  for each initial $\Sigma$ interpretation $\Inte=\Interpretation$,\\
  $\Aug\Speci$ is the total function from $\overline\Unive$ to $\Aug{S}$
  such that\\
  \hspace*{.5cm} for each $u\in\Unive$, $\Aug{\Speci}\left(u\right)=\Speci\left(u\right)$,\\
  \hspace*{.5cm} for each $u_1, \ldots, u_n\in\Unive$,
            \(\Aug{\Speci}\left(\left<u_1,\ldots,u_n\right>\right)=\left\{
\begin{tabular}{ll}
$\mathit{echain}$
&
if $n = 0$,\\
$\mathit{nechain}$
&
if $n > 0$
\end{tabular}
\right.\), and\\
$\Aug{\Atti}$ is the total function from $\Aug{A}$ to the set of partial
functions from $\overline{\Unive}$ to $\overline{\Unive}$ such that
\hspace*{.5cm} for each $\phi\in A$, $\Aug{\Atti}\left(\phi\right)=\Atti\left(\phi\right)$,\\
\hspace*{.5cm} $\Aug{\Atti}\left(\dagger\right)$ is the total function from
$\Unive^{+}$ to $\Unive$ such that for each
$\left<u_0,\ldots,u_n\right>\in\Unive^{+}$,\\
\hspace*{1cm} $\Aug{\Atti}\left(\dagger\right)\left(\left<u_0,\ldots,u_n\right>\right)=u_0$, and\\
\hspace*{.5cm} $\Aug{\Atti}\left(\triangleright\right)$ is the total function from
$\Unive^{+}$ to $\Unive^{*}$ such that for each
$\left<u_0,\ldots,u_n\right>\in\Unive^{+}$,\\
\hspace*{1cm} $\Aug{\Atti}\left(\triangleright\right)\left(\left<u_0,\ldots,u_n\right>\right)=\left<u_1,\ldots,u_n\right>$.
\end{mydef}

$\Aug{S}$ is the \emph{expanded species assignment function}, and
$\Aug{A}$ is the  \emph{expanded attribute interpretation function}.
The pseudo-species symbols \textit{echain} and \textit{nechain} label
empty chains and non-empty chains, respectively. Given a non-empty
chain, the pseudo-attribute $\dagger$ picks out its first member,
corresponding to the function of the \textsc{first} attribute on non-empty
lists. Conversely, $\triangleright$ cuts off the first element
of a non-empty chain and returns the remainder of the chain, as does the
standard attribute \textsc{rest} for lists.

In addition to attributes, terms may also contain variables. Term
interpretation thus requires a notion of \emph{variable assignments} in
(initial) interpretations.

\begin{mydef}
  For each signature $\Sigma$,
  for each initial $\Sigma$ interpretation $\Inte=\Interpretation$,\\
  $\VarInt_{\Inte}=\overline{\Unive}^{\mbox{$V$}}$ is the smallest set of variable
  assignments in $\Inte$.
\end{mydef}

An element of $\VarInt_{\Inte}$ (the set of total functions from the
set of variables to the set of objects and chains of objects of
$\Unive$) will be notated as $g$, following a convention frequently
observed in predicate logic.  With variable assignments in (initial)
interpretations, variables denote objects in the universe $\Unive$
and chains of objects of the universe.


\begin{mydef}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
for each initial $\Sigma$ interpretation $\Inte=\Interpretation$,
for each $g\in\VarInt_{\Inte}$,
$\Tinte^{g}_{\Inte}$ is the total function from $T^{\Sigma}$ to the smallest
set of partial functions from $\Unive$ to $\overline{\Unive}$ such that
for each $u\in\Unive$,\\
\hspace*{.5cm} $\Tinte^{g}_{\Inte}(\its)(u)$ is defined and
$\Tinte^{g}_{\Inte}(\its)(u)=u$,\\
\hspace*{.5cm}for each $v\in V$, $\Tinte^{g}_{\Inte}(v)(u)$ is defined and
$\Tinte^{g}_{\Inte}(v)(u)=g(u)$,\\
\hspace*{.5cm}for each $\tau\in T^{\Sigma}$, for each $\phi\in\Aug{A}$,\\
\hspace*{1cm}$\Tinte^{g}_{\Inte}(\tau\phi)(u)$ is defined %\\ \hspace*{1cm}
             iff $\Tinte^{g}_{\Inte}(\tau)(u)$ is defined and
             $\Aug{\Atti}(\phi)(\Tinte^{g}_{\Inte}(\tau)(u))$ is defined, and\\
\hspace*{1cm}if $\Tinte^{g}_{\Inte}(\tau\phi)(u)$ is defined then
             $\Tinte^{g}_{\Inte}(\tau\phi)(u)=\Aug{\Atti}(\phi)(\Tinte^{g}_{\Inte}(\tau)(u))$.
\end{mydef}
$\Tinte^{g}_\Inte$ is called the \emph{term interpretation function
  under $\Inte$ under $g$}. $\Sigma$ terms either start with a
variable or with the special symbol colon (`$\its$'). The colon
denotes the identity function. Interpreted on any object, it returns
that object.  If a term $\tau$ starts with the colon, its term
interpretation so to speak starts at the object $u$ to which it is
applied ($\Tinte^{g}_{\Inte}(\tau)(u)$) and, if each subsequent
attribute in $\tau$ is defined on the object to which the
interpretation of the earlier attribute(s) took us, the term
interpretation will yield the object reached by the last
attribute. When a $\Sigma$ term starts with a variable $v$, the given
variable assignment $g$ will determine the starting point of
interpreting the sequence of attributes ($g(v)$).  Of course,
variables may be assigned chains of objects, in which case the symbols
of the expanded attribute set can be used to navigate the elements of
the chain.

The set of objects which are reachable from a single given object in
an interpretation by following sequences of attribute interpretations
is important for the way in which quantification is conceived by
grammarians, it plays a role in thinking about which objects can in
principle stand in a relation, and it is crucial for explicating
different notions of the meaning of grammars. Definition~\ref{def-components-of-u}
captures this idea:  
  
\begin{mydef}\label{def-components-of-u}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
for each initial $\Sigma$ interpretation $\Inte=\Interpretation$,
for each $u\in\Unive$,\\
\hspace*{.5cm}  $\CInt_{\Inte}^u
   =\set[$u'\in\Unive$
    \set=for some $g\in\VarInt_{\Inte}$,\\
         for some \(\pi \in A^*\),\einruck
         $\Tinte^{g}_\Inte(\its\pi)(u)$ is defined, and\einruck
         $u'=\Tinte^{g}_\Inte(\its\pi)(u)$
    \set]$.

\end{mydef}

$\CInt_{\Inte}^u$ is the set of components of $u$ in $\Inte$.
The purpose of $\CInt_{\Inte}^u$ is to capture the set of all objects
that are reachable from some object $u$ in the universe by following a
path of interpreted attributes. Thinking of these configurations as
graphs, the set of components of $u$ in $\Inte$ is the set of nodes
that can be reached by following any sequence of arrows starting from $u$. This
corresponds to how linguists normally conceive of the substructures of
some structured object.\footnote{Phrasing this more carefully, the
  object itself is not structured, but there is a structure generated by
  the object by following the arrows, or more technically,
  by the composition of functions which interpret attribute symbols.}
The set of components of objects is used in two ways in the definitions
of full interpretations and description denotations: It restricts
the set of objects that are permitted in relations, and it provides
the domain of quantification in quantificational expressions of the
logical language.

According to Definition~\ref{def-initial-int} of initial
interpretations, relation symbols are simply interpreted as tuples of
objects (and chains of objects) in the universe of interpretation.
However, HPSGians have a slightly more restricted notion of relations:
For them, relations hold between objects that occur within a sign (or
a similar kind of larger linguistic structure), they are not relations
between objects that occur in separate (unconnected) signs. The
following notion of \emph{possible relation tuples in an
  interpretation} captures this intuition.

\begin{mydef}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
for each initial $\Sigma$ interpretation $\Inte=\Interpretation$,\\
\hspace*{.5cm}\(\ReliT_{\Inte}
=\bigcup\limits_{n\in\NatNum}
\set[$\left<u_1,\ldots,u_n\right>\in\overline{\Unive}^n$
  \set= for some $u\in\Unive$,\\
        for each $i\in\NatNum$, $1\leq i\leq n$\einruck
            $u_i\in\overline{\CInt_{\Inte}^u}$
\set]
\).
\end{mydef}

$\ReliT_{\Inte}$ is the set of possible relation tuples in $\Inte$.
Possible relation tuples in an initial interpretation are characterized
by the existence of some object in the interpretation from which each
object in a relation tuple can be reached by a sequence of attribute
interpretations. In case an argument in a tuple is a chain, then the
objects on the chain are thus restricted.

The notion of \emph{full interpretations} integrates this restriction on
possible relations, keeping everything else unchanged from initial
interpretations:

\begin{mydef}\label{def-full-interpretation}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
for each initial $\Sigma$ interpretation $\Inte'=\left<\Unive',\Speci',\Atti',
\Reli'\right>$, for the set of possible relation tuples in
$\Inte'$, $\ReliT_{\Inte'}$,
$\Inte=\Interpretation$ is a full $\Sigma$ interpretation iff\\
$\Unive=\Unive'$, $\Speci=\Speci'$, $\Atti=\Atti'$, and
$\Reli$ is a total function from $R$ to the power set of
$\ReliT_{\Inte'}$, and %\\ \hspace*{.5cm}
  for each $\rho\in R$,
  $\Reli(\rho)\subseteq\left(\ReliT_{\Inte'}\cap\overline{\Unive}^{Ar(\rho)}\right)$.
\end{mydef}


It can be checked that variable assignments in initial interpretations
and sets of components of objects in initial interpretations are the
same as in corresponding full interpretations with the same universe,
species interpretation and attribute interpretation functions, since
variable assignments and sets of components of objects do not depend on the
interpretation of relations. From now on all of the above will be used with
respect to full interpretations, and full interpretations will simply
be called interpretations.

The following definition of $\Sigma$ formula denotation needs a
notation for modifying variable assignments with respect to the
value of designated variables. For any variable assignment $g\in\VarInt_{\Inte}$,
for $g'=g[x \mapsto u]$, $g'$ is just like $g$ except that
$g'$ maps variable $x$ to object $u$.

%\begin{mydef}
%notation for modifying assignment functions
%\end{mydef}

\begin{mydef}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
for each (full) $\Sigma$ interpretation $\Inte=\Interpretation$,
for each $g\in\VarInt_{\Inte}$,
$\Dinte_{\Inte}^{g}$ is the total function from $D^{\Sigma}$ to the power
set of $\Unive$ such that\\
\hspace*{.5cm} for each $\tau\in T^{\Sigma}$, for each $\sigma\in \Aug{S}$,
$\Dinte_{\Inte}^{g}(\tau\sim\sigma)=
\set[ $u\in\Unive$
  \set= $\Tinte_{\Inte}^{g}(\tau)(u)$ is defined, and\\
  $\Aug{\Speci}\left(\Tinte_{\Inte}^{g}(\tau)(u)\right) \Aug{\sqsubseteq}\ \sigma$
\set]$,\\
\hspace*{.5cm} for each $\tau_1, \tau_2 \in T^{\Sigma}$,
$\Dinte_{\Inte}^{g}(\tau_1 \approx \tau_2)=
\set[ $u\in\Unive$
  \set= $\Tinte_{\Inte}^{g}(\tau_1)(u)$ is defined,\\
        $\Tinte_{\Inte}^{g}(\tau_2)(u)$ is defined, and\\
        $\Tinte_{\Inte}^{g}(\tau_1)(u)=\Tinte_{\Inte}^{g}(\tau_2)(u)$
  \set]$,\\
\hspace*{.5cm} for each $\rho\in R$, for each $x_1, \ldots, x_{Ar(\rho)}\in V$,\\
\hspace*{.75cm} $\Dinte_{\Inte}^{g}\left(\rho(x_1,\ldots,x_{Ar(\rho)})\right)=
\set[$u\in\Unive$
  \set= $\left<g(x_1),\ldots,g(x_{Ar(\rho)})\right>\in\Reli(\rho)$
  \set]$,\\
\hspace*{.5cm} for each $x\in V$, for each $\delta\in D^{\Sigma}$,
$\Dinte_{\Inte}^{g}\left(\exists x\delta\right)=
\set[$u\in\Unive$
  \set= for some $u'\in\overline{\CInt_{\Inte}^u}$\\
        $u\in\Dinte_{\Inte}^{g[x \mapsto u']}(\delta)$
\set]$,\\
\hspace*{.5cm} for each $x\in V$, for each $\delta\in D^{\Sigma}$,
$\Dinte_{\Inte}^{g}\left(\forall x\delta\right)=
\set[$u\in\Unive$
  \set= for each $u'\in\overline{\CInt_{\Inte}^u}$\\
        $u\in\Dinte_{\Inte}^{g[x \mapsto u']}(\delta)$
\set]$,\\
\hspace*{.5cm} for each $\delta\in D^{\Sigma}$,
$\Dinte_{\Inte}^g(\neg\delta)=\Unive\backslash\Dinte_{\Inte}^g(\delta)$,\\
\hspace*{.5cm} for each $\delta_1, \delta_2\in D^{\Sigma}$,
$\Dinte_{\Inte}^g\left((\delta_1\land\delta_2)\right)=\Dinte_{\Inte}^g(\delta_1)\cap\Dinte_{\Inte}^g(\delta_2)$\\
\hspace*{.5cm} for each $\delta_1, \delta_2\in D^{\Sigma}$,
$\Dinte_{\Inte}^g\left((\delta_1\lor\delta_2)\right)=\Dinte_{\Inte}^g(\delta_1)\cup\Dinte_{\Inte}^g(\delta_2)$\\
\hspace*{.5cm} for each $\delta_1, \delta_2\in D^{\Sigma}$,
$\Dinte_{\Inte}^g\left((\delta_1\rightarrow\delta_2)\right)=\left(\Unive\backslash\Dinte_{\Inte}^g(\delta_1)\right)\cup\Dinte_{\Inte}^g(\delta_2)$, 
and\\
\hspace*{.5cm} for each $\delta_1, \delta_2\in D^{\Sigma}$,\\
\hspace*{.75cm}$\Dinte_{\Inte}^g\left((\delta_1\leftrightarrow\delta_2)\right)=
((\Unive\backslash\Dinte_{\Inte}^g(\delta_1))\cap
(\Unive\backslash\Dinte_{\Inte}^g(\delta_2)))\cup
(\Dinte_{\Inte}^g(\delta_1)\cap\Dinte_{\Inte}^g(\delta_2))$.
\end{mydef}

$\Dinte_{\Inte}^g$ is the \emph{$\Sigma$ formula interpretation function with
  respect to $\Inte$ under a variable assignment, $g$, in $\Inte$}.
%The logical
%connectives are interpreted classically, and negation corresponds to set
%complement in the universe of objects.
Sort assignment formulae, $\tau\sim\sigma$, denote sets of objects on
which the attribute path $\tau$ is defined and leads to an object $u'$
of sort $\sigma$. If $\sigma$ is not a species, the object $u'$ must
be of a maximally specific subsort of $\sigma$. Path equations of the
form $\tau_1 \approx \tau_2$ hold of an object $u$ when path
$\tau_1$ and path $\tau_2$ lead to the same object $u'$. And an
$n$-ary relational formula $\rho(x_1,\ldots,x_n)$ denotes a set of
objects such that the $n$-tuples of objects (or chains of objects)
assigned to the variables $x_1$ to $x_n$ are in the denotation of the
relation $\rho$. This means that a relational formula either denotes
the entire universe $\Unive$ or the empty set, depending on the
variable assignment $g$ in $\Inte$. Applying
the idea behind this definition of the interpretation of relational
formulae to a typical HPSG grammar, each well formed phrase described
by an HPSG grammar is such that \texttt{append} holds of exactly those
triples of lists (and chains) in that phrase of which it is supposed
to hold according to the grammarian's definition of
\texttt{append}. This guarantees that the use of \texttt{append}
in grammar principles has the effect it is supposed to have. How exactly
it is achieved will become clear when we introduce the meaning of grammars.

Negation is interpreted as set complement of the denotation of a
formula, conjunction and disjunction of formulae as set intersection
and set union of the denotation of two formulae, respectively. The
meaning of implication and bi-implication follows the pattern of
classical logic and could alternatively be defined on the basis of
negation and disjunction (or conjunction) alone. Quantificational
expressions are special in that they implement the idea of restricted
quantification by referring to the set of components of objects in
$\Inte$.  An existentially quantified formula, $\exists x\delta$,
denotes the set of objects $u$ such that there is at least one
component (or chain of components) $u'$ of $u$, and interpreting $x$
as $u'$ leads to $\delta$ describing $u$.  With universal
quantification, the corresponding condition must hold for \emph{all}
components (or chains of components) of the objects $u$ in the denotation
of the quantified formula. Again turning to the
application of these definitions of formula denotations in grammar
writing, the intuition is that linguists quantify over the components
of grammatical structures (sentences, phrases), and not over a
universe of objects that may include unrelated sentences and
grammatical structures, or components thereof: a certain kind of
object exists within a given structure, or all objects in a certain
structure fulfill certain conditions.


A standard proof shows that the denotation of $\Sigma$ formulae
without free occurrences of variables, i.e.\ the denotation of
$\Sigma$ descriptions, is independent of the initial choice of
variable assignment. For $\Sigma$ descriptions we can thus define a
simpler \emph{$\Sigma$ description denotation function with respect to
  an interpretation $\Inte$}, $\Dinte_{\Inte}$:

\begin{mydef}
For each signature $\Sigma=\left<S,\sqsubseteq,S_{max},A,F,R,Ar\right>$,
for each (full) $\Sigma$ interpretation $\Inte=\Interpretation$,
$\Dinte_{\Inte}$ is the total function from $D_0^{\Sigma}$ to the power
set of $\Unive$ such that
$\Dinte_{\Inte}(\delta)=
\set[$u\in\Unive$
  \set= for each $g\in\VarInt_{\Inte}$, $u\in\Dinte_{\Inte}^g$
\set]$.
\end{mydef}

For each description $\delta$, $\Dinte_{\Inte}$ returns the set of
objects in the universe of $\Inte$ that are described by
$\delta$. With $\Sigma$ descriptions and their denotation as sets of
objects we have everything in place to symbolize all grammar
principles of a grammar such as the one presented by
\citet{PollardSag1994} in logical notation and to receive and
interpretation as intended by the authors. A comprehensive logical
rendering of their grammar of English can be found in Appendix~C of
\citet{Richter2004a-u}. Moreover, as shown there, all syntactic
constructs of the logical languages above are necessary to achieve
that goal without reformulating the grammar.



\section{Meaning of grammars}
\label{sec-grammar-meaning}

Grammars comprise sets of descriptions, the principles of grammar. These sets of
principles are often called \emph{theories} in the context of
logical languages for HPSG, although this terminology can occasionally
be confusing.\footnote{The problem with this term is that
  it can be argued that theories, defined this way, do not constitute what would
  traditionally be called a \emph{theory of a language}, since many central
  aspects of a theory in the latter sense are not embodied in that kind
  of formalized theory.} Theories, i.e.\ sets of descriptions, are
symbolized with $\theta$.  A grammar is simply a theory together with a
signature:

\begin{mydef}\label{def-grammar}
  $\Gamma$ is a \emph{grammar} iff
  
$\Gamma$ is a pair
\( \left<\Sigma, \theta \right>\), where
$\Sigma$ is a signature, and
$\theta \subseteq D_0^{\Sigma}$.
\end{mydef}

Essentially, theories denote the conjunction of their descriptions,
except that theories can, in principle (and contrary to deliberate linguistic
convention), be infinite:

\begin{mydef}
For each signature $\Sigma$,
for each $\Sigma$ interpretation $\Inte=\Interpretation$,
$\Theta_\Inte$ is the total function from the power set of $D_0^{\Sigma}$ to the
power set of $\Unive$ such that
for each $\theta\subseteq D_0^{\Sigma}$,

$\Theta_{\Inte}(\theta)= %\bigcap
\set[
$u\in \Unive$
\set= for each $\delta \in \theta$,
      $u\in\Dinte_{\Inte}(\delta)$
\set]$.
%\{u\in U\mid$ for each $\delta\in\theta$, $u\in D_\Inte(\delta)\}$.
\end{mydef}

$\Theta_{\Inte}$ is the \emph{theory denotation function with respect
  to $\Inte$}. A theory consisting of a set of descriptions holds of
every object $u$ in the universe exactly if every description in the theory
holds of $u$. In short, a theory denotes the set of objects that are described
by everything in the theory. They do not violate any restriction the theory
expresses in one of its descriptions.


A first approximation to the meaning of grammars is provided by the notion
of a $\Gamma$ model, a model of a grammar $\Gamma$:

\begin{mydef}\label{def-rsrl-model}
For each grammar $\Gamma = \left< \Sigma, \theta \right>$,
for each $\Sigma$ interpretation $\Inte=\Interpretation$,
$\Inte$
is a \emph{$\Gamma$ model} iff
\(\Theta_\Inte(\theta) =\Unive\).
\end{mydef}

A $\Gamma$ model is an interpretation $\Inte=\Interpretation$ in which
every description in the theory of grammar $\Gamma$ is true of every object
in the interpretation's universe $\Unive$. In other words, each object
in the interpretation fulfills all conditions which are imposed by
the grammar principles. There is no object in a $\Gamma$ model that
violates any principle.

Linguists use grammars to make predictions about the grammatical
structures of languages. In classical generative terminology, a
grammar undergenerates if there are grammatical structures it does not
capture. It overgenerates if it permits structures that are deemed
ungrammatical. It is uncontroversial that an appropriate notion of the
meaning of a grammar should support linguists in making such
predictions with their grammars. However, the previous notion of
$\Gamma$ models is not strong enough for this purpose. To see this,
suppose there is a signature $\Sigma$ which is fit to describe the
entire English language, and there is a theory $\theta$ which
expresses correctly all and only what there is to say about English. A
$\left< \Sigma, \theta \right>$ model may then consist of a structure
of the single sentence \emph{Elon is frying fish}.  This follows from
the definition of $\Gamma$ models because any appropriate grammar of
English describes all objects of which this sentence consists. But
this one-sentence-model is obviously too small for being a good
candidate for the English language, because English contains much more
than this single sentence. In arbitrary models it cannot be seen if
a grammar undergenerates or overgenerates.


\citet{King99a-u}'s \emph{exhaustive models} are a possibility to
define the meaning of grammars in such a way that they reflect the
basic expectations of generative linguists. The underlying intuition
is to choose a maximal model which contains a congruent copy of any
configuration of objects which can be found in some model of the grammar.

\begin{mydef}
For each grammar $\Gamma = \left< \Sigma, \theta \right>$,
for each $\Sigma$ interpretation $\Inte$,

$\Inte$ is an \emph{exhaustive $\Gamma$ model} iff

$\Inte$ is a $\Gamma$ model, and

for each \(\theta' \subseteq D_0^{\Sigma}\),
for each $\Sigma$ interpretation $\Inte'$,

if $\Inte'$ is a $\Gamma$ model and \(\Theta_{\Inte'}(\theta') \not= \emptyset\)
then \(\Theta_{\Inte}(\theta') \not= \emptyset\).
\end{mydef}

Any grammar with a non-empty model also has a non-empty exhaustive
model.
In addition to being a model of a given grammar $\Gamma = \left<
\Sigma, \theta \right>$, an exhaustive $\Gamma$ model $\Inte$ has the
property that each arbitrarily chosen set of descriptions $\theta'$
which denotes anything at all in any $\Gamma$ model also denotes
something in $\Inte$. An alternative algebraic way to characterize
this requirement is to say that any configuration of objects in any
$\Gamma$ model has a congruent counterpart in an exhaustive $\Gamma$
model. At the same time, since an exhaustive model is from a special class
of \emph{models}, if a description in $\theta$ does not describe some object in
a $\Gamma$ interpretation $\Inte'$, then this object in $\Inte'$ cannot
have a counterpart in an exhaustive $\Gamma$ model.

This is sufficient to capture relevant grammar-theoretic notions of
linguistics: A grammar $\Gamma$ of a language $\mathcal{L}$
overgenerates iff an exhaustive $\Gamma$ model contains configurations
that are not (congruent to) grammatical expressions in $\mathcal{L}$;
it undergenerates iff an exhaustive $\Gamma$ model does not contain
configurations which are (congruent to) grammatical expressions in
$\mathcal{L}$.

% don't discuss the problems (conceptual controversies) of this here yet
% -- postpone to the next section



\section{Alternative conceptions of the meaning of grammars}
\label{sec-alt-gr-meaning}

The exhaustive models of Section~\ref{sec-grammar-meaning} provide a
first solid notion of the meaning of HPSG grammars, but they adopt a
very particular conceptualization of the ontological status of the
structures in the denotation of grammars. This section outlines three
additional alternative ways to define the meaning of HPSG grammars
with different foundational assumptions.

%\citet{King99a-u} wants to\ldots

The theory of exhaustive models from the previous section decidedly supposes that a grammar
denotes a \emph{token} model of a language $\mathcal{L}$. According to
this theory, it is actual well-formed linguistic tokens which are
described by a grammar.  For any occurrence of an utterance of
$\mathcal{L}$ in the real world, the intended exhaustive model
contains the utterance itself. As a consequence, if there are several
occurrences of the same expression type (such as different occurrences
of {\em Elon is frying fish}), the intended exhaustive language model
contains the relevant number of copies of the expression, namely all
its past and future occurrences. However, it is clear that
most conceivable well-formed expressions of any given human language
were never produced and never will be. Since an exhaustive model must
contain all potential well-formed expressions of a language which obey
the principles of grammar, the theory of exhaustive models must admit
\emph{potential tokens} for those utterances which never occur in the
intended exhaustive model. If token models are already suspicious (or
unacceptable) to
some linguists, models comprising non-actual tokens are even more contentious.
Alternative theories of the meaning of grammars have been formulated
to avoid these consequences of exhaustive models.

The feature logical description languages of HPSG support alternative
theories of the meaning of grammars without the need to change the
syntax and semantics of descriptions. There exist four
alternative ways to determine the meaning of HPSG grammars, none of
which means that a given grammar of the form $\left< \Sigma, \theta
\right>$ in the sense of Definition~\ref{def-grammar} has to be
abandoned to adopt the alternative interpretation of the meaning of
grammars. The reason exhaustive models were presented first above is
simply that they require the fewest additional definitions to make them
fully explicit.

In addition to King's theory suggesting that the meaning of a grammar
is an exhaustive model containing the set of potentially non-actual
utterance tokens of a language (henceforth referred to as T1), there
are three other proposals in the literature. In chronological order
these are: (T2) a theory which proposes that grammars should be
interpreted as a set of linguistic types, formalized as a set of
abstract feature structures of a certain kind \citep{PollardSag1994};
(T3) the interpretation of grammars as collections of unique
mathematical idealizations isomorphic to actual language tokens,
outlined in \cite{Pollard99a}; and (T4) a theory of normalized HPSG
grammars (by systematically adding signature extensions and structural
axioms to given grammars) which denote \emph{minimal} exhaustive
models containing configurations of objects structurally identical to
the well-formed utterances of a language, sketched in
\cite{Richter2007a}.

The most traditional view of the meaning of HPSG grammars is the only
one that refers back in its formalization to a specialized variant of
classical feature structures. T2 of \citet{PollardSag1994} proposes
that a grammar denotes a set of mathematical representations of types
of linguistic events. The main intuition is that the object types
abstract away from individual circumstances of token occurrences. The
object types model linguistic token expressions in the sense that an
object type conventionally corresponds to one grammatical expression
of a language. The expressions of a language are observed as tokens in
the real world, for example as occurrences of sentences like
\emph{Elon is frying fish}).  The postulated intuitive correspondence is not
explicated further, but it is expected that a trained linguist will
recognize which object type a linguistic token encountered in the real
world corresponds to.  This loose and informal relation between the
denotation of a grammar (mathematical objects serving as object types)
and the domain of empirically measurable events (utterances of
grammatical expression of a language) is strongly criticized by
\citet{King99a-u}, who argues that it is far from clear how a linguist
would recognize the correspondence and if two linguists would reliably
agree on it. Falsification of the predictions
of a grammar would therefore become unnecessarily hard: The proponents
of a grammar could argue that their grammar is correct because the
correspondence between observed utterances and the object types
admitted by the grammar was not the one assumed by their
detractors. An utterance supposedly not predicted by the grammar could
be argued to correspond to an object type which another linguist did not
think it corresponded to, and an object type
that one linguist says corresponds to an ungrammatical token utterance
(thus claiming that the grammar overgenerates) could be claimed
to correspond  to a grammatical token utterance instead. In addition to this
weakness of the relation between object types and the domain of empirically
accessible data, object types have been objected to as being ontologically
dubious and in any case superfluous.

From a technical perspective, the abstract feature structures of T2
can be thought of as abstractions of configurations of objects under a
root node. The idea of a root node in a concrete feature structure
(conventionally depicted as a graph) corresponds to the object
$u$ in sets of components of an $u$ in an interpretation $\Inte$ as
introduced in Definition~\ref{def-components-of-u}. The abstract
feature structures used as mathematical representations of object
types, however, are not concrete feature structures, since two
concrete feature structures could be isomorphic, in violation of the
idea of object types without duplicates. The necessary abstract
feature structures are usually constructed set-theoretically by
representing each node $\nu$ as equivalence classes of paths that lead to
$\nu$ from the root node, a labeling function which assigns sorts to
these abstract nodes in accordance with the feature appropriateness
function of the signature, and a treatment of nodes in relations as
tuples of abstract nodes. An abstract feature structure satisfaction
function defines what it means for a feature structure to satisfy a
description, which is then elaborated in
the notion of grammars admitting sets of abstract
feature structures.
% explain in a fuller version of the article?

Meaning theory T3 is positioned against the theory of object types for
equivalence classes of linguistic tokens, T2, and against the idea of
employing actual and non-actual linguistic tokens in intended
exhaustive models, T1. With T3 \cite{Pollard99a} is strictly opposed
to a notion of meaning which employs tokens rather than some form
of mathematical idealization as fundamental to grammatical meaning,
and he finds the concept of non-actual tokens inacceptable and
self-contradictory. At the same time, the \emph{strong generative
  capacity} of HPSG grammar rejects T2's ontological commitment to
object types and instead strengthens the relationship between the
structures in the denotation of a grammar and empirically observable
token expressions. Fundamental assumptions of this theory are that no
two structures in the collection denoted by a grammar are structurally
isomorphic, and that each utterance token in the language which is
judged grammatical finds a structurally isomorphic counterpart in the
grammar's strong generative capacity. With the second requirement, T3
tightens the relationship between observables and the mathematical model,
establishing a much stricter link between the predictions of a grammar
and the domain of empirical phenomena than the abstract feature
structure models of \cite{PollardSag1994} offers with its reference
to an intuitive correspondence.

With respect to the technical details, T3 is spelled out on the basis of
models (Definition~\ref{def-rsrl-model}),\footnote{\cite{Pollard99a} is in
  fact based on Speciate Re-entrant Logic (SRL),
  King's precursor of RSRL, but a straightforward extension to full
  RSRL is provided in \cite{Richter2004a-u}.} offering three
alternative ways of characterizing the strong generative capacity of a
grammar. In the terminology presented above, the structures in
Pollard's models can be understood as pairs of interpretations $\Inte$
together with a root node $u$ whose set of components constitute
$\Inte$'s universe. The objects in $\CInt_{\Inte}^u$ are all defined
as canonical representations by a construction employing equivalence
classes of attribute paths originating at the root node: Given a
grammar $\Gamma$, its strong generative capacity can be obtained by taking
the set of all such canonical representations whose interpretations are
$\Gamma$ models. By construction, they are all pairwise non-isomorphic,
and with their internal structure they can be assumed to be structurally
isomorphic to grammatical utterance tokes of a language, in contrast to
the abstract feature structures of \cite{PollardSag1994}. The relationship
to exhaustive models can be understood by noting that the canonical
representations in the strong generative capacity can be abstracted from
each exhaustive model.

One of the main tenets of the theories of the meaning of grammars as
sets of abstract feature structures and as mathematical idealizations
in the strong generative capacity is that there is a one-to-one
correspondence either of object types or of mathematical idealizations
to grammatical utterances in a language. \citet{Richter2007a}
investigates the models of existing HPSG grammars such as the fragment
of English developed in \cite{PollardSag1994} and notes that the
theories of meaning T2 and T3 will necessarily lead to a one-to-many
relationship between grammatical utterances and structures in the
denotation of the grammar: one token utterance leads to more than one
structure in the grammar denotation. Informally, the reason for that
is that for both theories of meaning each structure which corresponds
to a grammatical utterance entails the presence of a potentially large
number of further structures. For the strong generative capacity,
these additional structures come from the substructural nodes in the
mathematical idealization of an utterance which, by design, must
function as root nodes of admissible structures. But these additional
structures are not mathematical idealizations of empirically
observable grammatical utterances. In fact, many of the structures
present in the strong generative capacity do not correspond to
structures which can occur in grammatical utterances at all. For
example, there are many structures under \textit{synsem} nodes in the
denotation of grammars that cannot occur as the \textsc{synsem} value
of signs, because the grammars impose structural restrictions on signs
which are incompatible with the shape of these configurations under a
\textit{synsem} object. Not only are they met-empirical, they are even
explicitly excluded from empirical events. As argued by
\citet{Richter2007a}, this problem even extends to expressions which
have the form of finite sentences containing extraction sites of long
distnace dependencies. These structures may contain configurations
that are impossible in any structure which also contains a filler
linked to the extraction site. In other words, there are no overt
fillers to complete these idealizations in a token utterance, but
these structures are contained
in the grammar denotation, including their potentially observable
phonetic string and a meaning representation, they are predicted to occur
as observable linguistic data.

In response to these problems, T4 develops \emph{normal form
  grammars}, presented as signature and theory extensions applicable
to any HPSG grammar. The basic idea behind the canonical grammar
extension is to partition the denotation of grammars into utterances
and making sure that every connected configuration of objects in a
grammar's denotation is isomorphic to a token utterance in a
language. It is then shown that for T2 and T3 this extension is
insufficient to establish the intended one-to-one correspondence
between observable utterances and object types (T2) or mathematical
idealizations (T3), because the structures predicted by T2 and T3 will
still comprise separate structures corresponding to each
substructure. However, normal form grammars allow the definition of
\emph{minimal} exhaustive models, because normal form grammars can be
shown to have exhaustive models which only contain non-isomorphic
configurations of objects with the additional property that each of
these configurations corresponds to a grammatical utterance. Proposal
T4 is not forced to make any assumptions about the ontological status of
its minimal exhaustive models of normal form grammars, since they do
not have to be defined as a particular kind of mathematical structure
(nor is this option excluded if it is desired). With T3, T4 shares the
commitment to providing an isomorphic structure to each grammatical
utterance of a given language rather than just a corresponding linguistic
type.


HPSG is among a small group of grammar formalisms with a very precise
outline of its formal foundations. It is exceptional with its
alternative characterizations of the meaning of grammars based on one
and the same set of core definitions of the syntax and semantics of
its descriptive devices. This common core of philosophically different
approaches to the scientific descripton of human languages makes their
respective advantages and disadvantages comparable within one single
framework, and it renders the discussion of very abstract concepts
unusually concrete.  Alternative approaches to grammatical meaning
based on different views of the nature of scientific description of an
empirical domain can be investigated and compared with a degree of
detail that is hardly achieved elsewhere in linguistics.



%\medskip
%\#\# mention closed world semantics of feature structures?

%advantage: the logical framework introduced above is capable of making
%all alternatives equally precise and supporting the study of their
%relationships


%\section{Alternative formalisms}
%\label{sec-alt-formalisms}



%The abstract feature structures of the previous section are a degenerated
%version of feature structures as carriers of partial information in
%unification-based grammar frameworks.

%very short remarks on HPSG87 (and related)



%say somewhere, that (technically) HPSG has never been conceived of as
%a PSG (formal language, type 2)









%--------------------------------------------------------------
%\section*{Abbreviations}
%\section*{Abbreviations}


%--------------------------------------------------------------
%\section*{Acknowledgements}


{\sloppy
\printbibliography[heading=subbibliography,notkeyword=this]
}
\end{document}


%------------------------------------------------------------

%\begin{table}
%\caption{Frequencies of word classes}
%\label{tab:1:frequencies}
% \begin{tabular}{lllll} 
%  \lsptoprule
%            & nouns & verbs & adjectives & adverbs\\ 
%  \midrule
%  absolute  &   12 &    34  &    23     & 13\\
%  relative  &   3.1 &   8.9 &    5.7    & 3.2\\
%  \lspbottomrule
% \end{tabular}
%\end{table}



%\ea
%\gll cogito                           ergo      sum\\  
%     think.\textsc{1sg}.\textsc{pres} therefore \textsc{cop}.\textsc{1sg}.\textsc{pres}\\ 
%\glt `I think therefore I am.'
%\z



