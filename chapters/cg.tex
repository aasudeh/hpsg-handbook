\documentclass[output=paper]{langsci/langscibook} 
\author{Yusuke Kubota\affiliation{University of Tsukuba}}
\title{HPSG and Categorial Grammar}

% \chapterDOI{} %will be filled in at production

%\epigram{Change epigram in chapters/03.tex or remove it there }
\abstract{This chapter aims to offer an up-to-date comparison of HPSG and
 categorial grammar (CG). Since the CG research itself
 consists of two major types of approaches with overlapping but
 distinct goals and research strategies, I start by giving an overview
 of these two variants of CG (section 2). This is
 followed by a comparison of HPSG and CG at a broad level, in terms of
 the general architecture of the theory (section 3), and then, by a
 somewhat more detailed comparison of specific linguistic analyses
 of some selected phenomena (section 4). The chapter ends by briefly
 touching on issues related to computational implementation and
 human sentence processing (section 5). Thoughtout the discussion, I  attempt to
 highlight both the similarities and differences between HPSG and CG
 research, in the hope of stimulating further research in the two
 research communities on their respective open questions, and so that the two
 communities can continue to learn from each other.}
\maketitle

\begin{document}
\label{chap-cg}


\section{Introduction}

The goal of this chapter is to provide a comparison between HPSG and
\term{categorial grammar} (CG). The two theories share certain important
insights, mostly due to the fact that they are among the so-called
`lexicalist', `non"=transformational' theories of syntax that were
proposed as major alternatives to the mainstream transformational
syntax in the 1980s (see \citet{BB2011a} for an overview of these
theories). However, due to the differences in the main research goals
in the respective communities in which these approaches have been
developed, there are certain nontrivial differences between them as
well. The present chapter assumes researchers working in HPSG or other
non-CG theories of syntax as its main audience, and aims to inform
them of key aspects of CG which make it distinct from other theories
of syntax. While computational implementation and investigations of the
formal properties of grammatical theory have been important in both
HPSG and CG research, I will primarily focus on the linguistic aspects
in the ensuing discussion, with pointers to literature on mathematical
and computational studies. Throughout the discussion, I presuppose
basic familiarity with HPSG (with pointers to relevant chapters in the
handbook). The present handbook contains chapters that compare HPSG
with other grammatical theories, including the present one. I
encourage the reader to take a look at the other theory comparison
chapters too (as well as other chapters dealing with specific
aspects of HPSG in greater detail), in order to obtain a fuller picture of the
theoretical landscape in current (non-transformational) generative
syntax research.


\section{Two varieties of CG }

CG is actually not a monolithic theory, but is a family of related
approaches (or, perhaps more accurately, it is much \emph{less of} a
monolithic theory than either HPSG or LFG is). For this reason, I will
start my discussion by sketching some important features of two major
varieties of CG, \term{Combinatory Categorial Grammar} (CCG)
\citep{Steedman2000a-u,steedman2012} and \term{Type-Logical Categorial Grammar}
(TLCG; or `Type-Logical Grammar')
\citep{Morrill94a-u,Moortgat2011a-u}.\footnote{For 
more detailed introductions to these different variants of CG, see 
\citet{steedman2011ccg} (on CCG) and \citet{oehrle2011} (on 
TLCG), both included in \citet{BB2011a}.} After presenting the `core'
component of CG that is shared between the two approaches---which is
commonly referred to as the `AB grammar'---I introduce
aspects of the respective approaches in which they diverge from each
other.


\subsection{Notation and presentation }

Before getting started, some comments are in order as to the notation
and the mode of presentation adopted. Two choices are made for the notation.
First, CCG and TLCG traditionally adopt different notations of the
slash. I stick to the TLCG notation throughout this chapter for
notational consistency. Second, I present all the fragments below in
the so-called \term{labelled deduction} notation of (Prawitz-style) natural
deduction. In particular, I follow \citet{oehrle1994} and \citet{Morrill94a-u} in
the use of `term labels' in labelled deduction to represent prosodic
and semantic information of linguistic expressions. This involves
writing linguistic expressions as \emph{tripartite signs}, formally, tuples
of prosodic form, semantic interpretation and syntactic category (or
syntactic type). Researchers familiar with
HPSG should find this notation easy to read and
intuitive; the idea is essentially the same as how
linguistic signs are conceived of in HPSG. In the CG literature, this
notation has its roots in the conception of `multidimensional'
linguistic signs in earlier work by Dick Oehrle \citep{oehrle88}. But the
reader should be aware that this is \emph{not} the standard notation in
which either CCG or TLCG is typically presented. Also, logically savvy
readers may find this notation somewhat confusing since it
(unfortunately) obscures certain aspects of CG pertaining to its
logical properties. In any event, it is important to keep in mind that
different notations co-exist in the CG literature (and the logic
literature behind it) partly because of standard convention and
practice in different (sub)fields and partly because of the need to
highlight different aspects of the same formal system in different
contexts. For the mode of presentation, the emphasis is consistently
on linguistic (rather than computational or logical) aspects, with
pointers to the relevant literature for readers interested in these
other aspects. The presentation below moreover will not necessarily
aim for historical accuracy, and I have chosen to gloss over certain minor
differences for the sake of facilitating comparison among 
different subspecies of CG, and, more generally, between CG and HPSG
(and other grammatical theories). The reader is therefore encouraged to
consult primary sources for more accurate (and authoritative) presentations of
each of the different subspecies of CG discussed below.



\subsection{The AB grammar \label{ab}}

I start with a simple fragment of CG called the \term{AB grammar},
consisting of just two syntactic rules in (\ref{SE0}):

\begin{samepage2}
\begin{exe}
 \ex\label{SE0} %\\[-15pt]
     \begin{multicols}{2} 
\begin{xlist}
 \ex\label{rse} \mbox{\term{Forward Slash Elimination}} 

\vspace*{-.2cm}
\begin{prooftree}
\NoSem
\hspace*{-2.5cm}
\def\defaultHypSeparation{\hskip.2in}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ }}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ }}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ }}{\syncat{\textit{A}}}}
\end{prooftree}

 \ex\label{lse} \mbox{\term{Backward Slash Elimination}} 

\begin{prooftree}
\NoSem
\hspace*{-2.5cm}
\def\defaultHypSeparation{\hskip.2in}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ }}{\syncat{\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ }}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ }}{\syncat{\textit{A}}}}
\end{prooftree}

\end{xlist} 
     \end{multicols}
\end{exe}
\end{samepage2}

\noindent
With a somewhat minimal lexicon in (\ref{lex1}), we can license the sentence
\textit{John saw Mary} as in (\ref{tree1}). The two slashes / and \ensuremath{\backslash}\
are used to form `complex' syntactic categories (more on this below)
indicating valence information: the transitive verb \textit{loves} is
assigned the  category \syncat{(NP\ensuremath{\backslash}{}S)/NP} since it first combines an NP to
its  right (i.e.\  the direct object) and then another NP to its right
(i.e.\  the subject).

\begin{exe}
 \ex\label{lex1}
  \begin{xlist}
 \ex\label{}
    \pt{john}; \  \syncat{NP}
 \ex\label{}
    \pt{mary}; \  \syncat{NP}
 \ex\label{intr}
    \pt{ran}; \  \syncat{NP\ensuremath{\backslash}{}S}
 \ex\label{tr}
    \pt{loves}; \ \syncat{(NP\ensuremath{\backslash}{}S)/NP}
  \end{xlist}
\end{exe}
\begin{samepage2}
\begin{exe}
 \ex\label{tree1}
\end{exe}
\vspace*{-1.4cm}
\begin{prooftree}
\NoSem
\DerivSize
\AxiomC{\LexEnt{\pt{john}}{\sem{ }}{\syncat{NP}}}
\AxiomC{\LexEnt{\pt{mary}}{\sem{ }}{\syncat{NP}}}
\AxiomC{\LexEnt{\pt{loves}}{\sem{ }}{\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{loves \ensuremath{\circ}\xspace mary}}{\sem{ }}{\syncat{NP\ensuremath{\backslash}{}S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace mary}}{\sem{ }}{\syncat{S}}}
\end{prooftree}
\end{samepage2}

\noindent
At this point, this is just like the familiar PSG analysis of the
following form, except that the symbol VP is replaced by \syncat{NP\ensuremath{\backslash}{}S}:

\begin{exe}
 \ex\label{}
  \Tree [.S [.NP John ] [.VP [.V loves ] [.NP Mary ] ] ]
\end{exe}
\noindent
Things will start looking more interesting as we make the fragment
more complex (and also by adding the semantics), but before doing so
I first introduce
some basic assumptions, first on syntactic categories (below) and then
on semantics (next section).

\term{Syntactic categories} (or \term{syntactic types}) are defined recursively
in CG. This can be concisely written using the so-called `BNC
notation' as follows:\footnote{See section \ref{sec:morphology} below
for the treatment of syntactic features (such as those used for
agreement). I ignore this aspect for the fragment developed below for the sake of
exposition. The treatment of syntactic features (or its analog)
is a relatively underdeveloped aspect of CG syntax literature,
as compared to HPSG research (where the whole linguistic theory is
built on the basis of a theory/formalism of complex feature
structures). See section \ref{sec:morphology}
below.}$^,$\footnote{Recognizing PP as a basic type is somewhat 
non-standard, although there does not seem to be any consensus on
what should be regarded as a (reasonably complete) set of
basic syntactic types for natural language syntax.}

\begin{exe}
 \ex\label{cat-def}
  \begin{xlist}
 \ex\label{bascat}
    BaseType := \{ N,  NP, PP, S \}
 \ex\label{complex-cat}
    Type := BaseType $|$ Type\ensuremath{\backslash} Type $|$ Type/Type
  \end{xlist}
\end{exe}
In words, anything that is a BaseType is a Type, and
any complex expression of form A\ensuremath{\backslash} B  or A/B where A and B are both
Types is a Type. To give some examples, the following expressions are
syntactic types according to the definition in
(\ref{complex-cat}):\footnote{I omit
parentheses for a sequence of the same type of slash, for which
disambiguation is obvious---for example, \syncat{A\ensuremath{\backslash}{}A\ensuremath{\backslash}{}A} is an abbreviation for \syncat{(A\ensuremath{\backslash}{}(A\ensuremath{\backslash}{}A))}.}

\begin{exe}
 \ex\label{}
  \begin{xlist}
 \ex\label{}
    \syncat{S\ensuremath{\backslash}{}S}
 \ex\label{}
    \syncat{(NP\ensuremath{\backslash}{}S)/NP/NP}
 \ex\label{}
    \syncat{(S/(NP\ensuremath{\backslash}{}S))\ensuremath{\backslash}{}(S/NP)}
 \ex\label{}
    \syncat{((NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S))\ensuremath{\backslash}{}((NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S))}
  \end{xlist}
\end{exe}

One important feature of CG is that, like HPSG, it lexicalizes the
valence (or subcategorization) properties of linguistic expressions.
Unlike HPSG, where this is done by a list (or set) valued syntactic
feature, in CG, complex syntactic categories directly represent the
combinatoric (i.e. valence) properties of lexical items. For example,
lexical entries for intransitive and transitive verbs in English will
look like the following (semantics is omitted here but will be
supplied later):

\begin{exe}
 \ex\label{lex1}
  \begin{xlist}
 \ex\label{intr}
    \pt{ran}; \  \syncat{NP\ensuremath{\backslash}{}S}
 \ex\label{tr}
    \pt{read}; \ \syncat{(NP\ensuremath{\backslash}{}S)/NP}
 \ex\label{tr}
    \pt{introduces}; \ \syncat{(NP\ensuremath{\backslash}{}S)/PP/NP}
  \end{xlist}
\end{exe}
(\ref{intr}) says that the verb \textit{ran} combines with its argument NP \emph{to its
left} to become an S. Likewise, (\ref{tr}) says that \textit{read} first
combines with an NP \emph{to its right} and then another NP to its left to
become an S.

One point to keep in mind (though it may not seem to make much
difference at this point) is that in CG, syntactic rules are
thought of as logical rules and the derivations of sentences like
(\ref{tree1}) as \emph{proofs} of the well-formedness of
particular strings as sentences.
From this `logical' point of view, the two slashes should really be
thought of as directional variants of implication (that is, both
\syncat{\textit{A}/\textit{B}} and \syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}} essentially mean `\emph{if} there is a \textit{B},
\emph{then} there is an \textit{A}'), and the two rules of Slash Elimination
introduced in (\ref{SE0})
should be thought of as directional variants of \term{modus ponens}
($B \ensuremath{ \rightarrow } A, B  \ensuremath{\vdash\xspace } A$). This analogy between natural language
syntax and logic is emphasized in particular in the TLCG research.


\subsection{Syntax-semantics interface in CG \label{interface}}

One attractive property of CG as a theory of natural language syntax
is its straightforward syntax-semantics
interface. In particular, there is a 
functional mapping from syntactic categories to semantic
types.\footnote{Technically, this is ensured in TLCG by the 
homomorphism from the syntactic type logic to the semantic type logic
(the latter of which is often implicit) and the so-called Curry-Howard
correspondence between proofs and terms. \mgcmt{ref to van Benthem}} For
the sake of exposition, I assume an extensional fragment of
Montagovian model-theoretic semantics in what follows, but it should be noted that the
CG syntax is neutral to the choice of the specific variant of
semantics to go with it.\footnote{See for example \citet{martin2013} and
\citet{bekkimineshima17} for recent proposals on adopting compositional variants
of (hyper)intensional dynamic semantics and proof theoretic semantics,
respectively, for the semantic component of CG-based theories of
natural language.}

Assuming the standard recursive definition of semantic types
as in (\ref{semtyp-def}) (with basic types $e$ (individuals) and $t$ (truth
values)), we can define the function \SemTyp\ that returns, for each
syntactic category given as input, its semantic type, as in
(\ref{semtyp-base}) and (\ref{semtyp-recur}).

\begin{exe}
 \ex\label{semtyp-def}
  \begin{xlist}
 \ex\label{}
    BaseSemType := \{ $e$, $t$ \}
 \ex\label{}
    SemType := BaseSemType $|$ SemType \ensuremath{ \rightarrow } SemType
  \end{xlist}
 \ex\label{semtyp-base}
  (Base Case)
  \begin{xlist}
 \ex\label{semtyp-np}
    \SemTyp(NP) = \SemTyp($\PP$) = $e$
 \ex\label{semtyp-np}
    \SemTyp(N) = $e \ensuremath{ \rightarrow } t$
 \ex\label{semtyp-s}
    \SemTyp(S) = $t$
  \end{xlist}
 \ex\label{semtyp-recur}
  (Recursive Clause)
  
  For any complex syntactic category of the form
     \syncat{\textit{A}/\textit{B}} (or \syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}), 
  
     \SemTyp(\syncat{\textit{A}/\textit{B}})
     (= \SemTyp(\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}})) =
     \SemTyp(\textit{B}) \ensuremath{ \rightarrow } \SemTyp(\textit{A})
\end{exe}
For example, we have \SemTyp(\syncat{S/(NP\ensuremath{\backslash}{}S)}) =  $(e \ensuremath{ \rightarrow } t) \ensuremath{ \rightarrow } t$
(for subject position quantifier in CCG).

Syntactic rules with semantics can then be written as in (\ref{SE-rev})
(where the semantic effect of these rules is \term{function application})
and a sample derivation with semantic annotation is given in (\ref{tree02}).

\begin{samepage2}
\begin{exe}
 \ex\label{SE-rev} %\\[-15pt]
     \begin{multicols}{2} 
\begin{xlist}
 \ex\label{rse} \mbox{\term{Forward Slash Elimination}} 

\vspace*{-.2cm}
\begin{prooftree}
\hspace*{-1cm}
\def\defaultHypSeparation{\hskip.2in}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \sF(\sG)}}{\syncat{\textit{A}}}}
\end{prooftree}

 \ex\label{lse} \mbox{\term{Backward Slash Elimination}} 

\begin{prooftree}
\hspace*{-1cm}
\def\defaultHypSeparation{\hskip.2in}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ \sF(\sG) }}{\syncat{\textit{A}}}}
\end{prooftree}

\end{xlist} 
     \end{multicols}
\end{exe}
\end{samepage2}


\begin{samepage2}
\begin{exe}
 \ex\label{tree02}
\end{exe}
\vspace*{-1.2cm}
\begin{prooftree}
\DerivSize
\AxiomC{\LexEnt{\pt{john}}{\sem{ \trns{j} }}{\syncat{NP}}}
\AxiomC{\LexEnt{\pt{loves}}{\sem{ \trns{love} }}{\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\AxiomC{\LexEnt{\pt{mary}}{\sem{ \trns{m} }}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{loves \ensuremath{\circ}\xspace mary}}{\sem{ \trns{love}(\trns{m}) }}{\syncat{NP\ensuremath{\backslash}{}S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace mary}}{\sem{ \trns{love}(\trns{m})(\trns{j}) }}{\syncat{S}}}
\end{prooftree}
\end{samepage2}

\noindent
A system of CG with only the Slash Elimination rules like the fragment
above is called the \term{AB grammar}, so called because it corresponds to
the earliest form of CG formulated by
\citet{Ajdukiewicz35a-u} and \citet{bar-hillel53}.


\subsection{Combinatory Categorial Grammar \label{sec:CCG}}

\subsubsection{An `ABC' fragment: AB grammar with order-preserving `combinatory' rules \label{preCCG}}

To do some interesting linguistic analysis, we need some more
machinery. I now extend the AB fragment above by adding two types of
rules: \term{Type-Raising} and (Harmonic) \term{Function Composition}. These are
a subset of rules typically entertained in CCG. I call the resultant
system \term{ABC Grammar} (AB + Function
\underline{C}omposition).\footnote{This is not a standard terminology,
giving a name to this fragment is convenient for the purpose of the
discussion below.} Though it is an impoverished version of CCG, it
already enables an interesting and elegant analysis of
\term{nonconstituent coordination} (NCC), originally due to \citet{Steedman85a-u} and
\citet{Dowty88a-u}, which is essentially identical to the analysis of NCC in
the current versions of both CCG and TLCG. I will then discuss the
rest of the rules constituting CCG in the next section. The reason for
drawing a distinction between the `ABC' fragment and (proper) CCG is
just for the sake of exposition. The rules introduced in the present
section have the property that they are all derivable as \emph{theorems} in
the (associative) Lambek calculus, the calculus that underlies most
variants of TLCG. For this reason, separating the two sets of rules
helps clarify the similarities and differences between CCG and TLCG.

The \term{type raising} and \term{function composition} rules are defined as in
(\ref{FC}) and (\ref{TR}), respectively. 

\begin{samepage2}
\begin{multicols}{2}
\begin{exe}
 \ex\label{FC} \begin{xlist}
      \ex\mbox{Forward Function Composition} 

\vspace*{-0cm}
\begin{prooftree}
\hspace*{-1cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG}}{\syncat{\textit{B}/\textit{C}}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(\sG(x))}}{\syncat{\textit{A}/\textit{C}}}}
\end{prooftree}

         \columnbreak
      \ex\mbox{Backward Function Composition}

\vspace*{-0cm}
\begin{prooftree}
\hspace*{-1cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG}}{\syncat{\textit{C}\ensuremath{\backslash}{}\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ \lambda x. \sF(\sG(x))}}{\syncat{\textit{C}\ensuremath{\backslash}{}\textit{A}}}}
\end{prooftree}

     \end{xlist}

\end{exe}
\end{multicols}
\end{samepage2}

\begin{samepage2}
\begin{multicols}{2}
\begin{exe}
 \ex\label{TR} \begin{xlist}
      \ex\mbox{Forward Type Raising} 

\vspace*{-0cm}
\begin{prooftree}
\hspace*{-1cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{A}}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\LexEnt{\pt{\ptv{a} }}{\sem{ \lambda v. v(\sF) }}{\syncat{\textit{B}/(\textit{A}\ensuremath{\backslash}{}\textit{B})}}}
\end{prooftree}

         \columnbreak
      \ex\mbox{Backward Type Raising}

\vspace*{-0cm}
\begin{prooftree}
\hspace*{-1cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF}}{\syncat{\textit{A}}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\LexEnt{\pt{\ptv{a} }}{\sem{ \lambda v. v(\sF) }}{\syncat{(\textit{B}/\textit{A})\ensuremath{\backslash}{}\textit{B}}}}
\end{prooftree}

     \end{xlist}

\end{exe}
\end{multicols}
\end{samepage2}
\medskip

\noindent
The Type-Raising rules are essentially rules of `type lifting'
familiar in the formal semantics literature, except that they specify
the `syntactic effect' of type lifting explicitly. Similarly
Function Composition rules can be understand as function composition
in the usual sense (as in mathematics and functional programming),
except, again, that the syntactic effects are explicitly specified.

As noted by \citet{Steedman85a-u}, with Type Raising and Function
Composition, we can analyze a string of words such as \textit{John loves} as
a constituent of type S/NP, that is, an expression that is looking for
an NP to its right to become an S:

\begin{samepage2}
\begin{exe}
 \ex\label{ }
\end{exe}
\vspace*{-1.2cm}
\begin{prooftree}
\hspace*{-1cm}
\small
\AxiomC{\LexEnt{\pt{john }}{\sem{ \trns{j} }}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\LexEnt{\pt{john }}{\sem{ \lambda f. f(\trns{j}) }}{\syncat{S/(NP\ensuremath{\backslash}{}S)}}}
\AxiomC{\LexEnt{\pt{loves}}{\sem{ \trns{love} }}{\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves }}{\sem{ \lambda x. \trns{love}(x)(\trns{j})}}{\syncat{S/NP}}}
\end{prooftree}
\end{samepage2}

\noindent Assuming generalized conjunction (with the standard definition for
the generalized conjunction operator \ensuremath{ \sqcap\xspace } \emph{a la}
\citet{partee-rooth1983a} and the polymorphic syntactic category
\syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})/\textit{X}}) for \textit{and}, the analysis for a right-node raising
(RNR) sentence such as (\ref{RNR-CCG}) is straightforward, as in (\ref{RNR-CCG-DV}).

\begin{exe}
 \ex\label{RNR-CCG}
  John loves, and Bill hates, Mary.
\end{exe}
\begin{samepage2}
\begin{exe}
 \ex\label{RNR-CCG-DV}
\end{exe}
\vspace*{-1.5cm}
\begin{prooftree}
\hspace*{-.5cm}
\small
\AxiomC{\Lemma}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont john \ensuremath{\circ}\xspace loves  }; \\ \sem{ \lambda x. \trns{love}(x)(\trns{j})}; \syncat{S/NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont and  }; \\ \sem{ \ensuremath{ \sqcap\xspace } }; \syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})/\textit{X}}}}
\AxiomC{\Lemma}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont bill \ensuremath{\circ}\xspace hates }; \\ \sem{ \lambda x. \trns{hate}(x)(\trns{b})}; \syncat{S/NP}}}
\RightLabel{\scalebox{.8}{FA}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont and \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace hates }; \\ \sem{ \ensuremath{ \sqcap\xspace } (\lambda x. \trns{hate}(x)(\trns{b}))}; \syncat{(S/NP)\ensuremath{\backslash}{}(S/NP)}}}
\RightLabel{\scalebox{.8}{FA}}
\BinaryInfC{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace hates}}{\sem{ (\lambda x. \trns{love}(x)(\trns{j})) \ensuremath{ \sqcap\xspace } (\lambda x. \trns{hate}(x)(\trns{b}))}}{\syncat{S/NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont mary  }; \\ \sem{ \trns{m} }; \syncat{NP}}}
\RightLabel{\scalebox{.8}{FA}}
\BinaryInfC{\LexEnt{\pt{john \ensuremath{\circ}\xspace loves \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace hates \ensuremath{\circ}\xspace mary}}{\sem{ \trns{love}(\trns{m})(\trns{j}) \ensuremath{ \wedge\xspace } \trns{hate}(\trns{m})(\trns{b})}}{\syncat{S}}}
\end{prooftree}
\end{samepage2}

\citet{Dowty88a-u} showed  that this analysis extends straightforwardly to
the (slightly) more complex case of argument cluster coordination
(ACC), such as (\ref{DCC-EG}), as in (\ref{ACC-CCG-DV}) (here, VP, TV and DTV are
abbreviations of \syncat{NP\ensuremath{\backslash}{}S}, \syncat{(NP\ensuremath{\backslash}{}S)/NP}, and \syncat{(NP\ensuremath{\backslash}{}S)/NP/NP}, respectively).

\begin{exe}
 \ex\label{DCC-EG}
  Mary gave Bill the book and John the record.
\end{exe}
\begin{samepage2}
\begin{exe}
 \ex\label{ACC-CCG-DV}
\end{exe}
\vspace*{-1.5cm}
\begin{prooftree}
\hspace*{-.2cm}
\small
\AxiomC{\MultiLine{\mathsf{ \ptfont mary  }; \\ \sem{ \trns{m}  }; \\\syncat{NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont gave  }; \\ \sem{ \trns{give} }; \\\syncat{DTV}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont bill  }; \\ \sem{ \trns{b}  }; \syncat{NP}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont bill  }; \\ \sem{ \lambda P. P(\trns{b})  }; \\\syncat{DTV\ensuremath{\backslash}{}TV}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont the \ensuremath{\circ}\xspace book  }; \\ \sem{ \I(\trns{bk})}; \syncat{NP}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont the \ensuremath{\circ}\xspace book  }; \\ \sem{ \lambda Q. Q(\I(\trns{bk}))}; \\\syncat{TV\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book  }; \\ \sem{ \lambda R. R(\trns{b})(\I(\trns{bk})) }; \syncat{DTV\ensuremath{\backslash}{}VP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont and  }; \\ \sem{ \ensuremath{ \sqcap\xspace } }; \\\syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})/\textit{X}}}}
\AxiomC{\Lemma}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \lambda R. R(\trns{j})(\I(\trns{rc})) }; \\\syncat{DTV\ensuremath{\backslash}{}VP}}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \ensuremath{ \sqcap\xspace } (\lambda R. R(\trns{j})(\I(\trns{rc}))) }; \\\syncat{(DTV\ensuremath{\backslash}{}VP)\ensuremath{\backslash}{}(DTV\ensuremath{\backslash}{}VP)}}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \lambda R. R(\trns{j})(\I(\trns{rc})) \ensuremath{ \sqcap\xspace } R(\trns{b})(\I(\trns{bk})) }; \syncat{DTV\ensuremath{\backslash}{}VP}}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont gave \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \trns{give}(\trns{j})(\I(\trns{rc})) \ensuremath{ \sqcap\xspace } \trns{give}(\trns{b})(\I(\trns{bk})) }; \syncat{VP}}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont mary \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book \ensuremath{\circ}\xspace and \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace record  }; \\ \sem{ \trns{give}(\trns{j})(\I(\trns{rc}))(\trns{m}) \ensuremath{ \wedge\xspace } \trns{give}(\trns{b})(\I(\trns{bk}))(\trns{m}) }; \syncat{S}}}
\end{prooftree}
\end{samepage2}

\noindent
Here, by Type Raising, the indirect and direct objects become
functions that can be combined via Function Composition, to form
a non-standard constituent that can be coordinated.  After two such
expressions are conjoined, the verb is fed as an argument to return a VP.
Intuitively, the idea behind this analysis is that
\textit{Bill the book} is of type \syncat{DTV\ensuremath{\backslash}{}VP} since if it were to combine with
an actual ditransitive verb (such as \textit{gave}), we would obtain
a VP (\textit{gave Bill a book}). 
Note that in both the RNR and ACC examples above, the right semantic
interpretation for the whole sentence is
assigned compositionally via the general definitions of
type raising and function composition given above.


\subsubsection{From ABC to CCG }

CCG is a version of CG developed by Mark Steedman since the 1980s with
extensive linguistic application. The best sources for CCG are the
three books by Steedman \citep{Steedman97a,Steedman2000a-u,steedman2012},
which present treatments of major linguistic phenomena in CCG, and
gives pointers to earlier literature. CCG is essentially a rule-based
extension of the AB grammar. We have already seen in the previous
section the two key components that constitute this extension: Type
Raising and (Harmonic) Function Composition.\footnote{There is
actually a subtle point about Type Raising rules. It seems that recent
versions of CCG \citep{steedman2012} do not take them to be syntactic rules
but rather assume that Type Raising is an operation in the lexicon,
due to parsing considerations. It is also worth noting in this
connection that the CCG-based syntactic fragment that
\citet{jacobson1999a,jacobson2000a} assumes for her Variable-Free Semantics
is actually a quite different system from Steedman's version of CCG in
that it crucially assumes Geach rules (which increases the complexity
of the syntactic type of the linguistic expression) in the syntactic
component.} There are aspects of natural language syntax that cannot
be handled adequately in this simple system, and in such situations,
CCG makes (restricted) use of additional rules. This point can be
illustrated nicely with two issues that arise in connection with the
analysis of long-distance dependencies.

The basic idea behind the CCG analysis of long-distance dependencies,
due originally to \citet{AS82a}, is very simple and is similar in spirit to
the HPSG analysis in terms of SLASH feature percolation. Specifically,
CCG analyzes extraction dependency via a chain of function
composition, as illustrated by the derivation for (\ref{ldd-EX}) in (\ref{ldd-CCG-DV}).

\begin{exe}
 \ex\label{ldd-EX}
  This is the book that John thought that Mary read {\gp}\xspace .  
\end{exe}
\begin{samepage2}
\begin{exe}
 \ex\label{ldd-CCG-DV}
\end{exe}
\vspace*{-1.8cm}
\begin{prooftree}
\footnotesize
\AxiomC{\MultiLine{\mathsf{ \ptfont that   }; \\ \sem{ \lambda P \lambda Q \lambda x. \\ \hspace{2ex} Q(x) \ensuremath{ \wedge\xspace } P(x) }; \\\syncat{(N\ensuremath{\backslash}{}N)/(S/NP)}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont john  }; \\ \sem{ \trns{j} }; \syncat{NP}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont john  }; \\ \sem{ \lambda P. P(\trns{j}) }; \\\syncat{S/(NP\ensuremath{\backslash}{}S)}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont thought   }; \\ \sem{ \trns{think}   }; \\\syncat{(NP\ensuremath{\backslash}{}S)/S'}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont that  }; \\ \sem{ \lambda p. p  }; \\\syncat{S'/S}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont mary }; \\ \sem{ \trns{m} }; \syncat{NP}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont mary }; \\ \sem{ \lambda P. P(\trns{m}) }; \\\syncat{S/(NP\ensuremath{\backslash}{}S)}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont read }; \\ \sem{ \trns{read} }; \\\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{read}(x)(\trns{m}) }; \syncat{S/NP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{read}(x)(\trns{m}) }; \syncat{S'/S}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont thought \ensuremath{\circ}\xspace that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{think}(\trns{read}(x)(\trns{m})) }; \syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont john \ensuremath{\circ}\xspace thought \ensuremath{\circ}\xspace that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda x. \trns{think}(\trns{read}(x)(\trns{m}))(\trns{j}) }; \syncat{S/NP}}}
\RightLabel{\scalebox{.8}{FA}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont that \ensuremath{\circ}\xspace john \ensuremath{\circ}\xspace thought \ensuremath{\circ}\xspace that \ensuremath{\circ}\xspace mary \ensuremath{\circ}\xspace read  }; \\ \sem{ \lambda Q \lambda x. Q(x) \ensuremath{ \wedge\xspace } \trns{think}(\trns{read}(x)(\trns{m}))(\trns{j})}; \syncat{N\ensuremath{\backslash}{}N}}}
\end{prooftree}
\end{samepage2}


Like (many versions of) HPSG, CCG does not assume any empty expression
at the gap site. Instead, the information that the subexpression such
as \textit{Mary read} and \textit{thought that Mary read} are missing an NP on the
right edge is encoded in the syntactic category of the linguistic
expression. \textit{Mary read} is assigned the type \syncat{S/NP} Since it is a
sentence missing an NP on its right edge. \textit{thought that Mary read} is
of type \syncat{VP/NP} Since it is a VP missing an NP on its right edge, etc.
Expressions that are not originally functions (such as the subject NPs
in the higher and lower clauses inside the relative clause in
(\ref{ldd-EX})) are first type-raised. Then, function composition
effectively `delay' the saturation of the object NP argument of the
embedded verb, until the whole relative clause meets the relative
pronoun, which itself is a higher-order function that takes a sentence
missing an NP (of type \syncat{S/NP}).

The successive passing of the /NP specification to larger structures
is essentially analogous to the treatment of extraction via the SLASH
feature in HPSG. However, unlike HPSG, which has a dedicated feature
that handles this information passing, CCG achieves the effect via the
ordinary slash that is also used for local syntactic
composition.\footnote{There are several complications that arise in
  the CCG analysis because of this architectural difference, which I
  discuss immediately below. But one possible welcome consequence of the CCG
  approach is that the quite complex set of specifications of SLASH
  feature percolation (including the \emph{termination} of percolation)
  are all unnecessary in CCG.}

This difference immediately raises some issues for the CCG analysis of
extraction. First, in (\ref{ldd-EX}), the NP gap happens to be on the right
edge of the sentence, but this is not always the  case. Harmonic
function composition alone cannot  handle non-peripheral
extraction of the sort found in examples such as the following:

\begin{exe}
 \ex\label{nonP}
  This is the book that John thought that [Mary read {\gp}\xspace at school].
\end{exe}
Assuming that  \textit{at school} is a VP modifier of type \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)},
what is needed here is a mechanism that assigns the type \syncat{VP/NP}
to the string \textit{read {\gp}\xspace at school}, despite the
fact that  the missing NP is not  
on the right edge. CCG employs a special  rule of `mixed' function
composition for this purpose, defined as follows:

\begin{exe}
 \ex\label{mc}
  Crossed Function Composition
\begin{prooftree}
\hspace*{-5cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sG}}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sF}}{\syncat{\textit{A}\ensuremath{\backslash}{}\textit{C}}}}
\RightLabel{\scalebox{.8}{xFC}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(\sG(x))}}{\syncat{\textit{C}/\textit{B}}}}
\end{prooftree}
\end{exe}
Unlike the harmonic counterpart of (\ref{mc}) (in which \ptv{a} has the type
\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}), the directionality of the premise is different
in the two premises, and the resultant category inherits the
slash of the main functor (i.e.\  \syncat{\textit{B}\ensuremath{\backslash}{}\textit{C}}).

Once this non-order-preserving version of function composition is 
introduced in the grammar, the derivation for (\ref{nonP}) is
straightforward, as in (\ref{nonP-DV}):

\begin{samepage2}
\begin{exe}
 \ex\label{nonP-DV}
\end{exe}
\vspace*{-1.2cm}
\begin{prooftree}
\DerivSize
\AxiomC{\LexEnt{\pt{mary}}{\sem{ \trns{m} }}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{TR}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont mary }; \\ \sem{ \lambda P. P(\trns{m}) }; \\\syncat{S/(NP\ensuremath{\backslash}{}S)}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont read }; \\ \sem{ \trns{read} }; \syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont at \ensuremath{\circ}\xspace school  }; \\ \sem{ \trns{at-school} }; \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)}}}
\RightLabel{\scalebox{.8}{xFC}}
\BinaryInfC{\LexEnt{\pt{read \ensuremath{\circ}\xspace at \ensuremath{\circ}\xspace school}}{\sem{ \lambda x. \trns{at-school}(\trns{read}(x)) }}{\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\LexEnt{\pt{mary \ensuremath{\circ}\xspace read \ensuremath{\circ}\xspace at \ensuremath{\circ}\xspace school}}{\sem{ \lambda x. \trns{at-school}(\trns{read}(x))(\trns{m}) }}{\syncat{S/NP}}}
\end{prooftree}
\end{samepage2}


Unless appropriately constrained, the addition of the crossed
composition rule leads to overgeneration, since non-extracted
expressions cannot change word order so freely in English. For
example, without additional restrictions, the simple CCG fragment
above overgenerates examples such as the following:

\begin{exe}
 \ex\label{overg}
  *Mary read at school it.
\end{exe}
Here, I will not go into the technical details of how this issue is
addressed in the CCG literature.\footnote{The word order in (\ref{overg}) is
of course fine in examples instantiating Heavy NP Shift.}
A broad consensus in the contemporary
CCG literature seems to be to regulate the syntactic contexts in which
special rules such as crossed composition in (\ref{mc}) are licensed
via the notion of `structural control' inherited to CCG from  the
`multi-modal' variant of TLCG (see \citet{Baldridge2002a-u} and
\citet{steedman2011ccg}).

Another issue that arises in connection to extraction is how to treat
multiple gaps corresponding to a single filler. The simple fragment
developed above cannot license examples involving parasitic gaps such
as the following:\footnote{Multiple gaps in coordination (i.e.\  ATB
  extraction) is not an
  issue, since these cases can be handled straightforwardly via the
  polymorphic definition of generalized conjunction in CCG, in just
  the same way that 
  unsaturated shared arguments in each conjunct are identified with one another.}

\begin{exe}
 \ex\label{PG}
  \begin{xlist}
 \ex\label{PG-1}
    This is the article that I filed {\gp}\xspace without reading {\gp}\xspace . 
 \ex\label{PG-2}
    Peter is a guy who even the best friends of {\gp}\xspace think  {\gp}\xspace should be
    closely watched.
  \end{xlist}
\end{exe}
Since neither type raising nor function composition changes the number
of `gaps' passed on to a larger expression, we need a new mechanism
here. \citet{steedman1987} proposes the following rule to deal with this
issue:

\begin{exe}
 \ex\label{subst}
  Substitution
\begin{prooftree}
\hspace*{-5cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sG}}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sF}}{\syncat{(\textit{A}\ensuremath{\backslash}{}\textit{C})/\textit{B}}}}
\RightLabel{\scalebox{.8}{S}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(x)(\sG(x))}}{\syncat{\textit{C}/\textit{B}}}}
\end{prooftree}
\end{exe}
\noindent
This `substitution' rule has the effect of
`collapsing' the  arguments of the
two inputs into one, to be saturated by a single filler.
The derivation for  the adjunct parasitic gap example in
(\ref{PG-1}) then goes as follows (where VP is an abbreviation for \syncat{NP\ensuremath{\backslash}{}S}):

\begin{samepage2}
\begin{exe}
 \ex\label{}
\end{exe}
\vspace*{-1.2cm}
\begin{prooftree}
\DerivSize
\AxiomC{\MultiLine{\mathsf{ \ptfont filed  }; \\ \sem{ \trns{file} }; \syncat{VP/NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont without  }; \\ \sem{ \trns{wo} }; \syncat{(VP\ensuremath{\backslash}{}VP)/VP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont reading  }; \\ \sem{ \trns{read} }; \syncat{VP/NP}}}
\RightLabel{\scalebox{.8}{FC}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont without \ensuremath{\circ}\xspace reading  }; \\ \sem{ \lambda x. \trns{wo}(\trns{read}(x)) }; \syncat{(VP\ensuremath{\backslash}{}VP)/NP}}}
\RightLabel{\scalebox{.8}{S}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont filed \ensuremath{\circ}\xspace without \ensuremath{\circ}\xspace reading  }; \\ \sem{ \lambda x. \trns{wo}(\trns{read}(x))(\trns{file}(x)) }; \syncat{VP/NP}}}
\end{prooftree}
\end{samepage2}

Like the crossed composition rule, the availability of the
substitution rule should be restricted to extraction environments. In
earlier versions of CCG, this was taken care of via a stipulation on
the rule itself. \citet{Baldridge2002a-u} proposed an improvement of the
organization of the CCG rule system in which the applicability of
particular rules is governed by lexically specified `modality'
encodings. See \citet{steedman2011ccg} for this relatively recent
development in CCG.



\subsection{Type-Logical Categorial Grammar}

The `rule-based' nature of CCG should be clear from the above
exposition. TLCG takes a somewhat different perspective on the
underlying architecture of the grammar of natural language.
Specifically, in TLCG, the rule system of grammar is literally taken
to be a kind of logic. Consequently, all grammar rules are logical
inference rules reflecting the properties of (typically a small number
of) logical connectives such as / and \ensuremath{\backslash}\ (which are viewed as
directional variants of implication). This conceptual shift can be
best illustrated by first replacing the ABC grammar introduced in
section \ref{preCCG} by the \term{Lambek calculus}, where all the rules
posited as primitive rules in the former are derived as \emph{theorems} (in
the technical sense of the term) in the latter.

Before moving on, I should hasten to note that the TLCG literature is
more varied than the CCG literature, involving multiple related but
distinct approaches. I choose to present one particular variant known
as Hybrid TLCG in what follows, in line with the present chapter's linguistic
emphasis. A brief comparison with major alternatives is offered in
section \ref{sec:comparison}. Other variants of TLCG, most notably,
the \term{Categorial Type Logics} \citep{Moortgat2011a-u} and
\term{Displacement Calculus} \citep{morrill2011} emphasize the logical and computational
aspects. \citet{mootretore2012} is a good introduction to TLCG covering
the logical and computational foundations. For readers interested in
linguistic application, \citet{Carpenter98a-u} is a very valuable source for
TLCG analyses of a wide range of linguistic phenomena at the
syntax-semantics interface, with very lucid exposition.


\subsubsection{The Lambek calculus \label{lambek}}

In addition to the Slash Elimination rules, which are identical to the
two rules in the AB grammar from section \ref{ab}, the Lambek calculus
posits the \term{Slash Introduction} rules, which can be written in the
current labelled deduction format as in (\ref{si}).\footnote{\citet{Morrill94a-u}
was the first to recast the Lambek calculus in this labelled deduction format.}

\begin{samepage2}
\begin{exe}
 \ex\label{SE} %\\[-15pt]
     \begin{multicols}{2} 
\begin{xlist}
 \ex\label{rse} \mbox{\term{Forward Slash Elimination}} 

\vspace*{-.2cm}
\begin{prooftree}
\hspace*{-1cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \sF(\sG)}}{\syncat{\textit{A}}}}
\end{prooftree}

 \ex\label{lse} \mbox{\term{Backward Slash Elimination}} 

\begin{prooftree}
\hspace*{-1cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{B}\ensuremath{\backslash}{}\textit{A}}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ptv{a}}}{\sem{ \sF(\sG) }}{\syncat{\textit{A}}}}
\end{prooftree}

\end{xlist} 
     \end{multicols}
\end{exe}
\end{samepage2}

\begin{samepage2}
 \begin{exe}
 \ex\label{si} %\\[-15pt]
    \begin{multicols}{2}
 \begin{xlist}
 \ex\label{rsi} \mbox{\term{Forward Slash Introduction}}

\vspace*{-.2cm}
\begin{prooftree}
\hspace*{-1cm}
\AxiomC{\Lemma}
\UnaryInfC{\Lemma}
\AxiomC{[\LexEnt{\pt{ \ensuremath{\greekp}}}{\sem{ x}}{\syncat{\textit{A}}}]$^n$}
\UnaryInfC{\LemmaAlt}
\AxiomC{\Lemma}
\UnaryInfC{\Lemma}
\TrinaryInfC{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \p}}{\sem{ \sF}}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{/I$^n$}}
\UnaryInfC{\LexEnt{\pt{\ptv{b}}}{\sem{ \lambda x. \sF}}{\syncat{\textit{B}/\textit{A}}}}
\end{prooftree}

 \ex\label{lsi} \mbox{\term{Backward  Slash Introduction}}

\vspace*{-.2cm}
\begin{prooftree}
\hspace*{-1cm}
\AxiomC{\Lemma}
\noLine 
\UnaryInfC{\Lemma}
\AxiomC{[\LexEnt{\pt{\p}}{\sem{ x}}{\syncat{\textit{A}}}]$^n$}
\UnaryInfC{\LemmaAlt}
\AxiomC{\Lemma}
\noLine 
\UnaryInfC{\Lemma}
\TrinaryInfC{\LexEnt{\pt{\p \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \sF}}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I$^n$}}
\UnaryInfC{\LexEnt{\pt{\ptv{b}}}{\sem{ \lambda x. \sF}}{\syncat{\textit{A}\ensuremath{\backslash}{}\textit{B}}}}
\end{prooftree}

 \end{xlist}
\end{multicols}
 \end{exe}
\end{samepage2}

I illustrate the workings of these rules in some examples first and
then come back to some formal and notational issues.
One consequence that immediately follows is that Type Raising and
Function Composition (as well as other theorems; see, e.g., \citet[sec
2.2.5, pp.~46--]{jaeger05}) are now derivable as theorems. As an
illustration, the proofs for (\ref{TR}a) and (\ref{FC}a) are shown in (\ref{TRproof})
and (\ref{FCproof}), respectively.

\begin{samepage2}
\begin{multicols}{2}

\begin{exe}
 \ex\label{TRproof}
\end{exe}
\vspace*{-1cm}
\begin{prooftree}
\hspace*{-1cm}
\DerivSize
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp} }}{\sem{ v }}{\syncat{\textit{A}\ensuremath{\backslash}{}\textit{B}}}]^1\,\,}}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ensuremath{\greekp}}}{\sem{ v(\sF)}}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}I\ensuremath{^1}}}
\UnaryInfC{\LexEnt{\pt{\ptv{a}}}{\sem{ \lambda v. v(\sF) }}{\syncat{\textit{B}/(\textit{A}\ensuremath{\backslash}{}\textit{B})}}}
\end{prooftree}

 \columnbreak
\begin{exe}
 \ex\label{FCproof}
\end{exe}
\vspace*{-2cm}
\begin{prooftree}
\DerivSize
\hspace*{-0cm}
\def\defaultHypSeparation{\hskip.6cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}/\textit{B}}}}
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp} }}{\sem{ x }}{\syncat{\textit{C}}}]^1\,\,}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}/\textit{C}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ptv{b} \ensuremath{\circ}\xspace \ensuremath{\greekp}}}{\sem{ \sG(x) }}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b} \ensuremath{\circ}\xspace \ensuremath{\greekp}}}{\sem{ \sF(\sG(x)) }}{\syncat{\textit{A}}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}I\ensuremath{^1}}}
\UnaryInfC{\LexEnt{\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b}}}{\sem{ \lambda x. \sF(\sG(x)) }}{\syncat{\textit{A}/\textit{C}}}}
\end{prooftree}
\end{multicols}
\end{samepage2}

\noindent
These are just formal theorems, but they intuitively make sense.
For example, what's going on in  (\ref{FCproof}) is simple. We first
hypothetically assume the existence of some expression of type
\textit{C} and combine it with \syncat{\textit{B}/\textit{C}}. This gives us a larger expression
of type \textit{B}, which then can be fed as an argument to  \syncat{\textit{A}/\textit{B}}.
At that point, we withdraw the initial hypothesis and conclude that
what we really had was just something that would become an \textit{A} \emph{if}
there is a \textit{C} to its right, namely, an expression of type \textit{A}/\textit{C}.
Thus, a sequence of expression of types \syncat{\textit{A}/\textit{B}} and \syncat{\textit{B}/\textit{C}}
is proven to be of type \syncat{\textit{A}/\textit{C}}.  This type of proof
is known as \term{hypothetical reasoning}, since it involves a step of
positing a hypothesis initially and withdrawing that hypothesis
at some later point.

Getting back to some notational issues, there are two crucial things
to keep in mind about the notational 
convention adopted here (which I implicitly assumed above). First, the
connective \ensuremath{\circ}\xspace in the prosodic component designates string
concatenation and is associative in both directions
(i.e.\  \pt{ (\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2}) \ensuremath{\circ}\xspace \ensuremath{\greekp_3}} \ensuremath{ \equiv\xspace } \pt{ \ensuremath{\greekp_1} \ensuremath{\circ}\xspace (\ensuremath{\greekp_2} \ensuremath{\circ}\xspace \ensuremath{\greekp_3}) }). In other words, hierarchical
structure is irrelevant for the prosodic representation. Thus, the
prosodic variable \pt{ \ensuremath{\greekp} } in (\ref{FCproof}) appears at the right on the
left periphery in the prosodic representation
\pt{\ptv{a} \ensuremath{\circ}\xspace \ptv{b} \ensuremath{\circ}\xspace \ensuremath{\greekp} } on the condition of Forward Slash
Introduction (\ref{rsi}). Note in particular that the application of the
Introduction rules is conditioned on the position of the prosodic
variable, and not on the position of the hypothesis itself in the
proof tree (this latter convention is more standardly adopted when the
Lambek calculus is presented in Prawitz-style natural deduction, though the two are
equivalent---see, e.g., \citet{Carpenter98a-u} and \citet{jaeger05}).

Hypothetical reasoning with Slash Introduction makes it possible to
recast the CCG analysis of nonconstituent coordination we saw above
within the logic of / and $\backslash$. This reformulation fully
retains the essential analytic ideas of the original CCG analysis but
makes the underlying `logic' of how bits and pieces are combined in
this construction more transparent.

The following derivation illustrates how the `reanalysis' of the
string \textit{Bill the book} as a derived constituent of the same type
\syncat{(VP/NP/NP)\ensuremath{\backslash}{}VP} as in (\ref{ACC-CCG-DV}) can be obtained in the Lambek
calculus:

\begin{samepage2}
\begin{exe}
 \ex\label{NCC-arg-deriv}
\end{exe}
\vspace*{-1.3cm}
\begin{prooftree}
\hspace*{-1cm}
\small
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp} }}{\sem{ f}}{\syncat{VP/NP/NP}}]^1\,\,}}
\AxiomC{\LexEnt{\pt{bill }}{\sem{ \trns{b} }}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp} \ensuremath{\circ}\xspace bill }}{\sem{ f(\trns{b}) }}{\syncat{VP/NP}}}
\AxiomC{\LexEnt{\pt{the \ensuremath{\circ}\xspace book }}{\sem{ \I(\trns{bk}) }}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp} \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ f(\trns{b})(\I(\trns{bk})) }}{\syncat{VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I\ensuremath{^1}}}
\UnaryInfC{\LexEnt{\pt{bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ \lambda f. f(\trns{b})(\I(\trns{bk})) }}{\syncat{(VP/NP/NP)\ensuremath{\backslash}{}VP}}}
\end{prooftree}
\end{samepage2}


At this point, one may wonder what the relationship is between the
analysis of nonconstituent coordination via Type Raising and Function
Composition in the ABC grammar in section \ref{preCCG} and the hypothetical
reasoning-based analysis in the Lambek calculus just presented. Intuitively, they
seem to achieve the same effect in slightly different ways. The
logic-based perspective of TLCG allows us to obtain a deeper
understanding of the relationship between them. To facilitate
comparison, I first recast the TR+FC analysis in the Lambek calculus. The relevant
part is the part that derives the `noncanonical constituent' \textit{Bill the
book}:

\begin{samepage2}
\begin{exe}
 \ex\label{redundant-DV}
\end{exe}
\vspace*{-1cm}
\begin{prooftree}
\hspace*{-.5cm}
\small
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_3} }}{\sem{ R }}{\syncat{DTV}}]^3\,\,}}
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_2} }}{\sem{ P }}{\syncat{DTV}}]^2\,\,}}
\AxiomC{\LexEnt{\pt{bill }}{\sem{ \trns{b}  }}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_2} \ensuremath{\circ}\xspace bill }}{\sem{ P(\trns{b})  }}{\syncat{TV}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I\ensuremath{^2}}}
\UnaryInfC{\LexEnt{\pt{bill }}{\sem{ \lambda P. P(\trns{b})  }}{\syncat{DTV\ensuremath{\backslash}{}TV}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_3} \ensuremath{\circ}\xspace bill }}{\sem{ R(\trns{b})  }}{\syncat{TV}}}
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_1} }}{\sem{ Q }}{\syncat{TV}}]^1\,\,}}
\AxiomC{\LexEnt{\pt{the \ensuremath{\circ}\xspace book }}{\sem{ \I(\trns{bk})}}{\syncat{NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ Q(\I(\trns{bk}))}}{\syncat{VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I\ensuremath{^1}}}
\UnaryInfC{\LexEnt{\pt{the \ensuremath{\circ}\xspace book }}{\sem{ \lambda Q. Q(\I(\trns{bk}))}}{\syncat{TV\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_3} \ensuremath{\circ}\xspace bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ R(\trns{b})(\I(\trns{bk})) }}{\syncat{VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I\ensuremath{^3}}}
\UnaryInfC{\LexEnt{\pt{bill \ensuremath{\circ}\xspace the \ensuremath{\circ}\xspace book }}{\sem{ \lambda R. R(\trns{b})(\I(\trns{bk})) }}{\syncat{DTV\ensuremath{\backslash}{}VP}}}
\end{prooftree}
\end{samepage2}

\noindent
By comparing (\ref{redundant-DV}) and (\ref{NCC-arg-deriv}), we see that
(\ref{redundant-DV}) contains some redundant steps. First, hypothesis 2 is
introduced only to be replaced by hypothesis 3. This is completely
redundant since we could have obtained exactly the same result by
directly combining hypothesis 3 with the NP \textit{Bill}. Similarly,
hypothesis 1 can be eliminated by replacing it with the
TV \pt{ \ensuremath{\greekp_3} \ensuremath{\circ}\xspace book} on the left-hand side of the third line from the bottom. By
making these two simplifications, we obtain the derivation in
(\ref{NCC-arg-deriv}).

The relationship between the more complex proof in (\ref{redundant-DV}) and
the simpler one in (\ref{NCC-arg-deriv}) is parallel to the relationship
between an unreduced lambda term (such as \sem{  \lambda P [P(\trns{b})](R)]  }) %
{}%
and its
$\beta$-normal form (i.e.\  \sem{  R(\trns{b})  })%
{}%
{}. In fact, there is a formally precise
one-to-one relationship between linear logic (of which the Lambek calculus is known
to be a straightforward extension) and the typed lambda calculus known
as the Curry-Howard Isomorphism \cmt{ref}, according to which
the lambda term that represents the proof 
(\ref{redundant-DV}) $\beta$-reduces  to the term that represents the proof
(\ref{NCC-arg-deriv}).\footnote{There is a close relationship between these
lambda terms representing proofs and the lambda  terms that we write
to notate semantics translations, especially if we write the latter
at each step of derivation \emph{without} performing $\beta$ reduction. But
it should be noted that these are distinct things.}
Technically, this is known as \term{proof normalization}
(\citet{jaeger05} contains a particularly useful discussion on this notion). 

Thus, the logic-based architecture of the Lambek calculus (and various versions of
TLCG, which are all extensions of the Lambek calculus) enables us to say, in a
technically precise way, how (\ref{redundant-DV}) and (\ref{NCC-arg-deriv}) are
the `same' (or, more precisely, equivalent), by building on
independently established results in mathematical logic and computer
science. This is one big advantage of taking seriously the view,
advocated by the TLCG research, that `language \emph{is} logic'.


\subsubsection{Extending the Lambek calculus \label{sec:extending}}

Hypothetical reasoning is a very powerful (yet systematic) tool, but
with forward and backward slashes, it is only good for analyzing
expressions missing some material at the (right or left) periphery.
This is problematic in the analyses of many linguistic
phenomena, such as \textit{wh}-extraction (where the `gap' can be in a
sentence-medial position) and quantifier scope (where the
quantifier needs to `covertly' move from a sentence-medial position),
as well  as various kinds of discontinuous constituency phenomena
(see for example \citet{morrill-ea11}, which contains analyses of
various types of discontinuous constituency phenomena in a recent
version of TLCG known as `Displacement Calculus').
In what follows, I sketch one particular, relatively recent approach
to this problem, known as \term{Hybrid Type-Logical Categorial Grammar}
(Hybrid TLCG; \citealt{kubota-diss,kubota-NCC,kubota-levine-coord}). This approach
combines the Lambek calculus with \citeposs{oehrle1994} term-labelled calculus, which deals
with discontinuity via by employing $\lambda$-binding in the prosodic
component.

Hybrid TLCG extends the Lambek calculus with the Elimination and Introduction rules for the
\term{vertical slash}:

\begin{samepage2}
\begin{multicols}{2}
\begin{exe}
 \ex\label{scoping} \begin{xlist}
      \ex\label{upI} \mbox{\term{Vertical  Slash Introduction}} 

\vspace*{-.2cm}
\begin{prooftree}
\hspace*{-1cm}
\AxiomC{\Lemma}
\noLine 
\UnaryInfC{\Lemma}
\AxiomC{[\LexEnt{\pt{\p}}{\sem{ x}}{\syncat{\textit{A}}}]$^n$}
\UnaryInfC{\LemmaAlt}
\AxiomC{\Lemma}
\noLine 
\UnaryInfC{\Lemma}
\TrinaryInfC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sF }}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{$|$I$^n$}}
\UnaryInfC{\LexEnt{\pt{\ensuremath{\lambda} \p.  \ptv{b}}}{\sem{ \lambda x. \sF }}{\syncat{\textit{B}\vs \textit{A}}}}
\end{prooftree}

      \ex\label{upE} \mbox{\term{Vertical  Slash Elimination}}

\vspace*{-.2cm}
\begin{prooftree}
\hspace*{-1cm}
\AxiomC{\LexEnt{\pt{\ptv{a}}}{\sem{ \sF }}{\syncat{\textit{A}\vs \textit{B}}}}
\AxiomC{\LexEnt{\pt{\ptv{b}}}{\sem{ \sG }}{\syncat{\textit{B}}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\LexEnt{\pt{\ptv{a}(\ptv{b})}}{\sem{  \sF(\sG)}}{\syncat{\textit{A}}}}
\end{prooftree}

     \end{xlist}

\end{exe}
\end{multicols}
\end{samepage2}

\medskip
\noindent These rules allow us to model what (roughly) corresponds to
covert movement in derivational frameworks. This is illustrated
in (\ref{inv-scope00}) for the \sem{   \forall > \exists   } %
{}%
reading for the sentence
\textit{Someone talked to everyone today}:

\begin{samepage2}
\begin{exe}
 \ex\label{inv-scope00}
\end{exe}
\vspace*{-.7cm}
\begin{prooftree}
\DerivSize
\hspace*{-.3cm}
\AxiomC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks}.\ensuremath{\greeks}(everyone) }; \\ \sem{ \GQU{A}{person}}; \\\syncat{S\vs (S\vs NP)}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks}.\ensuremath{\greeks}(someone) }; \\ \sem{ \GQU{E}{person} }; \\\syncat{S\vs (S\vs NP)}}}
\AxiomC{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_2} }; \\ \sem{ y}; \\\syncat{NP}}}^2}}
\AxiomC{\MultiLine{\mathsf{ \ptfont talked \ensuremath{\circ}\xspace to }; \\ \sem{ \trns{talked-to} }; \\\syncat{(NP\ensuremath{\backslash}{}S)/NP}}}
\AxiomC{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_1} }; \\ \sem{ x}; \\\syncat{NP}}}^1}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} }; \\ \sem{ \trns{talked-to}(x) }; \syncat{NP\ensuremath{\backslash}{}S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_2} \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} }; \\ \sem{ \trns{talked-to}(x)(y) }; \syncat{S}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont today }; \\ \sem{ \trns{tdy} }; \\\syncat{S\ensuremath{\backslash}{}S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_2} \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \trns{tdy}(\trns{talked-to}(x)(y)) }; \syncat{S}}}
\RightLabel{\maru{1}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}I\ensuremath{^2}}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp_2}. \ensuremath{\greekp_2} \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \lambda y. \trns{tdy}(\trns{talked-to}(x)(y)) }; \syncat{S\vs NP}}}
\RightLabel{\maru{2}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont someone \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \GQU{E}{person(}\lambda y. \trns{tdy}(\trns{talked-to}(x)(y))) }; \syncat{S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}I\ensuremath{^1}}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp_1}. someone \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace today }; \\ \sem{ \lambda x. \GQU{E}{person(}\lambda y. \trns{tdy}(\trns{talked-to}(x)(y))) }; \syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont someone \ensuremath{\circ}\xspace talked \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace everyone \ensuremath{\circ}\xspace today }; \\ \sem{ \GQU{A}{person(}\lambda x. \GQU{E}{person(}\lambda y. \trns{tdy}(\trns{talked-to}(x)(y)))) }; \syncat{S}}}
\end{prooftree}
\end{samepage2}
\medskip\medskip
\noindent
A quantifier has the ordinary G{}Q meaning  (\sem{  \GQU{E}{person}  } %
{}%
and \sem{  \GQU{A}{person}  } 
{}%
abbreviate the terms \sem{   \lambda P. \exists x [ \trns{person}(x) \ensuremath{ \wedge\xspace } P(x)]   } %
{}%
and
 \sem{   \lambda P. \forall x [ \trns{person}(x) \ensuremath{ \rightarrow }  P(x)]   }, %
{}%
respectively), but its phonology is a
function of type (\st\xspace \shortarrow \st\xspace) \shortarrow \st\xspace.
By abstracting over the position in which the quantifier `lowers
into' in an S via the Vertical Slash Introduction rule (\ref{upI}), we
can obtain an expression of
type \syncat{S\vs NP} (phonologically \st\xspace \shortarrow \st\xspace) (\maru{1}),
which can be given as an argument to the quantifier. Then, by function
application via $|$E (\maru{2}), the subject quantifier \textit{someone}
semantically scopes over the sentence and lowers its phonology to the
`gap' position kept track of by $\lambda$-binding in phonology. The same
process takes place for the object quantifier \textit{everyone} to complete
the derivation.  The scopal
relation between multiple quantifiers depends on the order of
application of this hypothetical reasoning.
The surface scope reading is obtained by switching the order of the 
hypothetical reasoning for the two quantifiers (which results in the
same string of words, but with the opposite scope relation.

This formalization of quantifying-in by \citet{oehrle1994} has later been
extended by \citet{barker07} for more complex types of scope-taking
phenomena known as `parasitic scope' (a notion coined by
\citet{barker07} where, in transformational terms, some  expression
takes scope at LF by parasitizing on the scope created by
a different scopal operator's LF movement---in versions of
(TL)CG  of the sort discussed here, this corresponds to 
double lambda-abstraction via the order"=insensitive slash) in the
analysis of symmetrical predicates. 
Empirical application of parasitic scope include `respective' readings
\citep{kubota-levine14fg}, `split scope' of negative quantifiers
\citep{kubota-levine-gapping} and modified numerals such as \textit{exactly N}
\citep{pollard-numerical}.

Hypothetical reasoning with prosodic $\lambda$-binding enables a simple
analysis of \textit{wh} extraction too, as originally noted by \citet{muskens03}.
The key idea is that sentences with medial gaps can be analyzed
as expressions  of type \syncat{S\vs NP}, as in the derivation for
(\ref{bagels}) in (\ref{bagels-derivation}).

\begin{exe}
 \ex\label{bagels}
  Bagels\ensuremath{_i}, Kim gave  \ensuremath{t_i}   to Chris.
\end{exe}
\begin{samepage2}
\begin{exe}
 \ex\label{bagels-derivation}
\end{exe}
\vspace*{-1cm}
\begin{prooftree}
\DerivSize
\hspace*{-.3cm}
\AxiomC{\MultiLine{\mathsf{ \ptfont bagels  }; \\ \sem{ \trns{b} }; \syncat{NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks} \lambda \ensuremath{\greekp}. \ensuremath{\greekp} \ensuremath{\circ}\xspace \ensuremath{\greeks}(\E)  }; \\ \sem{ \lambda \ensuremath{\mathcal{F}}. \ensuremath{\mathcal{F}} }; \\\syncat{(S\vs \textit{X})\vs (S\vs \textit{X})}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont kim  }; \\ \sem{ \trns{k} }; \syncat{NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont gave }; \\ \sem{ \trns{gave} }; \\\syncat{VP/PP/NP}}}
\AxiomC{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp}  }; \\ \sem{ x }; \\\syncat{NP}}}^1}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{gave \ensuremath{\circ}\xspace \ensuremath{\greekp}  }}{\sem{ \trns{gave} (x) }}{\syncat{VP/PP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\AxiomC{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace chris }; \\ \sem{ \trns{c} }; \syncat{PP}}}
\BinaryInfC{\LexEnt{\pt{gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(x)(\trns{c}) }}{\syncat{VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}I\ensuremath{^1}}}
\RightLabel{\maru{1}}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp}. kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris  }; \\ \sem{ \lambda x. \trns{gave}(x)(\trns{c})(\trns{k}) }; \syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\RightLabel{\maru{2}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp}. \ensuremath{\greekp} \ensuremath{\circ}\xspace kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \lambda x. \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\LexEnt{\pt{bagels \ensuremath{\circ}\xspace kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(\trns{b})(\trns{c})(\trns{k}) }}{\syncat{S}}}
\end{prooftree}

\end{samepage2}

\noindent Here, after deriving an \syncat{S\vs NP}, which keeps track of the gap
position via the $\lambda$-bound variable \pt{ \ensuremath{\greekp} }, the topicalization
operator fills in the gap with an empty string and concatenates the
topicalized NP to the left of the string thus obtained. This way, the
difference between `overt' and `covert' movement reduces to a lexical
difference in the prosodic specifications of the operators that induce
them. Covert movement operator throws in some material in the gap
position, whereas the overt movement operator `closes off' the gap
with an empty string.

As illustrated above, hypothetical reasoning for the Lambek slashes /
and \ensuremath{\backslash}\ and for the vertical slash \ensuremath{\vs} have important empirical
motivations, but the real strength of a `hybrid' system like Hybrid TLCG which
recognizes both mechanisms is that it extends automatically to cases
in which `directional' and `non-directional' phenomena interact. A
case in point comes from the interaction of nonconstituent
coordination and quantifier scope. Examples such as those in (\ref{NCCq})
allow for at least a reading in which the shared quantifier outscopes
conjunction.\footnote{Whether the other scopal relation (one in which the
quantifier meaning is `distributed' to each conjunct, as in the
paraphrase `I gave a couple of books to Pat on Monday and to Sandy on
Tuesday' for (\ref{NCCq})) is possible seems to depend on various factors.
With downward-entailing quantifiers such as (\ref{nothingEG}), this reading
seems difficult to obtain without heavy contextualization and
appropriate intonational cues. See \citet{kubota-levine-coord} for some discussion.}

\begin{exe}
 \ex\label{NCCq}
  \begin{xlist}
 \ex\label{}
    I gave a couple of books to Pat on Monday and to Sandy on Tuesday.
 \ex\label{nothingEG}
    Terry said nothing  to Robin on Thursday or to Leslie on Friday. 
  \end{xlist}
\end{exe}

I now illustrate how this wide scope reading for the quantifier in NCC
sentences like (\ref{NCCq}) is immediately predicted to be available in the
fragment developed so far (Hybrid TLCG actually predicts both scopal relations
for all NCC sentences; see \citet{kubota-levine-coord} for how the
distributive scope is licensed). The derivation for (\ref{nothingEG}) is
given in (\ref{RT-derivation}).

\begin{samepage2}
\begin{exe}
 \ex\label{RT-derivation}
\end{exe}
\vspace*{-1.4cm}
\begin{prooftree}
\hspace*{.5cm}
\DerivSize
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_1}}}{\sem{ P}}{\syncat{VP/PP/NP}}]^1\,\,}}
\AxiomC{\ensuremath{[\LexEnt{\pt{\ensuremath{\greekp_2}}}{\sem{ x}}{\syncat{NP}}]^2\,\,}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2}}}{\sem{ P(x) }}{\syncat{VP/PP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace robin }; \\ \sem{ \trns{r} }; \syncat{PP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin}}{\sem{ P(x)(\trns{r}) }}{\syncat{VP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont on \ensuremath{\circ}\xspace thursday }; \\ \sem{ \trns{onTh} }; \syncat{VP\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday}}{\sem{ \trns{onTh}(P(x)(\trns{r}))}}{\syncat{VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I\ensuremath{^1}}}
\UnaryInfC{\LexEnt{\pt{\ensuremath{\greekp_2} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday}}{\sem{ \lambda P. \trns{onTh}(P(x)(\trns{r}))}}{\syncat{(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} I\ensuremath{^2}}}
\UnaryInfC{\LexEnt{\pt{to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday}}{\sem{ \lambda x \lambda P. \trns{onTh}(P(x)(\trns{r}))}}{\syncat{NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\end{prooftree}
\end{samepage2}

\vspace*{-.4cm}
\begin{prooftree}
\hspace*{.5cm}
\DerivSize
\AxiomC{\Lemma}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday }; \\ \sem{ \lambda x \lambda P. \trns{onTh}(P(x)(\trns{r}))}; \\\syncat{NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont or }; \\ \sem{ \lambda \ensuremath{\mathcal{V}} \lambda \ensuremath{\mathcal{W}}. \ensuremath{\mathcal{W}} \ensuremath{ \sqcup\xspace } \ensuremath{\mathcal{V}} }; \\\syncat{(\textit{X}\ensuremath{\backslash}{}\textit{X})/\textit{X}}}}
\AxiomC{\Lemma}
\UnaryInfC{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday }; \\ \sem{ \lambda x \lambda P. \trns{onFr}(P(x)(\trns{l}))}; \\\syncat{NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\ \sem{ \lambda \ensuremath{\mathcal{W}}. \ensuremath{\mathcal{W}} \ensuremath{ \sqcup\xspace } [ \lambda x \lambda P. \trns{onFr}(P(x)(\trns{l}))]}; \\\syncat{(NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP)}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday }; \\ \sem{ \lambda x \lambda P \lambda z. \trns{onTh}(P(x)(\trns{r}))(z) \ensuremath{ \vee\xspace } \trns{onFr}(P(x)(\trns{l}))(z) }; \syncat{NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\end{prooftree}
\medskip\medskip

\vspace*{-.6cm}
\begin{prooftree}
\hspace*{-0cm}
\DerivSize
\AxiomC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greeks}.  \ensuremath{\greeks}(nothing) }; \\ \sem{ \ensuremath{ \neg\xspace} \GQU{E}{thing} }; \\\syncat{S\vs (S\vs NP)}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont terry }; \\ \sem{ \trns{t} }; \syncat{NP}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont said }; \\ \sem{ \trns{said} }; \\\syncat{VP/NP/PP}}}
\AxiomC{\ensuremath{\TwoColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp_3}  }; \\ \sem{ x }; \syncat{NP}}}^3}}
\AxiomC{\Lemma}
\UnaryInfC{\MultiLine{\mathsf{  \ptfont to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \lambda x \lambda P \lambda z. \trns{onTh}(P(x)(\trns{r}))(z)  \ensuremath{ \vee\xspace } \\ \hspace{2ex} \trns{onFr}(P(x)(\trns{l}))(z) }; \\\syncat{NP\ensuremath{\backslash}{}(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\MultiLine{\mathsf{  \ptfont \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace     {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \lambda P \lambda z. \trns{onTh}(P(x)(\trns{r}))(z) \ensuremath{ \vee\xspace } \\ \hspace{2ex} \trns{onFr}(P(x)(\trns{l}))(z)}; \\\syncat{(VP/PP/NP)\ensuremath{\backslash}{}VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\MultiLine{\mathsf{  \ptfont said \ensuremath{\circ}\xspace \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \lambda z. \trns{onTh}(\trns{said}(x)(\trns{r}))(z) \ensuremath{ \vee\xspace } \trns{onFr}(\trns{said}(x)(\trns{l}))(z)  }; \syncat{VP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash} E}}
\BinaryInfC{\MultiLine{\mathsf{  \ptfont terry \ensuremath{\circ}\xspace said \ensuremath{\circ}\xspace \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \trns{onTh}(\trns{said}(x)(\trns{r}))(\trns{t})\/  \trns{onFr}(\trns{said}(x)(\trns{l}))(\trns{t})  }; \syncat{S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}I\ensuremath{^3}}}
\UnaryInfC{\MultiLine{\mathsf{  \ptfont \lambda \ensuremath{\greekp_3}. terry \ensuremath{\circ}\xspace said \ensuremath{\circ}\xspace \ensuremath{\greekp_3} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace    {} }\\\mathsf{  \ptfont   \hspace{2ex} or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday  }; \\   \sem{ \lambda x.  \trns{onTh}(\trns{said}(x)(\trns{r}))(\trns{t}) \ensuremath{ \vee\xspace } \trns{onFr}(\trns{said}(x)(\trns{l}))(\trns{t}) }; \syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont terry \ensuremath{\circ}\xspace said \ensuremath{\circ}\xspace nothing \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace robin \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace thursday \ensuremath{\circ}\xspace  or \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace leslie \ensuremath{\circ}\xspace on \ensuremath{\circ}\xspace friday }; \\ \sem{ \ensuremath{ \neg\xspace} \GQU{E}{thing} ( \lambda x. \trns{onTh}(\trns{said}(x)(\trns{r}))(\trns{t}) \ensuremath{ \vee\xspace } \trns{onFr}(\trns{said}(x)(\trns{l}))(\trns{t})) }; \syncat{S}}}
\end{prooftree}

\noindent The key point in this derivation is that, via hypothetical
reasoning, the string \textit{to Robin on Thursday or to Leslie on Friday}
forms a syntactic constituent with a full-fledged meaning assigned to
it in the usual way. Then the quantifier takes scope above this whole
coordinate structure, yielding the non-distributive, quantifier
wide-scope reading.

Licensing the correct scopal relation between the quantifier and
conjunction in the analysis of NCC remains a challenging problem in
the HPSG literature. See section \ref{NCCcomparison} for some
discussion.


\subsubsection{Notes on other variants of TLCG \label{sec:comparison}}

\paragraph{Displacement Calculus and NL\ensuremath{_\lambda} \label{disp}}

Among different variant of TLCG, Morrill's Displacement Calculus and
Barker and Shan's continuation-based calculus NL\ensuremath{_\lambda} are most closely
related to Hybrid TLCG. Roughly speaking, Hybrid TLCG's vertical slash \syncat{\ensuremath{\vs}} plays more
or less the same role as the discontinuity connectives $\uparrow$ and
$\downarrow$ in Displacement Calculus and the `continuation' slashes
// and \ensuremath{\backslash}\ensuremath{\backslash}\ in NL\ensuremath{_\lambda}. Many empirical analyses of linguistic
phenomena formulated in one of these variants of TLCG translate to the
other two more or less straightforwardly (for example, the analyses of
Gapping and symmetrical predicates in \citet{kubota-levine-gapping} and
\citet{kubota-levine-resp}, whose key ideas are briefly sketched above,
build on Morrill's and Barker's analyses of the respective phenomena),
though there are some important differences (some of which I point out
below).

One major difference between the Displacement Calculus and Hybrid TLCG
on the one hand and NL\ensuremath{_\lambda} on the other is that the latter takes \textbf{NL},
namely, the Non-associative Lambek Calculus, as the underlying
calculus for the directional slashes / and \ensuremath{\backslash}. \citet{barkershan2015}
briefly comment on this property of their system, alluding to the
possibility of controlling flexibility of constituency via the notion
of `structural control' (see section \ref{multi} below) in Multi-Modal
Type-Logical Grammar. This certainly is a viable view, but no explicit
extension of NL\ensuremath{_\lambda} along these lines currently exists.
Morrill's approach differs from K\&L's in certain important ways in the
treatment of specific linguistic phenomena. The most substantial
disagreement pertains to the treatment of island constraints. Unlike
K\&L, who take (most) island constraints to be processing-oriented,
Morrill consistently holds the view that major island constraints
should be treated within the narrow syntax
\citep{Morrill94a-u,morrill2011,morrilllp}. See also a brief
discussion about determiner gapping in section \ref{sec:gapping}
for another point of disagreement.




\paragraph{Multi-Modal TLCG \label{multi}}

TLCG come in different varieties, including Hybrid TLCG and the Displacement
Calculus. There is a line of work most actively investigated back in
the 90s and whose core architecture provides the theoretical
underpinnings to Barker and Shan's NL\ensuremath{_\lambda}, called Multi-Modal Categorial
Type Logics (MMCTL)
\citep{moortgatoehrle94,Moortgat2011a-u,bernardiphd,vermaat05} (but it
should be kept in mind that the actual form that NL\ensuremath{_\lambda} takes as a
linguistic theory and the kinds of linguistic phenomena most
extensively studied in it differs considerably from MMCTL research
from the 90s). One crucial difference between Hybrid TLCG and the Displacement
Calculus on the one hand and MMCTL on the other is that, instead of
recognizing a separate level of prosodic representation, MMCTL deals
with various (somewhat heterogeneous) phenomena ranging from
morpho-syntactic properties of verb clusters in Dutch cross-serial
dependencies \citep{moortgatoehrle94}  to technical difficulties
with the Lambek Calculus in dealing with medial extraction
\citep{Moortgat2011a-u}  (see the discussion in section X above)
via the abstract notion of `structural control', building on the
technique of mixing different kinds of logic within  a single
deductive system originally developed in the literature of
substructural logic \citep{restall2000}. 

To see this point, it is instructive to take a look at the analysis of
medial extraction in MMCTL, illustrated by the following derivation
(adapted from \citet{bernardiphd}):

\begin{exe}
 \ex\label{ctlDV}
\end{exe}
\vspace*{-1cm}
\begin{prooftree}
\footnotesize
\hspace*{-.3cm}
\AxiomC{ \MultiLine{ \syncat{(N\ensuremath{\backslash} N)/(S\ensuremath{/_{\mathit{a}}}NP)}   \ensuremath{\vdash\xspace }  \\ \syncat{(N\ensuremath{\backslash} N)/(S\ensuremath{/_{\mathit{a}}}NP)}   }}
\AxiomC{ \MultiLine{ \syncat{NP}  \ensuremath{\vdash\xspace } \\ \syncat{NP}}}
\AxiomC{ \MultiLine{ \syncat{(NP\ensuremath{\backslash} S)\ensuremath{/_{\mathit{a}}}NP}  \ensuremath{\vdash\xspace } \\ \syncat{(NP\ensuremath{\backslash} S)\ensuremath{/_{\mathit{a}}}NP} }}
\AxiomC{[ NP  \ensuremath{\vdash\xspace } NP ]$^1$}
\RightLabel{\scalebox{.8}{\ensuremath{/_a}E}}
\BinaryInfC{ \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} \ensuremath{\circ_a} NP  \ensuremath{\vdash\xspace } \syncat{NP\ensuremath{\backslash}{}S}}
\AxiomC{\MultiLine{ \syncat{(NP\ensuremath{\backslash} S)\ensuremath{\backslash} (NP\ensuremath{\backslash} S)}  \ensuremath{\vdash\xspace } \\ \syncat{(NP\ensuremath{\backslash} S)\ensuremath{\backslash} (NP\ensuremath{\backslash} S)} }}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash}{}E}}
\BinaryInfC{ (\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} \ensuremath{\circ_a} NP) \ensuremath{\circ} \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)}   \ensuremath{\vdash\xspace } \syncat{NP\ensuremath{\backslash}{}S}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash}{}E}}
\BinaryInfC{ NP \ensuremath{\circ} ((\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} \ensuremath{\circ_a} NP) \ensuremath{\circ} \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)})   \ensuremath{\vdash\xspace } S}
\RightLabel{\scalebox{.8}{Diss}}
\UnaryInfC{ NP \ensuremath{\circ} ((\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} \ensuremath{\circ} \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)}) \ensuremath{\circ_a} NP)   \ensuremath{\vdash\xspace } S}
\RightLabel{\scalebox{.8}{MixAssoc}}
\UnaryInfC{ (NP \ensuremath{\circ} (\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} \ensuremath{\circ} \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)})) \ensuremath{\circ_a} NP   \ensuremath{\vdash\xspace } S}
\RightLabel{\scalebox{.8}{$/_a$I$^1$}}
\UnaryInfC{ NP \ensuremath{\circ} (\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} \ensuremath{\circ} \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)})  \ensuremath{\vdash\xspace } S\ensuremath{/_{\mathit{a}}}NP}
\RightLabel{\scalebox{.8}{\ensuremath{/}E}}
\BinaryInfC{ \syncat{(N\ensuremath{\backslash}{}N)/(S\ensuremath{/_{\mathit{a}}}NP)}  \ensuremath{\circ} ((NP \ensuremath{\circ} (\syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{/_{\mathit{a}}}NP} O \syncat{(NP\ensuremath{\backslash}{}S)\ensuremath{\backslash}{}(NP\ensuremath{\backslash}{}S)})) \ensuremath{\circ_a} NP)   \ensuremath{\vdash\xspace } \syncat{N\ensuremath{\backslash}{}N}}
\end{prooftree}

\noindent
Here, the derivation is written in the sequent style natural deduction
(see Chapter~\ref{chap-evolution}), where a sequent is a
construct of the form $\Gamma  \ensuremath{\vdash\xspace } A$ where the consequent $A$ is a
syntactic type and the antecedent $\Gamma$ is a structured object
consisting of possibly multiple syntactic types that are combined with
one another via binary connectives (of several sorts). On the leaves
of the trees, we find axioms of the form $A  \ensuremath{\vdash\xspace } A$ and the sequent on
the final line of the proof tree is obtained by applying the rules of
logic (such as the Elimination and Introduction rules similar to the
ones we have posited above) and `structural rules', which manipulate
the forms of the antecedents of the sequents that are derived.

Intuitively, what is going on in the above derivation is that an object
NP that is hypothesized on the right of a transitive verb is `moved'
to the right edge via the structural rule of Mixed Associativity in
the structured antecedent on the third line from the bottom. This
licenses the /aI rule, withdrawing the hypothesis to create the S/aNP
category that is suitable as an argument to the relative pronoun.
After lexical substitution, the derivation in 
(\ref{ctlDV}) embodies, for example, a formal proof for the
fact that  the string `which Sara wrote {\gp}\xspace there' is a well-formed
relative clause in English containing a medial gap.

There are a couple of important points of difference between MMCTL and
other variants of TLCG that are already clear from this rather cursory
exposition. First, the analysis of extraction via a series of
structure rewriting operations in the abstract substructural component
of the type logic is very different from the prosodic $\lambda$
abstraction analysis presented above, due originally to \citet{muskens03}
and incorporated in many contemporary variants of TLCG (such as Hybrid TLCG and
Pollard's Linear Categorial Grammar \citep{mihalicek-pollard10,pollard13}).
Second, the exact ontological status of this `multi-modal'
substructural component is somewhat unclear. As noted above, the
structure-changing operations have been put to use not just for
extraction in English but for dealing with complex morpho-syntactic
properties of verb clustering in Dutch cross-serial dependencies
\citep{moortgatoehrle94}. All this is done, not in a separate prosodic
component, but inside a complex logic of syntactic types, where these
syntactic types are taken as primitives that enter into binary
composition operations of various sorts having different combinatorial
possibilities. The different ontological setup in different variants
of contemporary TLCG may have to do with the different research goals
and research practices. When the emphasis is on linguistic
application, a clear separation of ontologically distinct components
seems important, but when the emphasis is on studying the meta-logical
properties of the formal calculus, building directly on the rich
literature of substructural logic and formalizing the type logic for
natural language syntax literally \emph{as} a substructural logic certain
seems to be a natural choice.


\paragraph{Linear Categorial Grammar}

There is a family of related approaches in the TLCG tradition
\citep{oehrle1994,degroote01,muskens03,mihalicek-pollard10,pollard13} that
are distinctly different from the variants of TLCG discussed above in
\emph{not} recognizing the directional slashes of the Lambek calculus as
primitive logical connectives. I call this family of approaches
\term{Linear Categorial Grammar} (LCG) in what follows.\footnote{Among
these variants, \term{Abstract Categorial Grammar} (ACG) has mostly been studied
in the formal grammar literature as a meta-framework for embedding
different types of linguistic theories (as opposed to as a linguistic
theory by itself), despite the fact that \citet{degroote01}, the very first
paper that describes it, motivated it by pointing out some empirical
issues with standard variants of TLCG. For a highly readable and
interesting recent work on
using ACG as a meta-framework for embedding Tree Adjoining Grammar,
see \citet{Pogodalla2017}.}

LCG is essentially a subsystem of Hybrid TLCG without the rules for the
directional slashes. The original conceptual motivation for this
architecture is similar to the motivation for linearization-based
HPSG---the idea, originally due to \citet{curry61}, that separating the
combinatoric component of grammar and surface word order realization
leads to a cleaner theoretical architecture. To get a flavor of LCG,
an analysis of \textit{wh-}extraction from section \ref{sec:extending} can be
reformulated in an LCG fragment as follows:

\begin{samepage2}
\begin{exe}
 \ex\label{wh}
\end{exe}
\vspace*{-1.2cm}
\begin{prooftree}
\DerivSize
\hspace*{-.2cm}
\AxiomC{\MultiLine{\mathsf{ \ptfont bagels  }; \\ \sem{ \trns{b} }; \syncat{NP}}}
\AxiomC{\MultiLine{\mathsf{  \ptfont \lambda \ensuremath{\greeks} \lambda \ensuremath{\greekp}.    }\\\mathsf{  \ptfont   \hspace{2ex} \ensuremath{\greekp} \ensuremath{\circ}\xspace \ensuremath{\greeks}(\E)   }; \\   \sem{ \lambda \ensuremath{\mathcal{F}}. \ensuremath{\mathcal{F}} }; \\\syncat{(S\vs \textit{X})\vs (S\vs \textit{X})}}}
\AxiomC{\MultiLine{\mathsf{ \ptfont kim  }; \\ \sem{ \trns{k} }; \\\syncat{NP}}}
\AxiomC{\MultiLine{\mathsf{  \ptfont \lambda \ensuremath{\greekp_1} \lambda \ensuremath{\greekp_2} \lambda \ensuremath{\greekp_3}.    }\\\mathsf{  \ptfont   \hspace{2ex} \ensuremath{\greekp_3} \ensuremath{\circ}\xspace  gave \ensuremath{\circ}\xspace \ensuremath{\greekp_1} \ensuremath{\circ}\xspace \ensuremath{\greekp_2}   }; \\   \sem{ \trns{gave} }; \\\syncat{S\vs NP\vs PP\vs NP}}}
\AxiomC{\ensuremath{\ThreeColHyp{\MultiLine{\mathsf{ \ptfont \ensuremath{\greekp}  }; \\ \sem{ x }; \\\syncat{NP}}}^1}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\MultiLine{\mathsf{ \ptfont \lambda \ensuremath{\greekp_2} \lambda \ensuremath{\greekp_3}. \ensuremath{\greekp_3} \ensuremath{\circ}\xspace  gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace \ensuremath{\greekp_2}  }; \\ \sem{ \trns{gave}(x) }; \syncat{S\vs NP\vs PP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\AxiomC{\MultiLine{\mathsf{ \ptfont to \ensuremath{\circ}\xspace chris }; \\ \sem{ \trns{c} }; \syncat{PP}}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp_3}. \ensuremath{\greekp_3} \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(x)(\trns{c}) }}{\syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\backslash}E}}
\BinaryInfC{\LexEnt{\pt{kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}I\ensuremath{^1}}}
\UnaryInfC{\LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp}. kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \lambda x. \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\LexEnt{\pt{\ensuremath{\lambda} \ensuremath{\greekp}. \ensuremath{\greekp} \ensuremath{\circ}\xspace kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \lambda x. \trns{gave}(x)(\trns{c})(\trns{k}) }}{\syncat{S\vs NP}}}
\RightLabel{\scalebox{.8}{\ensuremath{\vs}E}}
\BinaryInfC{\LexEnt{\pt{bagels \ensuremath{\circ}\xspace kim \ensuremath{\circ}\xspace gave \ensuremath{\circ}\xspace to \ensuremath{\circ}\xspace chris }}{\sem{ \trns{gave}(\trns{b})(\trns{c})(\trns{k}) }}{\syncat{S}}}
\end{prooftree}

\end{samepage2}

\noindent The key difference from the Hybrid TLCG derivation above in (\ref{bagels-derivation}) is
that here the syntactic categories of linguistic expressions do not
specify word order. Note in particular that the ditransitive verb
\textit{gave} is syntactically of type \syncat{S\vs NP\vs PP\vs NP}, and specifies the order
of the arguments relative to the verb itself in the prosodic
representation, which is formally a function of
type \st\xspace \shortarrow \st\xspace \shortarrow \st\xspace \shortarrow \st\xspace.

While the clear separation of the underlying combinatorics and surface
word order is certainly appealing, the conceptual elegance comes at
some nontrivial empirical cost. Specifically, the simple analysis of
NCC as constituent coordination, discussed in section \ref{preCCG},
which is the hallmark of many variants of CG, does not
straightforwardly carry over to this setup. This issue was already
anticipated by \citet{muskens01}, and is discussed in detail in
\citet{kubota-levine-coord}. To see what's at issue, note that
the distinction between \syncat{S/NP} and \syncat{NP\ensuremath{\backslash}{}S}, which
is crucial for distinguishing well-formed RNR sentence and
ungrammatical strings in the CCG/Lambek calculus analysis of RNR is lost in
LCG, since the two will be collapsed into the same type \syncat{S\vs NP}.
But then, neither the syntactic type nor the 
prosodic type (which is uniformly \st\xspace \shortarrow \st\xspace) of the conjuncts provides
enough information to determine conjoinability.

\citet{worth-diss} presents the most comprehensive treatment of coordination
in LCG addressing this problem (see also \citet{kanazawa15}). The core idea
of his proposal is to encode word order information via fine-grained
subtypes of the prosodic types of linguistic expressions. Whether this
extension of LCG offers the same (or better) empirical coverage as
more traditional variants of TLCG (including Hybrid TLCG) is currently an open
question.

\cmt{It may be useful to include a discussion of Minimalist Grammars
in this section, but it may also be a bit tricky.}


%==== Minimalist Grammars

%\cmt{say something about Minimalist Grammars too?}

%There is another line of work in the CG literature known as
%\term{Minimalist Grammars} (MG). Unlike CCG and at least some variants of TLCG
%(such as Morrill's Displacement Calculus and K\&L's Hybrid TLCG), which share
%with HPSG the goal of of writing an explicit and detailed
%fragment of the grammar of natural language covering a reasonably
%wide range of empirical phenomena, the general research goals
%(and accordingly the  research output) of MG is somewhat
%different. Thus, a direct comparison (with other variants of CG,  let
%alone HPSG) is difficult, but because of the  rather
%distinct research orientation, this line of work is worth mentioning
%here.

%MG research generally start with the assumption that major proposals
%in the literature of minimalist syntax is `given', and study the
%mathematical properties of toy grammars embodying mathematically
%precise definitions of such proposals. Research in the MG literature
%ask questions such as whether the generative capacity of the grammar
%increases by admitting the operation of `remnant movement'. This type
%of research is interesting and important, since many critical concepts
%in the mainstream syntactic literature are often proposed without any
%clear definitions and the consequences of either adopting or not
%adopting a particular `constraint' for the mathematical properties of
%the formal architecture of grammar is often unknown. What makes it
%difficult to relate this line of work to the central research
%questions of HPSG (and at least some variants of CG) is that, due to
%the very explicit decision about the research objectives, MG research
%often tends to have consequences that are only directly relevant to
%the mainstream syntactic literature. This is a somewhat unfortunate
%situation, since meta-theoretical studies of the sort that the MG
%research excels in clearly has a much larger relevance for theoretical
%syntax research more generally.




\section{Architectural similarities and differences}

\subsection{Broad architecture}

One important property common to HPSG and CG is that they are both
`lexicalist' theories of syntax in the broader sense.\footnote{I say
`broader sense' here since not all variants of either HPSG or CG subscribe to the
so-called `lexical integrity hypothesis', which says that syntax and
morphology are distinct components of grammar. For example,
the treatments of verb clustering in Dutch by \citet{moortgatoehrle94} and
in Japanese by \citet{Kubota2014a-u}, seem to go against the tenet of the
lexical integrity hypothesis. See also Chapter~\ref{chap-lexicon} for some discussion
on lexicalism.} \mgcmt{Need to check where the issue of `lexical
integrity hypothesis' is discussed in the book.}
This is due to an explicit choice made at an early
stage of the development of HPSG to encode valence information in the
syntactic categories of linguistic expressions, following CG (see
Chapters~\ref{chap-evolution} and \ref{chap-lexicon}).\footnote{This
  point is explicitly noted by the original founders of HPSG
  in the following passage in \citet[11]{ps}:

\begin{quote}
A third principle of universal grammar posited by HPSG, the
Subcategorization Principle, is essentially a generalization of the
`argument cancellation' employed in categorial grammar
\end{quote}

}
The two theories share many similarities in the
analyses of specific linguistic phenomena due to this basic
architectural similarity. For example, many phenomena that are treated
by means of A movement operations (or via empty categories) in
mainstream syntax, such as passivization, raising/control in English
and `complex predicate' phenomena in a typologically broad range of
languages are generally treated by the sharing of valence information
in the lexicon in these theories. For HPSG
analyses of these phenomena, see Chapters~\ref{chap-arg-st} and \ref{chap-control-raising}.
\citet{steedman2011ccg} contains a good
summary of CG analyses of local dependencies 
(passivization, raising/control). \citet{Kubota2014a-u} contains
a comparison of HPSG and CG analyses of complex predicates.
The heavy reliance on `lexicalist' analyses of
local dependencies 
is perhaps the most important property that is shared in common in
HPSG and various versions of CG.

But emphasizing this commonality too much may be a bit misleading,
since the valence features of HPSG and the slash connectives in CG
have very different ontological statuses in the respective theories.
The valence features in HPSG are primarily specifications, closely
tied to the specific phrase structure rules, that specify the ways in
which hierarchical representations are built. To be sure, the lexical
specifications of the valence information play a key role in the
movement-free analyses of local dependencies along the lines noted
above, but still, there is a rather tight connection between these
valence specifications originating in the lexicon and the ways in
which they are `cancelled' in specific phrase structure rules.

Things are quite different in CG, especially in TLCG. As discussed in
section 2, TLCG views the grammar of natural language \emph{not} as a
structure-building system, but as a logical deductive system. The two
slashes / and \ensuremath{\backslash}\ are thus not `features' that encode the
subcategorization properties of words in the lexicon, but have a much
more general and fundamental role within the basic architecture of
grammar in TLCG. These connectives are literally 
implicational connectives within a logical calculus. Thus, in TLCG,
`derived' rules such as
type-raising and function composition are \emph{theorems}, in just the same
way that the 
transitivity inference is a theorem in classical propositional logic.
Note that this is not just a matter of high-level conceptual
organization of the theory, since, as discussed in section 2, the
ability to assign `constituent' statuses to non-canonical constituents
in the CG analyses of NCC directly exploits this property of the
underlying calculus. The straightforward mapping from syntax to
semantics discussed in section \ref{interface} is also a direct
consequence of adopting this `derivation as proof' perspective on
syntax, building on the results of Curry-Howard correspondence \cmt{ref} in
setting up the syntax-semantics interface.\footnote{Although CCG does
not embody the idea of `derivation as proof' as explicitly as TLCG
does, it remains true to a large extent that the role of the slash
connective within the overall theory is different from that of the
valence features in HPSG, given that CCG and TLCG share share many key
ideas in the analyses of actual empirical phenomena.}

%the two versions of CG share many key ideas
%in the analyses of actual empirical phenomena, including both local
%dependencies and coordination. In fact, as far as linguistic
%application is concerned, the only major difference between CCG and
%TLCG is in the treatment of long-distance dependencies and scopal
%operators (that is, `overt' and `covert' movement operations). CCG
%opts for an approach that simulates the non-local dependencies
%of these phenomena via a series of local dependencies, similar to the
%feature percolation treatments (via SLASH and Cooper storage) in HPSG,
%whereas TLCG generally deals with these phenomena by some sort of
%hypothetical reasoning, which, in a way,  is more similar to
%movement operations in the mainstream syntax.}

%\footnote{It should be noted that,
%despite this elegant analysis of NCC and related phenomena, there is
%actually no consensus in the CG literature as to whether the notion of
%hierarchical constituency should be completely glossed over.  \cmt{some
%discussion of different views} }

%In any event, the flexible notion of constituency that enables the
%elegant treatment of NCC is a property that is common to both CCG and
%TLCG, and this is something that doesn't have any direct counterpart
%in HPSG. While some authors (such as K\&L) argue that the flexible
%constituency is a property that is not limited to coordination, even
%within the TLCG literature, there is no consensus as to whether taking
%a fully associative calculus \textbf{L} as the underlying component of
%natural language syntax is the right choice.

%On the other hand, HPSG also has a variant, called linearization-based
%HPSG, which relaxes the mapped between the `underlying syntax' and
%surface word order in a certain way. While many recent analyses of NCC
%in HPSG rely on this architecture, these approaches aren't simply the
%HPSG implementations of CG analyses of NCC, as discussed in section
%4.2 below. How much and what kind of flexibility should be allowed for
%in the basic architecture of the syntax-semantics interface natural
%language still seems to be a big open question on which more careful
%research is needed.

Another notable difference between (especially a recent variant of)
HPSG and CG is that CG currently lacks a detailed theory of
`constructions', 
that is, patterns and (sub)regularities that are
exhibited by linguistic expressions that cannot (at least according to
the proponents of `constructionist' approaches) be lexicalized easily.
As discussed in Chapter~\ref{chap-cxg},  recent Sign-Based Construction
Grammar (SBCG) variants of HPSG \citep{SBK2012a} 
incorporates ideas from Construction Grammar \citep{Goldberg95a} and capture such
generalizations via a set of constructional templates (or schemata),
which are essentially a family of related phrase structure rules that
are organized in a type inheritance hierarchy.

Such an architecture seems nearly impossible to implement literally in
CG, except via of empty operators or lexical operations
corresponding to each such constructional schema. In particular, in
TLCG, syntactic rules are logical inference rules, so, there is no
option to freely add syntactic rules in the deductive system. However,
the crucial tenet of Construction Grammar in its strongest form,
namely, the claim that the grammar of natural language robustly
exhibits patterns that cannot be perspicuously captured within the
lexicon seems still highly controversial even within the the HPSG
literature. Thus, if the relevant generalizations can all be
lexicalized, along the lines argued by Mueller and Wechsler \cmt{ref} (see
also Chapter~\ref{chap-cxg} and \citet{steedman2011ccg}, the latter of which
briefly discuss ways in which some of the empirical generalizations
that \citet{Goldberg95a} adduces to the notion of constructions can be
lexicalized within CCG), then it may not pose any fundamental problem
for the strongly lexicalist architecture of CG. 
This being said, it should be noted that much less research has been
conducted on the so-called `peripheral' phenomena from a CG
perspective, and incorporating the recent advances on
the analyses of these phenomena in the SBCG literature
remains to be 
an important task for future CG research. 



\subsection{Syntax-semantics interface }

As should be clear from the above exposition, both CCG and TLCG (at
least in the simplest form) adopt a very rigid, one-to-one
correspondence between syntax and semantics. Steedman's work on CCG
has demonstrated that 
this simple and systematic mapping between syntax and semantics
enables attractive analyses of
a number of empirical phenomena at the syntax-semantics interface,
including some notorious problems such as the scope parallelism issue
in right-node raising known as Geach sentences (\textit{Every boy loves, and
every girl detests, some saxophonist}). Other important work on issues
at the syntax-semantics interface include Jacobson's
(\citeyear{jacobson1999a,jacobson2000a}) work on pronominal anaphora
in Variable-Free Semantics (covering a wide range of phenomena
including the paycheck/Bach-Peters paradigms and binding parallelism
in right-node raising), \citeposs{barkershan2015} work on `continuation-based'
semantics (weak crossover, superiority effects and `parasitic scope'
treatments of symmetrical predicates and sluicing) and Kubota and
Levine's (\citeyear{kubota-levine-coord,kubota-levine-pseudo}) Hybrid TLCG,
dealing with interactions between coordination, ellipsis and scopal
phenomena.

%Things are actually not so simple. As long as the meanings of
%individual lexical items are specified as lambda terms, it is trivial
%to add a layer of underspecification to both CCG and TLCG.
%-steedman2012/s approach to quantification adopts this technique and
%treats indefinites as underspecified Skolem terms. There is another
%approach that makes use of underspecification in CG. Dependent Type
%Semantics \citep{bekki14,TanakaEA15} B\&M, a version of proof theoretic
%semantics that crucially makes use of underspecification for the
%treatment of anaphora, has been applied to both CCG B\&M and TLCG
%(K\&L-LENLS).


%While this is definitely an interesting and promising body of work, it
%is probably fair to say that the rigid correspondence between syntax
%adn semantics is both a blessing and a curse. For example, how to
%compactly represent ambiguous scopal relations between multiple
%quantifiers would be a major headache for this type of
%architecture.\footnote{In this respect, it is interesting to see that
%Steedman's version of CCG, which is the most practically-oriented
%version of CG among different variants, partially incorporates
%the idea of underspecification in the treatment of quantification, as
%noted above.}

As discussed in Chapter~\ref{chap-semantics}, recent HPSG work on complex empirical
phenomena at the syntax-semantics interface makes heavy use of
underspecification.\mgcmt{need to check Chapter~\ref{chap-semantics}} For example, major
analyses of nonconstituent coordination in recent HPSG use some
version of underspecification framework to deal with complex
interactions between coordination and scopal operators.
\citep{BS2004a,Yatabe2001a,parkea18gapping}. In a sense, HPSG retains a
rigid phrase structure-based syntax (modulo the flexibility
entertained with the use of the linearization-based architecture) and
deals with the complex mapping to semantics 
via the use of underspecification languages in the semantic
component such as MRS and LRS.
CG, on the other hand, sticks to a tight mapping from syntax to
semantics, but makes the syntactic component itself
flexible.\footnote{But note that in principle this architecture does
not preclude the use of underspecification. \citeposs{steedman2012} recent
approach to quantification crucially employs a limited use of
underspecification for the treatment of indefinites. Similarly, Bekki
and Mineshima's Dependent Type Semantics \citep{bekki14,bekkimineshima17},
a version of compositional
proof theoretic semantics that has a straightforward interface to CG
syntax, makes use of underspecification for the treatment of
anaphora.} These differences may again be a reflection of a
somewhat broader `philosophical' difference between the two theories
alluded to above (HPSG's surface-oriented syntax vs.\  CG's abstract view
on the nature of the combinatoric component).

%These proposals contrast sharply with the CG analyses of the same
%phenomena (see K\&L and references cited therein) in that they reconcile
%the apparent mismatch between the syntactic representations and final
%semantic interpretations by making the compositional mechanism of
%grammar to manipulate underspecified semantic representations. The CG
%approach instead maintains a direct mapping from the combinatoric
%structure of syntax to semantic interpretation, but makes the
%combinatoric component itself radically flexible. These two competing
%views are interestingly different, and this may again be another
%reflection of a somewhat broader `philosophical' difference between
%the two general approaches alluded to at some other points in this
%chapter.



\subsection{Morpho-syntax and word order \label{sec:morphology}}

\cmt{This section is not yet written.}

\noindent
Some things which seem worth discussing here:\mgcmt{need to check 
  Chapter~\ref{chap-morphology} to see what's discussed in there}

\begin{itemize}
 \item Linearization HPSG (Reape, Kathol) vs.\  multi-modal CG (Moortgat/Oehrle, Baldridge) 
\end{itemize}
\begin{itemize}
 \item treatment of features --- more detailed development in HPSG as
  compared to CG (but see Morrill 1994)
  
  cf. Also:
  \begin{itemize}
   \item Unification-based CG (Uszkoreit, Zeevat)
   \item \citet{BJ95,Bayer96} vs.\  HPSG work on feature neutralization \mgcmt{need
    to check whether/ where feature neutralization is discussed in the book} 
  \end{itemize}
\end{itemize}
\begin{itemize}
 \item Recent advances in work on morphology in HPSG (Bonami/Crysmann)
  but no comparable detailed theory in CG
  
  But see  Carpenter on lexical rules in CG; also
  the idea of categorial morphology in Hoeksema's early work
  and the paper by ???
\end{itemize}

\section{Specific empirical phenomena }

Since Part II of the present handbook contains an excellent
introduction to recent developments of HPSG research on major
linguistic phenomena, here I will try to highlight the differences
between HPSG and CG in the analyses of a selected empirical phenomena.
So as to make the ensuing discussion maximally informative, I choose
to focus on phenomena over which there is some ongoing major
cross-theoretical debate, and those for which I believe one or the
other theory would benefit from recent developments/rich research
tradition in the other.

%In any event, my goal here is not to argue for the superiority of any
%particular approach over others, but simply to provide a reasonably
%complete sketch of the state of the art of research in these empirical
%domains in HPSG and CG, with an up-to-date bibliographical reference.



\subsection{Long-distance dependencies}

As noted in section \ref{sec:CCG}, CCG treats long-distance
dependencies via a sequence of function composition, which is similar
to the SLASH percolation analysis in HPSG. CCG offers a treatment of
major aspects of long-distance dependencies, including island effects
\citep{Steedman2000a-u} and parasitic gaps \citep{Steedman1987}. Earlier
versions of CCG involved a somewhat ad-hoc stipulation on the use of
`crossed composition' rules \citep{Steedman97a}, but this was overcome in
the more recent, `multi-modal' variant of CCG \citep{Baldridge2002a-u},
which controls the application of such non-order-preserving rules via
a fine-grained system of `lexicalized modality' by means of the
lexical specifications of the relevant linguistic expressions.

The situation is somewhat different in TLCG. TLCG typically makes use
of a movement-like operation for the treatment of extraction
phenomena, but the specific implementations differ considerably in
different variants of TLCG. Major alternatives include the notion of
`structural control' in multi-modal variants of TLCG
\citep{Morrill94a-u,Moortgat2011a-u}, and prosodic $\lambda$-binding in LCG and
related approaches (see section \ref{sec:extending}). In either
approach, extraction phenomena are treated by means of some form of
hypothetical reasoning, and this raises a major technical issue. The
underlying calculus of TLCG is a version of linear logic, and this
means that the implication connective is resource sensitive. This is
problematic in situations in which a single filler corresponds to
multiple gaps, as in parasitic gaps and related phenomena. These cases
of extraction require some sort of extension of the underlying logic
or some special operator that is responsible for resource duplication.
Currently, the most detailed treatment of extraction phenomena in the
TLCG literature is \citet{morrilllp}, which lays out in detail an analysis
of long-distance dependencies capturing both major island constraints
and parasitic gaps within the most recent version of Morrill's
Displacement Calculus.

There are several complex issues that arise in relation to the
linguistic analysis of extraction phenomena. One major open question
is whether island constraints should be accounted for within narrow
grammar. Both Steedman and Morrill follow the standard practice in
generative grammar research in taking island effects to be syntactic,
but this consensus has been challenged by a new body of research in
the recent literature proposing various alternative explanations on
different types of island constraints (see Chapter~\ref{chap-islands}, \citet{levine2017}
and \citet{newmeyer2016} for an overview of this line of work). Recent
syntactic analyses of long-distance dependencies in the HPSG
literature explicitly avoid directly encoding major island constraints
within the grammar \citep{Sag2010b,chaves12b}.

Another major empirical problem in connection to the analysis of
long"=distance dependencies is the so-called extraction pathway marking
phenomenon \citep{mccloskey79,Zaenen83a-u}. \mgcmt{Is this (going
to be) discussed in Chapter~\ref{chap-udc}?} While
this issue has received considerable attention in the HPSG literature,
through a series of work by Levine and Hukari (see \citealt{LH2006a}), there
is currently no explicit treatment of this phenomenon in the CG
literature. CCG can probably incorporate the HPSG analysis relatively
easily, given the close similarity of the basic analysis of extraction
in the two approaches, which crucially involves keeping track of
extraction pathway explicitly in the syntactic category of the
expression containing a gap at each step of syntactic composition (but
no explicit analysis has been worked out to date). Extraction pathway
marking seems to pose a somewhat trickier challenge to analyses of
extraction in TLCG, which typically do not involve the HPSG/CCG type
step-by-step explicit encoding of extraction pathways. How to address
this issue in TLCG is currently one major empirical question in TLCG
research.

%At the end of section 3, I noted briefly that the reconceptualization
%of the Steedman/Dowty CCG analysis of NCC within TLCG by
%-Morrill94a-u/ and \citet{Carpenter98a-u} offers a more transparent view on
%the underlying patterns of the empirical phenomenon, and that the
%notion of proof normalization is at the heart of this
%reconceptualization. However, the relative degrees of difficulty that
%the extraction pathway marking phenomenon poses for CCG and TLCG may
%perhaps suggest that things are not so simple and that there are after
%all different empirical consequences between function
%composition-based analyses and hypothetical reasoning-based analyses
%of linguistic phenomena, which in a way is similar to the relationship
%between non-movement-based analyses and movement-based analyses of
%linguistic phenomena in the debate between HPSG and mainstream syntax.
%Here, I just note this point as an issue worth exloring further in
%future work.




\subsection{Coordination and ellipsis}

Coordination and ellipsis are both major issues in contemporary
syntactic theory. There are moreover some phenomena, such as Gapping
and Stripping, which seem to lie at the boundary between the two
empirical domains. There are some important similarities and
differences between analytic ideas entertained in the HPSG and CG
literature for problems in these empirical domains, which I will try
to highlight in the following discussion.


\subsubsection{Analyses of nonconstituent coordination  \label{NCCcomparison}}

CG is perhaps best known in the wider linguistic literature for its
analysis of nonconstituent coordination. Steedman's work on CCG
\citep{Steedman97a,Steedman2000a-u,steedman2012} in particular has shown
how this analysis of coordination interacts smoothly with analyses of
other major linguistic phenomena (such as long-distance dependencies,
control and raising and quantification) to achieve a surface-oriented
grammar that has wide empirical coverage and at the same time has
attractive computational properties. \citet{kubota-levine-coord} offer 
an up-to-date TLCG analysis of coordination, and compare it with
major alternatives in both CCG and HPSG literature.

%A more recent line of work by K\&L \cmt{ref} propose a version of TLG called
%Hybrid TLCG, which aims to offer comparable empirical coverage but with
%somewhat different perspective on various issues (such as the
%relationship between grammar proper and extra-grammatical factors
%pertaining to island effects).

As compared to long-distance dependencies, coordination (in particular
NCC) has received considerably less attention in the (H)PSG literature
initially (\citet{sgww} is an important exception in the early literature).
Things started to change somewhat around 2000, with a series of
related proposals including \citet{Yatabe2001a}, \citet{Crysmann2003c}, \citet{BS2004a}
and \citet{chaves07} (see Chapters~\ref{chap-coordination} and
\ref{chap-ellipsis}). Here, I take up \citet{BS2004a} (B\&S) and
\citet{Yatabe2001a} as two representative proposals in this line of work.
The two proposals share some common assumptions and ideas, but they
also differ in important respects which do not seem to have received
enough attention in the literature (see also \citet{yatabe-tam2017} for a
more detailed comparison).

Both B\&S and \citet{Yatabe2001a} adopt linearization-based HPSG, together
with (a version of) Minimal Recursion Semantics for
semantics.\mgcmt{need to adjust the discussion here on the basis of what's
  in the ellipsis chapter (already in place)} Of the two, B\&S's
analysis is more in line with standard assumptions in HPSG. The basic
idea of B\&S's analysis is indeed very simple: by exploiting the flexible
mapping between the combinatoric component and the surface word order
realization in linearization-based HPSG, they essentially propose a
surface deletion-based analysis of NCC according to which NCC examples
are analyzed as follows:

\begin{exe}
 \ex\label{BeaversSag}
  [S Terry gave no man a book on Friday] or
  [S \sout{Terry gave no man} a record on Saturday]. 
\end{exe}
where the material in strike-out is underlyingly present but undergoes
deletion in the prosodic representation.

In its simplest form, this analysis gets the scopal relation between
the quantifier and coordination wrong in examples like (\ref{BeaversSag}) (a
well-known problem for the conjunction reduction analysis from the 70s; cf.\ \citet{partee70}).
B\&S address this issue by introducing a condition called `Optional
Quantifier Merger', which says the following:

\begin{exe}
 \ex\label{OQM}
  \textbf{Optional Quantifier Merger:} For any elided phrase 
  denoting a generalized quantifier in the 
  domain of either conjunct, the semantics of that phrase may
  optionally be identified with the semantics of its non-elided
  counterpart.
\end{exe}
As noted by \citet{levine11} and \citet{kubota-levine-coord}, this condition
does not follow from any general principle and is merely stipulated in
B\&S's account.

\citet{Yatabe2001a} and \citet{yatabe-tam2017} (the latter of which contains a
much more accessible exposition of essentially the same proposal as
the former) propose a somewhat different analysis. Unlike B\&S, who
assume that semantic composition is carried out on the basis of the
meanings of \emph{signs} on each node (which is the standard assumption
about semantic composition in HPSG), Yatabe shifts the locus of
semantic composition to the list of domain objects, that is, the
component that directly gets affected by the deletion operation that
yields the surface string.

This crucially changes the default meaning
predicted for examples such as (\ref{BeaversSag}). \mgcmt{Not sure how much
  detail I should go into re the Y\&T unpublished MS. This is
  important work (and different from B\&S in some nontrivial
  respects), but at the same time, it's still unpublished.}
Specifically, on Yatabe's
analysis, surface string for (\ref{BeaversSag}) is obtained by the
`compaction' operation on word order domains that collapses two
quantifiers originally contained in the two conjuncts into one. The
semantics of the whole sentence is computed on the basis of this
resultant word order domain representation, which contains only \emph{one}
instance of a domain object corresponding to the quantifier. The
quantifier is then required to scope over the whole coordinate
structure due to independently motivated principles of
underspecification resolution. While this approach successfully yields
the wide-scope reading for quantifiers, the distributive, narrow scope
reading for quantifiers (which was trivial for B\&S) now becomes a
challenge. Yatabe and Tam simply stipulate a complex disjunctive
constraint on semantic interpretation tied to the `compaction'
operation that takes place in coordination so as to generate the two
scopal readings.

\citet{kubota-levine-coord} note that B\&S's approach suffers from similar
issues in the interpretations of symmetrical predicates, summative
predicates and `respective' readings (see \citet{chaves12} for a lucid
discussion of the empirical parallels between the three phenomena and
how the basic cases can receive a uniform analysis within HPSG).
\citet{yatabe-tam2017} offer a response to K\&L, arguing that \citeposs{Yatabe2001a}
analysis does not suffer from the problems that K\&L point out, and
discuss some empirical issues which they argue are problematic for CG
analyses of coordination (see also \citet{kubota-levine-summative} for a
rebuttal). At least part of the controversy (or confusion) in this
debate seems to be related to different perspectives on what counts as
linguistic analysis: is the primary goal of linguistic analysis an
accurate description of observed facts, or do we want to aim for a
deeper `explanation'? There is no easy answer to this questions, and
different research communities seem to have varying degrees of
emphasis on the opposite poles of the spectrum, as the comparison
of HPSG and minimalism in Chapter~\ref{chap-minimalism} makes clear.


\subsubsection{Gapping and Stripping \label{sec:gapping}}

Descriptively, Gapping is a type of ellipsis phenomenon that occurs
in coordination and which deletes some material including the main
verb:\footnote{There is some disagreement as to whether Gapping is
restricted to coordination. \citet{kubota-levine-gapping}, following authors
such as \citet{johnson2009}, take Gapping to be restricted to coordination.
\citet{parkea18gapping} take a different view, and argue that Gapping
should be viewed as a type of ellipsis phenomenon that is not
restricted to coordination environments.}

\begin{exe}
 \ex\label{gapping}
  \begin{xlist}
 \ex\label{}
    Leslie \textbf{bought} a CD, and Robin   \ensuremath{\varnothing}  a book.
 \ex\label{}
    Terry \textbf{can go} with me, and Pat  \ensuremath{\varnothing}  with you.
 \ex\label{}
    John \textbf{wants to try to begin to write} a novel, and Mary  \ensuremath{\varnothing}  a play.
  \end{xlist}
\end{exe}
Gapping has invoked some theoretical controversy in the recent
HPSG/CG literature for the `scope anomaly' issue that it exhibits.
The relevant data involving auxiliary verbs such as (\ref{mrj1}) and (\ref{ex2b})
have long been known in the literature since \citet{oehrle71,oehrle1987},
\citet{siegel87} and \citet{mccawley1993} later pointed out similar examples
involving downward-entailing determiners such as (\ref{mccawley}).

\begin{exe}
 \ex\label{scope}
  \begin{xlist}
 \ex\label{mrj1}
    Mrs.\ J can't live in Boston and Mr.\ J  \ensuremath{\varnothing}  in LA.
 \ex\label{ex2b}
    Kim didn't play bingo or Sandy  \ensuremath{\varnothing}  sit at home all evening.
 \ex\label{mccawley}
    No dog eats Whiskas or  \ensuremath{\varnothing}  cat  \ensuremath{\varnothing}  Alpo.
  \end{xlist}
\end{exe}

\citet{kubota-levine-nels,kubota-levine-gapping} noting some difficulties
for earlier accounts of Gapping in the (H)PSG literature and argue
for a constituent coordination analysis of Gapping in TLCG, building
on earlier analyses of Gapping in CG 
\citep{Steedman90a-u,hendriks95,morrillsolias93}. The key idea of
K\&L's analysis involves taking Gapping as coordination of clauses
missing a verb in the middle, which can be transparently represented
as a function from strings to strings:

\begin{exe}
 \ex\label{}
  \pt{ \ensuremath{\lambda} \ensuremath{\greekp}. leslie \ensuremath{\circ}\xspace \ensuremath{\greekp} \ensuremath{\circ}\xspace a \ensuremath{\circ}\xspace cd }
\end{exe}
A special type of conjunction entry then conjoins two such expressions
and returns a conjoined sentence missing the verb only in the first
conjunct (on the prosodic representation). By feeling the verb to
this resultant expression,  a proper form-meaning  pair is obtained
for Gapping sentences like those in (\ref{gapping}).

The apparently unexpected wide scope readings for auxiliaries and
quantifiers in (\ref{scope}) turns out to be straightforward on this
analysis. I refer the interested reader to \citet{kubota-levine-gapping} for
details, but the key idea is that the apparently `anomalous' scope in
such examples isn't really anomalous on this approach, since the
auxiliary (which prosodically lowers into the first conjunct) takes
the whole conjoined gapped clause as its argument. Thus, the existence
of the wide scope reading is automatically predicted on this analysis.
\citet{puthawala2018} extends this approach to a similar `scope anomaly'
data found in Stripping, another type of (coordinate) ellipsis
phenomenon.

The determiner gapping example in (\ref{mccawley}) requires a somewhat more
elaborate treatment. \citet{kubota-levine-gapping} analyze determiner
gapping via higher-order functions. \citet{morrillvalentin16} criticize this
approach for a certain type of overgeneration problem regarding word
order and propose an alternative analysis in Displacement Calculus.

\citet{parkea18gapping} propose an analysis of Gapping in HPSG that
overcomes the limitations of previous (H)PSG analyses of Gapping
\citep{sgww,abeille-ea,chaves05}, couched in Lexical Resources Semantics.
\mgcmt{Need to check the status of the Park et al.~paper. Is a written
version already available?}
In their analysis, the lexical entries of the clause-level conjunction
words \textit{and} and \textit{or} are underspecified as to the relative scope
between the propositional operator contributed by the modal auxiliary
in the first conjunct and the boolean conjunction or disjunction
connective that is contributed by the conjunction word itself. Park et
al. argue that this is sufficient to capture the scope anomaly in the
Oehrle/Siegel data such as (\ref{mrj1}) and (\ref{ex2b}). Extension to
the determiner gapping case (\ref{mccawley}) is left for future work.

At a somewhat general level, the main difference between the HPSG and
(Hybrid) TLCG analyses of Gapping again seems to at least partly
pertain to the level of linguistic analysis that the respective
approaches mainly target. K\&L's analysis crucially makes use of
higher-order functions both in the semantic and prosodic components.
This may pose issues, for example, in computational implementation or
in the context of building an explicit human sentence processing
model. The standard view in generative grammar of course is to
separate the competence grammar and performance, but still, it seems
legitimate to note that in the type of analysis advocated by K\&L, the
theoretical elegance is gained at the cost of making the relationship
between the competence grammar and performance less direct.
\citeposs{parkea18gapping} approach on the other hand is more in line with the
usual practice of HPSG research where the main emphasis is on writing
an explicit grammar fragment that is constraint-based and
surface-oriented. Again, this type of conflict is not easy to
overcome, especially when (as in the present situation) the competing
analyses are formulated in different theories. Perhaps we will arrive
at a synthesizing view at some point in the future. Until then, it
seems most fruitful for different approaches to pursue their own
goals, while keeping an eye on alternative perspectives.


\subsubsection{Ellipsis}

Analyses of major ellipsis phenomena in HPSG and CG share the same
essential idea that ellipsis is a form of anaphora, without any
invisible hierarchically structured representations corresponding to
the `elided' expression. See Chapter~\ref{chap-ellipsis} for an overview of approaches
to ellipsis phenomena in HPSG.\mgcmt{check
ellipsis chapter (now complete) to see how much to include here}
In this section, I try to
highlight some possible differences between some of the representative
proposals in the recent literature on ellipsis in HPSG and CG.

Recent analyses of ellipsis in HPSG \citep{GSag2000a-u,millereisspseudo}
make heavy use of the notion of `constructions' adopted from
Construction Grammar (this idea is even borrowed into some of the CG
analyses of ellipsis such as \citet{jacobson2016}). Many ellipsis phenomena
are known to exhibit some form of `syntactic sensitivity'
\citep{kennedy2003,chung13,yoshida-ea-pg}, and this fact has long been
taken to provide strong evidence for the `covert structure' analyses
of ellipsis popular in mainstream syntactic literature \citep{merchant13}.
\cmt{Perhaps explain GS's analysis of sluicing a bit here
and make it clear how it captures syntactic sensitivity}

Some of the early work on ellipsis in CG include \citet{hendriks-diss} and
\citet{morrillmerenciano1996}. \citet{morrillmerenciano1996} in particular show
how hypothetical reasoning in TLCG allows treatments of important
properties of ellipsis phenomena such as strict/sloppy ambiguity and
scope ambiguity of elided quantifiers in VP ellipsis. \citet{jaeger05}
integrates these earlier works with a general theory of anaphora in
TLCG incorporating the key empirical analyses of pronominal anaphora
by \citet{jacobson1999a,jacobson2000a}. Jacobson's
\citeyear{jacobson_p1998a,jacobson2008} analysis of
Antecedent-Contained Ellipsis is also important.
ACE is often taken to offer a strong piece of evidence for 
the representational analysis of
ellipsis in mainstream generative syntax.
Jacobson offers a counterproposal to this standard analysis
that completely dispenses with covert structural representations.
While the above works from the 90s
have mostly focused on VP ellipsis, recent developments in the CG
literature, including \citet{barker-sluicing} on sluicing, \citet{jacobson2016} on
fragment answers and \citet{kubota-levine-pseudo} on pseudogapping,
considerably extended the empirical coverage of the same line of
analysis.

The relationship between recent CG analyses of ellipsis phenomena and
HPSG counterparts seems to be similar to the situation with competing
analyses on coordination. Both \citet{barker-sluicing} and
\citet{kubota-levine-pseudo} exploit hypothetical reasoning to treat the
antecedent of an elided material as a `constituent' with full-fledged
semantic interpretation at an abstract combinatoric component of
syntax. The anaphoric mechanism can then refer to both the syntactic
and semantic information of the antecedent expression to capture
syntactic sensitivity observed in ellipsis phenomena, without the need
to posit hierarchical representations at the ellipsis site. Due to its
surface-oriented nature, HPSG is not equipped with an analogous
abstract combinatoric component that assigns `constituent' statuses to
expressions that do not (in any obvious sense) correspond to
constituents in the surface representation. In HPSG, the major work in
constraining the appropriate pairing of meaning and form is instead
taken over by constructional schemata, which can both specify
fine-grained details of specific subtypes of ellipsis phenomena and at
the same time capture broader generalizations about ellipsis phenomena
in general via highly articulate inheritance hierarchy organizing
constructional schemata. This again pertains to a fundamental
difference between the two theoretical frameworks, and at this point,
it is unclear whether any consensus or synthesis of perspectives will
ultimately emerge in the future.




\subsubsection{Mismatches in right-node raising}

\begin{itemize}
 \item Some important work in the HPSG literature:
  Shiraishi et al. (EISS), \citeposs{Chaves2014a-u} detailed  analysis of RNR
\end{itemize}
\begin{itemize}
 \item So far, no explicit analyses in the CG literature, though there does
  not seem to be any fundamental obstacle to treating some cases of RNR
  as instances of ellipsis within CG.
\end{itemize}

\subsection{Binding}

Empirical phenomena that have traditionally been analyzed by means of
Binding Theory (both in the transformational and the
nontransformational literature) potentially pose a major challenge to
the `non-representational' view of the syntax-semantics interface
common to most variants of CG. As discussed in Chapter~\ref{chap-binding}, the HPSG
Binding Theory captures Principle A and B effects at the level of
argument structure, while Principle C makes reference to the
configurational structure (i.e.\  the feature-structure encoding of the
constituent geometry). The status of Principle C itself is
controversial to begin with, but if this condition would need to be
stated in the syntax, then, it would constitute the greatest challenge
to CG-based theories of syntax.

While there seems to be no consensus in the current CG literature on
how the standard binding theoretic facts are to be captured, there are
some important ideas and proposals in the wider literature of CG-based
syntax (broadly construed to include work in the Montague Grammar
tradition). First, as for Principle A, there is a recurrent suggestion
in the literature that these effects can (and should) be captured
simply via strictly lexical properties of reflexive pronouns
(e.g.\  \citet{szabolcsi1992}; see \citet{buringbinding} for a concise summary).
For example, for a reflexive in the direct object position
of a transitive verb bound by the subject NP,
the following type assignment (where the reflexive pronoun first takes
a transitive verb and then the subject NP as arguments) suffices to
capture its bound status:

\begin{exe}
 \ex\label{SELF}
  \LexEnt{\pt{himself }}{\sem{ \lambda R \lambda x. R(x)(x)}}{\syncat{NP\ensuremath{\backslash}{}((NP\ensuremath{\backslash}{}S)/NP)\ensuremath{\backslash}{}S}}
\end{exe}
While this approach is attractively simple, there are at least two
things to keep in mind, in order to make it a complete analysis of
Principle A within some contemporary variant of CG. First, while the
lexical treatment of reflexive binding might at first sight (\ref{SELF})
appear to capture the locality of binding quite nicely, CG's flexible
syntax potentially overgenerates unacceptable long-distance binding
readings for (English) reflexives. Since right-node raising can take
place across clause boundaries,  it seems necessary to
assume that hypothetical reasoning for the Lambek-slash
(or a chain of function composition that has the same effect in CCG)
can generally take place across clause boundaries. But then,
expressions such as \textit{thinks Bill hates} can be assigned
the same syntactic type (i.e.\  \syncat{(NP\ensuremath{\backslash}{}S)/NP}) as lexical transitive verbs,
overgenerating non-local binding of a reflexive from a subject NP in
the upstairs clause (*\textit{John\ensuremath{_i} thinks Bill hates {himself\ensuremath{_i}}}).

In order to prevent this situation while still retaining the
`reflexive as a local higher-order function' analysis sketched above,
some kind of restriction needs to be imposed as to the way in which
reflexives combine with other linguistic expressions (one possibility
would be to incorporate some version of the notion of `modal control'
from the literature of Multi-Modal TLCG, to draw a distinction between
lexical transitive verbs and derived transitive verb-like expressions
by positing different `modes of composition' in the two cases and
making the lexical specification of the reflexive pronoun sensitive to
this distinction).

The other issue is that the lexical entry in (\ref{SELF}) needs to be
generalized to cover all cases in which a reflexive is bound by an
argument that is higher in the obliqueness hierarchy. This amounts to
positing a polymorphic lexical entry for the reflexive. While the use
of polymorphism is needed anyway in the grammar (at least in CG---e.g.,
to achieve a tight syntax-semantics correspondence in the analysis of
coordination), unlike in standard accounts of binding phenomena in
other lexicalist theories such as HPSG and LFG, this account would
capture the Principle A effects purely via the specific lexical
encoding for reflexive pronouns. In this sense, it is worth noting
that such an account would not be a simple `translational equivalent'
of the HPSG binding theory, even though both are certainly
`lexicalist' analyses of binding effects in the broader sense.

While Principle A effects are in principle amenable to a quite
straightforward lexical treatment, Principle B turns out to be
considerably more challenging. To see this point, note that the
lexical analysis of reflexives sketched above crucially relies on the
fact that the constraint associated with reflexives corresponds to a
straightforward semantic effect of variable binding. Pronouns instead
require \emph{disjointness} of reference from less oblique co-arguments, but
such an effect cannot be captured by simply specifying some
appropriate lambda term as the semantic translation for the pronoun.

To date, the most detailed treatment of Principle B effects in CG that
explicitly addresses this difficulty is the proposal by \citet{jacobson07},
formulated in a version of CCG (standard CCG has its own treatment of
syntactic Binding Principles, which effectively recognizes an
intermediate level of representation for the purpose of stating
Binding Principles---I will discuss this approach briefly at the end
of this section).

\cmt{section incomplete; explain Jacobson's approach in what follows}

\noindent
Other things to discuss:

\begin{itemize}
 \item Steedman's take on binding
 \item What about Principle C?
\end{itemize}


\section{Brief notes on proccessing and implementation}

The discussion above has mostly focused on linguistic analysis. In
this final section, I will briefly comment on CG's implications for
psycholinguistics and computational linguistics research.

One attractive feature of CCG (but not CG in general), when viewed as
an integrated model of the competence grammar and human sentence
processing, is that it enables `surface-oriented', incremental
analyses of strings from left to right. This aspect was emphasized in
the early literature of CCG \citep{AS82a,CS85a}, but it does not seem to
have had much impact on psycholinguistic research in general since
then. A notable exception is the work by \citet{pickering-barry91,PB93a} in
early 90s. There is some work on the relationship between processing
and TLCG (see \cite[Chapters 9 and 10]{morrill2011}, and references
therein). In any event, serious investigations of the relationship
between competence grammar and human sentence processing from a CG
perspective (either CCG or TLCG) seems to be a large untouched
opportunity for future research, much like the situation with HPSG
(see Chapter~\ref{chap-processing}).

As for connections to computational linguistics/NLP research, like
HPSG, large-scale computational implementation has been an important
research agenda for CCG (see, e.g., \citet{CC2007a-u}). I refer
the reader to \cite[Chapter 13]{steedman2012} for an excellent
summary on this subject (this chapter contains a discussion of human
sentence processing as well). Together with work on linguistically
informed `deep parsing' in HPSG, CCG parsers seem to be attracting
some renewed interest in CL/NLP research recently, due to the new trend of
combining the insights of statistical approaches and
linguistically-informed approaches. In particular, the straightforward
syntax-semantics interface of (C)CG seems to be an attractive feature
in building CL/NLP systems that have an explicit compositional
semantics component. See \citet{steedmanlewis13} and \citet{mineshima-etal:2016:emnlp}
for this type of work. TLCG research has traditionally been less
directly related to CL/NLP research. But there are recent attempts at
constructing large-scale treebanks \citep{moot2015} and combining TLCG
frameworks with more mainstream approaches in NLP research such as
distributional semantics \citep{moot2018}.


\section{Conclusion}

 
\section*{Abbreviations}
\section*{Acknowledgements}

\printbibliography[heading=subbibliography,notkeyword=this] 
\end{document}
